                <p><h2>Why tensor networks?</h2>
<p>If you follow machine learning, you have definitely heard of <a href="http://neuralnetworksanddeeplearning.com">neural networks</a>.
If you are a physicist, you may have heard of <a href="https://arxiv.org/abs/1603.03039">tensor networks</a> too.
Both are schemes for assembling simple units (neurons or tensors) into complicated functions: 
decision functions in the case of machine learning or wavefunctions in the case of quantum mechanics.</p>
<p>But tensor networks have only linear elements. Neural networks crucially 
require <em>non-linear</em> elements for their success (specifically, non-linear <a href="https://en.wikipedia.org/wiki/Activation_function">neuron activation functions</a>). 
And neural networks have been so wildly successful in recent years that leading tech companies are 
staking their futures on them. </p>
<p>So why bother with linear tensor networks if non-linearity is the key to success?</p>
<p>The key is dimensionality. Problems which are difficult to solve in low dimensional spaces
become easier when "lifted" into a higher dimensional space. 
Think how much easier your day would be if you could move freely in the extra 
dimension we call time. Data points hopelessly intertwined in their native, low-dimensional
form can become linearly separable when given the extra breathing room of more dimensions.</p>
<p>But extra dimensions come at a steep price, known as the "curse of dimensionality". 
When constructing a high dimensional space from products of smaller spaces, its dimension
grows exponentially. Optimizing even a linear classifier in an exponentially big space becomes
costly very quickly.</p>
<p>This is the problem tensor networks solve: if you can live with a reduced set of
linear transformations, then tensor networks let you manipulate objects in an exponentially high 
dimensional space easily and efficiently.</p>
<p>Tensor networks can do linear things within spaces so big, there's no reason they couldn't be
as effective as neural networks, which do non-linear things to much smaller spaces.
But whether they can really compete with neural nets remains to be seen!</p>
<h2>Tensor networks in physics</h2>
<p>Quantum physicists have faced the challenges of exponentially large spaces
since the days of Dirac. Wavefunctions describing the collective state
of N quantum particles live in just such a high-dimensional space. For decades, approaches
to deal with the huge size of quantum state space relied on using a modest set of ad-hoc variables 
to parameterize a large wavefunction, then optimizing these variables and hoping for the best. 
Or physicists would avoid wavefunctions altogether in favor of large-scale simulations of 
particle motions (e.g. quantum Monte Carlo methods).</p>
<p>But the exponential size of quantum wavefunction space turns out to be an illusion. 
The wavefunctions typically occurring in nature are just a tiny fraction of 
the huge mathematical space they belong to. And nearly all of this small corner 
of wavefunction space is within reach of tensor networks.</p>
<p>The first tensor network developed in the late 80's was the "matrix product state" or MPS. 
An MPS is a scheme to encode wavefunction amplitudes as the
product of matrices (surrounded by boundary vectors, so the result is a 
scalar). For S=1/2 spins on a lattice, there are two matrices associated to each lattice site; 
which matrix goes into the product depends on whether the spin is up or down. 
If the matrix sizes required to capture the true wavefunction 
do not grow with system size, then the MPS format compresses exponentially 
many parameters down to only a linear number of parameters! (That is, linear in the system size.) </p>
<p>Whether the MPS approach can usefully compress the wavefunction of a physical system
hinges on its dimensionality and how much <em>quantum entanglement</em> it has across various spatial cuts.
For "lightly" entangled, one-dimensional quantum states, such as ground states 
of local 1D Hamiltonians, MPS work incredibly well. There is a proof of 
their optimality for a wide set of cases. MPS are even useful for two-dimensional
systems with extra computational effort.</p>
<p>Inspired by the success of the MPS tensor network, other tensor network proposals
followed, such as the PEPS network for two-dimensional quantum systems (<a href="https://arxiv.org/abs/cond-mat/0407066">Verstraete, 2004</a>)
and the MERA network for critical systems (<a href="http://link.aps.org/doi/10.1103/PhysRevLett.99.220405">Vidal, 2007</a>). Both of these are trickier to use than
MPS, but offer a more compression, and in the case of MERA, deeper physics insights.</p>
<h2>Tensor network machines</h2>
<p>High-dimensional spaces pop up everywhere in machine learning theory. So tensor 
networks seem a natural tool to try (with benefit of hindsight, of course). 
There have already been creative proposals to use MPS
(also known as <em>tensor trains</em>) to parameterize Bayesian models (<a href="http://www.jmlr.org/proceedings/papers/v32/novikov14.pdf">Novikov, 2014</a>); to perform PCA/SVD
analysis of huge matrices (<a href="http://arxiv.org/abs/1410.6895">Lee, 2014</a>); to extract useful features from data tensors (<a href="http://arxiv.org/abs/1503.00516">Bengua, 2015</a>); and to parameterize neural net layers (<a href="http://arxiv.org/abs/1509.06569">Novikov, 2015</a>).</p>
<p>But which machine learning framework is most natural for tensor 
networks, in the sense of applying the full power of optimization techniques, insights, and generalizations
of MPS developed in physics?</p>
<p>Two recent papers propose that tensor networks could be especially powerful 
 in the setting of non-linear kernel learning: <a href="http://arxiv.org/abs/1605.03795">Novikov, Trofimov, Oseledets (2016)</a> and <a href="http://arxiv.org/abs/1605.05775">Stoudenmire, Schwab (2016)</a>.
Kernel learning basically means optimizing a decision function of the form
</p>
<div class="math">$$
f(\mathbf{x}) = W\cdot\Phi(\mathbf{x})
$$</div>
<p>
where <span class="math">\(\mathbf{x}\)</span> is a moderate-size vector of inputs (e.g. pixels of an image) and the output
<span class="math">\(f(\mathbf{x})\)</span> determines how to classify each input. </p>
<p>The function <span class="math">\(\Phi(\mathbf{x})\)</span> is known as the <em>feature map</em>. Its role is to "lift" 
inputs <span class="math">\(\mathbf{x}\)</span> to a higher dimensional <em>feature space</em> before they are classified. 
The feature map <span class="math">\(\Phi\)</span> is a non-linear yet rather generic and simple function, in the sense of having only a few
adjustable "hyper parameters" (a common example of a feature map is the one associated with the <a href="https://en.wikipedia.org/wiki/Polynomial_kernel">polynomial kernel</a>).</p>
<p>The weight vector <span class="math">\(W\)</span> contain the actual parameters of the model to be learned. The clever thing about
kernel learning is although the inputs enter non-linearly via the feature map <span class="math">\(\Phi\)</span>, the
weights <span class="math">\(W\)</span> enter <em>linearly</em> &mdash; the model is just a linear classifier in feature space. 
The number of weights is determined by the dimensionality of the feature space (the target space of <span class="math">\(\Phi\)</span>).
A higher-dimensional feature space can produce a more powerful model, but also requires 
optimizing more weights. In some common approaches, the number of weight parameters
to optimize can be exponentially big or even infinite.</p>
<p>But this smells like a perfect problem for tensor networks: finding the best
set of linear weights in an exponentially big space.</p>
<p>To obtain a weight vector with a structure like a quantum wavefunction, and
suitable for the tensor network approximations used in physics,
recall that combining independent quantum systems corresponds to taking a tensor
product of their state spaces. For a feature map mimicking this rule,
first map each component <span class="math">\(x_j\)</span> of the input vector <span class="math">\(\mathbf{x}\)</span> into a
small d-dimensional vector via a <em>local feature map</em> <span class="math">\(\phi(x_j)\)</span>. Then
combine these local feature vectors using a tensor product:
</p>
<div class="math">$$
\Phi^{s_1 s_2 \cdots s_N}(\mathbf{x}) = \phi^{s_1}(x_1) \otimes \phi^{s_2}(x_2) \otimes \cdots \otimes \phi^{s_N}(x_N)
$$</div>
<p>
The result is a <span class="math">\(d^N\)</span>-dimensional feature vector. However, it has the structure
of a product-state wavefunction (or rank-1 tensor in applied math parlance), making it
easy to store and manipulate.</p>
<figure>
<img src="/images/phi.png" style="float: center; width: 250px; margin-right: 1%; margin-bottom: 0.0em;"/>
<p style="clear: both;">
<figcaption>Feature map as a tensor product of local feature maps</figcaption>
</figure>

<p><br/></p>
<p>With the above construction, <span class="math">\(W\)</span> also has <span class="math">\(d^N\)</span> components, and has the structure of an
<span class="math">\(N^\text{th}\)</span> order tensor (N indices of size d). This is an object in the same
mathematical space as a wavefunction of N spins (d=2 corresponding to S=1/2, d=3 to S=1, etc.).
But while some wavefunctions in state space (now feature space) are readily 
compressible into tensor networks, the vast majority cannot be compressed at all. 
Do weights of machine learning models live in the 
same nicely compressible part of state space as tensor networks?</p>
<figure>
<img src="/images/decision.png" style="float: center; width: 350px; margin-right: 1%; margin-bottom: 0.0em;" />
<p style="clear: both;">
<figcaption>Decision function with a tensor-product feature map (top) and MPS approximation of weights (bottom)</figcaption>
</figure>

<p><br/></p>
<p>In lieu of a general answer, we did an experiment. Our work (<a href="http://arxiv.org/abs/1605.05775">Stoudenmire, 2016</a>)
considered grayscale image data of handwritten digits (the MNIST data set). Taking an
ad-hoc local feature map which maps grayscale pixels into two-component vectors mimicking
S=1/2 spins, we trained a model to distinguish the digits 0,1,2,...,9. We approximated
the weight tensor <span class="math">\(W\)</span> as an MPS and optimized it by minimizing a squared-error cost
function. The results were surprisingly good: for a very modest
size of 100 by 100 matrices in the MPS, we obtained over 99% classification accuracy on the
training and test data sets. The experiments of <a href="http://arxiv.org/abs/1605.03795">Novikov, 2016</a> considered another
standard data set and synthetic, highly correlated data and found similarly good results.</p>
<h2>A Linear Path Ahead</h2>
<p>Compressing the weight tensor into an MPS is interesting, but what are the benefits for machine learning?</p>
<p>One immediate gain comes from optimizing the weights directly. 
The typical way to avoid the costs 
of high-dimensional feature space is avoid touching the weights by using the so-called "kernel trick" 
which keeps the weights hidden in favor of alternate "dual variables". But this trick
requires constructing a kernel matrix whose number of entries grows quadratically
with training set size.
In the era of big data, this scaling issue is cited
as one reason why neural nets have overtaken kernel methods.
In contrast, optimizing the weights as an MPS scales at
most linearly in the training set size (for a fixed size of the MPS matrices).
The cost of applying or testing the model is independent of training set size.</p>
<p>The simplicity of a model where the decision function depends linearly on the weights
also makes it straightforward to import past insights and powerful optimization
techniques developed in the physics community.
Instead of using gradient descent / backpropagation, we borrowed the powerful DMRG technique
for optimizing the weights (also known as alternating least squares in applied math). 
Not only is DMRG very efficient, but it is adaptive, allowing
the matrix dimensions of our MPS to grow in regions where more resources are needed
and shrink in less important regions, such as near the boundary of an image where the pixels
do not vary. In the future we could take advantage of DMRG developments, such
as the use of conserved "quantum numbers"; continuous MPS for data in the continuum limit;
or real-space parallel DMRG for optimizing large models on distributed hardware.</p>
<p>Finally, tensor networks may enhance interpretability and generalization. While more
work is needed on these fronts, the linearity of tensor networks could lead to rapid
progress. The MPS format already lends itself to an interpretation as 
an "finite state machine" processed in different ways depending
on the input data. Putting in a tensor network like the MERA instead might lead to interpretations
similar to deep neural network architectures, yet easier to analyze 
by virtue of being linear. Intriguingly, a tensor network model need not obey 
the "representer theorem"; this means tensor network models do not just interpolate the 
training set data, which could improve generalization.</p>
<p>Will the tensor network approach continue to be successful for more difficult 
data sets? How do its costs compare to neural networks for problems where 
both yield similar performance? Can tensor networks outperform neural networks
on certain problems? </p>
<p>Given the excellent track record of tensor networks in physics, and the 
deep theoretical underpinnings of kernel learning the future could be bright.</p>
<p><br/></p>
<!--
## Taking off to feature space
Lifting data into a high-dimensional space to classify it is not a new idea. It is the starting
point for the very elegant support vector machine (SVM) method, and non-linear kernel learning
methods more broadly.
Before classifying an input data vector $\mathbf{x}$, kernel methods lift the vector to a higher-dimensional
*feature space* by applying a feature map $\Phi(\mathbf{x})$ to them.
As a simple example, say the input vector is $\mathbf{x} = (x\_1,x\_2)$. Then we can lift this vector
by defining a feature map as follows:
$$
\Phi(\mathbf{x}) = (1,\,x\_1,\,x\_2,\, x\_1^2 ,\,x\_2^2)
$$
This non-linear map takes the two-dimensional vector $\mathbf{x}$ into a five-dimensional vector $\Phi(\mathbf{x})$.
We then classify our data with a decision function of the form
$$
f(\mathbf{x}) = W \cdot \Phi(\mathbf{x})
$$
which is just a linear classifier applied to the vector $\Phi(\mathbf{x})$.
It is a linear classifier in *feature space*.
What have we gained? Linear classifiers are straightforward to optimize. If there is a hyperplane that
can separate our data, it only takes a bit of linear algebra to find it. By solving a linear problem,
we get the full power of a non-linear decision function. ($\Phi(\mathbf{x})$ is non-linear so
$f(\mathbf{x})$ will be too.)
As an example, say we need to separate all inputs $\mathbf{x}$ with
$|\mathbf{x}| < 1$ from all other inputs with $|\mathbf{x}| > 1$. The decision boundary
will be a circle, which we cannot make with a plain "unlifted" linear classifier of the 
form $\mathbf{n} \cdot \mathbf{x}$. But in our lifted model, by choosing $W = (-1,0,0,1,1)$ we obtain
an $f(\mathbf{x})$ which is negative for points inside our circle and positive for points outside.
There is a catch of course: a feature map like the example above may not be high-dimensional or 
non-linear enough to make the data separable for the toughest problems. We can try to solve this 
by mapping to a really high dimensional space as follows:
\begin{align}
\Phi(\mathbf{x}) = & (1,\,x\_1,\,x\_2,\,\ldots,\,x\_N,\, \\\\
 &\  x\_1\cdot x\_2,\ \ \ x\_1 \cdot x\_3,\,\ldots \\\\
 &\  x\_1 \cdot x\_2 \cdot x\_3,\ \ \  x\_1 \cdot x\_2 \cdot x\_4,\,\ldots \\\\
 &\  \ldots, \\\\
 &\  x\_1 \cdot x\_2 \cdot x\_3 \cdot x\_4 \cdots x\_N \, )
\end{align}
The above feature map has all possible products of the components of $\mathbf{x}$ to all orders.
But if you count the number of terms above,
you will see that $\Phi(\mathbf{x})$ is a $2^N$-dimensional vector where $N$ is the number of entries
of $\mathbf{x}$. That means the weight vector $W$ would need to be $2^N$ dimensional also. 
There is no way to directly manipulate a vector that large once $N$ gets bigger than 40 or so.
-->

<h2>Appendix: A brief history of tensor networks</h2>
<p>Tensor networks as a computational tool originated in the field of quantum physics
(not to be confused with <a href="http://www.socher.org/index.php/Main/ReasoningWithNeuralTensorNetworksForKnowledgeBaseCompletion">neural tensor networks</a>). Early on
they were considered for describing the mathematical structure of exotic states of quantum
spin models, such as the <a href="https://en.wikipedia.org/wiki/AKLT_model">AKLT chain</a> (<a href="http://www.sciencedirect.com/science/article/pii/0370157381900703">Accardi, 1981</a>; <a href="https://arxiv.org/abs/cond-mat/9409107">Lange, 1994</a>).</p>
<p>The tensor network known as the matrix product state (MPS) came into prominence with the 
development of the DMRG algorithm (<a href="http://link.aps.org/doi/10.1103/PhysRevLett.69.2863">White, 1992</a>; <a href="http://dx.doi.org/10.1016/j.aop.2010.09.012">Schollwoeck, 2011</a>), later shown to be a particularly efficient
algorithm for optimizing MPS to approximate ground states of quantum systems (<a href="http://dx.doi.org/10.1103/PhysRevLett.75.3537">Ostlund, 1995</a>).</p>
<p>Since then, tensor networks have been influential in many areas of physics such as quantum chemistry
(<a href="http://www.annualreviews.org/doi/abs/10.1146/annurev-physchem-032210-103338">Chan, 2011</a>), condensed matter physics (<a href="https://arxiv.org/abs/1603.03039">Bridgeman, 2016</a>), and even in quantum gravity, where
tensor networks have been proposed as a model of how gravity could emerge from quantum mechanics
(<a href="https://doi.org/10.1103/PhysRevD.86.065007">Swingle, 2012</a>; <a href="https://www.quantamagazine.org/20150428-how-quantum-pairs-stitch-space-time/">Quanta Magazine, 2015</a>). Some key developments in tensor network technology
were the proposal of the PEPS network for two-dimensional quantum systems (<a href="https://arxiv.org/abs/cond-mat/0407066">Verstraete, 2004</a>)
and the MERA network for critical systems (<a href="http://link.aps.org/doi/10.1103/PhysRevLett.99.220405">Vidal, 2007</a>).</p>
<p>More recently, there has been growing interest in tensor networks within the applied mathematics community 
following the proposal of the hierarchical Tucker (<a href="http://rdcu.be/l1K0">Hackbush, 2009</a>) and tensor train (<a href="http://dx.doi.org/10.1137/090752286">Oseledets, 2011</a>) 
decompositions (respectively known in the physics literature as the tree tensor network and MPS decompositions).</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></p>
