<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.dropotron.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/skel-layers.min.js"></script>
    <script src="/js/init.js"></script>
    <link rel="stylesheet" href="/css/pygment.css" />
    <noscript>
        <link rel="stylesheet" href="/css/skel.css" />
        <link rel="stylesheet" href="/css/style.css" />
        <link rel="stylesheet" href="/css/style-noscript.css" />
    </noscript>
    <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
    <!-- <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" integrity="sha384-3AB7yXWz4OeoZcPbieVW64vVXEwADiYyAEhwilzWsLw+9FgqpyjjStpPnpBO8o8S" crossorigin="anonymous">
    <link  href="http://fonts.googleapis.com/css?family=Anonymous+Pro:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" >
    <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie/v8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie/v9.css" /><![endif]-->

    <title>Papers | &#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297; </title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width">
</head>

<body class=" loading">



    <!-- Header -->
    <header id="header" >
        <h1 class="logo">
            <a href="..">&#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297;</a>
        </h1>
        <nav id="nav">
            <ul>
                <!-- <li class="current"><a href="index.html">Welcome</a></li> -->
                    <li><a href="/category/news.html">News</a></li>
                    <li><a href="/category/articles.html">Blog</a></li>
                    <li class="submenu">
                            <a href="#">Papers</a>
                            <ul style="list-style: none;">
                                    <li><a href="/pages/papers-19.html">2019</a></li>
                                    <li><a href="/pages/papers-18.html">2018</a></li>
                                    <li><a href="/pages/papers-17.html">2017</a></li>
                                    <li><a href="/pages/papers-16.html">2016</a></li>
                                    <li><a href="/pages/papers-15.html">2015</a></li>
                            </ul>
                    </li>
                <!--
                <li class="submenu">
                    <a href="../">No Blog</a>
                    <ul>
                            <li >
                                <a href="../category/articles/">Articles</a>
                            </li>
                            <li >
                                <a href="../category/news/">News</a>
                            </li>
                    </ul>
                </li>
                -->
                <!--
                <li><a href="#" class="button special">Nothing to Sign Up to</a></li>
                -->
            </ul>
        </nav>
    </header>

<!-- Main -->
<article id="main">

  <header class="special container">
    <span class="icon fas fa-paperclip"></span>
    <h2>Papers</h2>
    <!-- add page sub title here -->
    <p>The following are recent papers combining the fields of physics - especially quantum mechanics - and machine learning. </br> Please email <a href="mailto:agolubeva@pitp.ca">Anna Go</a> if you would like to see a paper added to this page.</p>
  </header>

  <!-- One -->
  <section class="wrapper style4 container">

    <!-- Content -->
      <div class="content">
        <section>
          <!-- <a href="#" class="image feature"><img src="images/pic04.jpg" alt="" /></a> -->
          <p><h2>Applying Machine Learning to Physics</h2>
<ul>
  <li>
<p><a href="http://arxiv.org/abs/1905.04351v1" title="Recent work has introduced a simple numerical method for solving partial
differential equations (PDEs) with deep neural networks (DNNs). This paper
reviews and extends the method while applying it to analyze one of the most
fundamental features in numerical PDEs and nonlinear analysis: irregular
solutions. First, the Sod shock tube solution to compressible Euler equations
is discussed, analyzed, and then compared to conventional finite element and
finite volume methods. These methods are extended to consider performance
improvements and simultaneous parameter space exploration. Next, a shock
solution to compressible magnetohydrodynamics (MHD) is solved for, and used in
a scenario where experimental data is utilized to enhance a PDE system that is
\emph{a priori} insufficient to validate against the observed/experimental
data. This is accomplished by enriching the model PDE system with source terms
and using supervised training on synthetic experimental data. The resulting DNN
framework for PDEs seems to demonstrate almost fantastical ease of system
prototyping, natural integration of large data sets (be they synthetic or
experimental), all while simultaneously enabling single-pass exploration of the
entire parameter space.">"Solving Irregular and Data-enriched Differential Equations using Deep
  Neural Networks"</a>,
Craig Michoski, Milos Milosavljevic, Todd Oliver, David Hatch,
arXiv: <a href="http://arxiv.org/abs/1905.04351v1">1905.04351</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.04312v1" title="We review the development of generative modeling techniques in machine
learning for the purpose of reconstructing real, noisy, many-qubit quantum
states. Motivated by its interpretability and utility, we discuss in detail the
theory of the restricted Boltzmann machine. We demonstrate its practical use
for state reconstruction, starting from a classical thermal distribution of
Ising spins, then moving systematically through increasingly complex pure and
mixed quantum states. Intended for use on experimental noisy intermediate-scale
quantum (NISQ) devices, we review recent efforts in reconstruction of a cold
atom wavefunction. Finally, we discuss the outlook for future experimental
state reconstruction using machine learning, in the NISQ era and beyond.">"Machine learning quantum states in the NISQ era"</a>,
Giacomo Torlai, Roger G. Melko,
arXiv: <a href="http://arxiv.org/abs/1905.04312v1">1905.04312</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.04305v1" title="We explore artificial neural networks as a tool for the reconstruction of
spectral functions from imaginary time Green's functions, a classic
ill-conditioned inverse problem. Our ansatz is based on a supervised learning
framework in which prior knowledge is encoded in the training data and the
inverse transformation manifold is explicitly parametrised through a neural
network. We systematically investigate this novel reconstruction approach,
providing a detailed analysis of its performance on physically motivated mock
data, and compare it to established methods of Bayesian inference. The
reconstruction accuracy is found to be at least comparable, and potentially
superior in particular at larger noise levels. We argue that the use of
labelled training data in a supervised setting and the freedom in defining an
optimisation objective are inherent advantages of the present approach and may
lead to significant improvements over state-of-the-art methods in the future.
Potential directions for further research are discussed in detail.">"Spectral Reconstruction with Deep Neural Networks"</a>,
Lukas Kades, Jan M. Pawlowski, Alexander Rothkopf, Manuel Scherzer, Julian M. Urban, Sebastian J. Wetzel, Nicolas Wink, Felix Ziegler,
arXiv: <a href="http://arxiv.org/abs/1905.04305v1">1905.04305</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.03255v1" title="We report an experimental demonstration of a machine learning approach to
identify exotic topological phases, with a focus on the three-dimensional
chiral topological insulators. We show that the convolutional neural
networks---a class of deep feed-forward artificial neural networks with
widespread applications in machine learning---can be trained to successfully
identify different topological phases protected by chiral symmetry from
experimental raw data generated with a solid-state quantum simulator. Our
results explicitly showcase the exceptional power of machine learning in the
experimental detection of topological phases, which paves a way to study rich
topological phenomena with the machine learning toolbox.">"Machine Learning Topological Phases with a Solid-state Quantum Simulator"</a>,
Wenqian Lian, Sheng-Tao Wang, Sirui Lu, Yuanyuan Huang, Fei Wang, Xinxing Yuan, Wengang Zhang, Xiaolong Ouyang, Xin Wang, Xianzhi Huang, Li He, Xiuying Chang, Dong-Ling Deng, Lu-Ming Duan,
arXiv: <a href="http://arxiv.org/abs/1905.03255v1">1905.03255</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.02860v2" title="For the last few years, the NASA Quantum Artificial Intelligence Laboratory
(QuAIL) has been performing research to assess the potential impact of quantum
computers on challenging computational problems relevant to future NASA
missions. A key aspect of this research is devising methods to most effectively
utilize emerging quantum computing hardware. Research questions include what
experiments on early quantum hardware would give the most insight into the
potential impact of quantum computing, the design of algorithms to explore on
such hardware, and the development of tools to minimize the quantum resource
requirements. We survey work relevant to these questions, with a particular
emphasis on our recent work in quantum algorithms and applications, in
elucidating mechanisms of quantum mechanics and their uses for quantum
computational purposes, and in simulation, compilation, and physics-inspired
classical algorithms. To our early application thrusts in planning and
scheduling, fault diagnosis, and machine learning, we add thrusts related to
robustness of communication networks and the simulation of many-body systems
for material science and chemistry. We provide a brief update on quantum
annealing work, but concentrate on gate-model quantum computing research
advances within the last couple of years.">"From Ansätze to Z-gates: a NASA View of Quantum Computing"</a>,
Eleanor G. Rieffel, Stuart Hadfield, Tad Hogg, Salvatore Mandrà, Jeffrey Marshall, Gianni Mossi, Bryan O'Gorman, Eugeniu Plamadeala, Norm M. Tubman, Davide Venturelli, Walter Vinci, Zhihui Wang, Max Wilson, Filip Wudarski, Rupak Biswas,
arXiv: <a href="http://arxiv.org/abs/1905.02860v2">1905.02860</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.02345v1" title="Topological quantum error-correcting codes are a promising candidate for
building fault-tolerant quantum computers. Decoding topological codes
optimally, however, is known to be a computationally hard problem. Various
decoders have been proposed that achieve approximately optimal error
thresholds. Due to practical constraints, it is not known if there exists an
obvious choice for a decoder. In this paper, we introduce a framework which can
combine arbitrary decoders for any given code to significantly reduce the
logical error rates. We rely on the crucial observation that two different
decoding techniques, while possibly having similar logical error rates, can
perform differently on the same error syndrome. We use machine learning
techniques to assign a given error syndrome to the decoder which is likely to
decode it correctly. We apply our framework to an ensemble of Minimum-Weight
Perfect Matching (MWPM) and Hard-Decision Re-normalization Group (HDRG)
decoders for the surface code in the depolarizing noise model. Our simulations
show an improvement of 38.4%, 14.6%, and 7.1% over the pseudo-threshold of MWPM
in the instance of distance 5, 7, and 9 codes, respectively. Lastly, we discuss
the advantages and limitations of our framework and applicability to other
error-correcting codes. Our framework can provide a significant boost to error
correction by combining the strengths of various decoders. In particular, it
may allow for combining very fast decoders with moderate error-correcting
capability to create a very fast ensemble decoder with high error-correcting
capability.">"Neural ensemble decoding for topological quantum error-correcting codes"</a>,
Milap Sheth, Sara Zafar Jafarzadeh, Vlad Gheorghiu,
arXiv: <a href="http://arxiv.org/abs/1905.02345v1">1905.02345</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.02342v2" title="Random number generators (RNGs) that are crucial for cryptographic
applications have been the subject of adversarial attacks. These attacks
exploit environmental information to predict generated random numbers that are
supposed to be truly random and unpredictable. Though quantum random number
generators (QRNGs) are based on the intrinsic indeterministic nature of quantum
properties, the presence of classical noise in the measurement process
compromises the integrity of a QRNG. In this paper, we develop a predictive
machine learning (ML) analysis to investigate the impact of deterministic
classical noise in different stages of an optical continuous variable QRNG. Our
ML model successfully detects inherent correlations when the deterministic
noise sources are prominent. After appropriate filtering and randomness
extraction processes are introduced, our QRNG system, in turn, demonstrates its
robustness against ML. We further demonstrate the robustness of our ML approach
by applying it to uniformly distributed random numbers from the QRNG and a
congruential RNG. Hence, our result shows that ML has potentials in
benchmarking the quality of RNG devices.">"Machine Learning Cryptanalysis of a Quantum Random Number Generator"</a>,
Nhan Duy Truong, Jing Yan Haw, Syed Muhamad Assad, Ping Koy Lam, Omid Kavehei,
arXiv: <a href="http://arxiv.org/abs/1905.02342v2">1905.02342</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.02038v1" title="Anomalous diffusion occurs in many physical and biological phenomena, when
the growth of the mean squared displacement (MSD) with time has an exponent
different from one. We show that recurrent neural networks (RNN) can
efficiently characterize anomalous diffusion by determining the exponent from a
single short trajectory, outperforming the standard estimation based on the MSD
when the available data points are limited, as is often the case in
experiments. Furthermore, the RNN can handle more complex tasks where there are
no standard approaches, such as determining the anomalous diffusion exponent
from a trajectory sampled at irregular times, and estimating the switching time
and anomalous diffusion exponents of an intermittent system that switches
between different kinds of anomalous diffusion. We validate our method on
experimental data obtained from sub-diffusive colloids trapped in speckle light
fields and super-diffusive microswimmers.">"Measurement of Anomalous Diffusion Using Recurrent Neural Networks"</a>,
Stefano Bo, Falko Schmidt, Ralf Eichhorn, Giovanni Volpe,
arXiv: <a href="http://arxiv.org/abs/1905.02038v1">1905.02038</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.01205v1" title="One of the open problems in scientific computing is the long-time integration
of nonlinear stochastic partial differential equations (SPDEs). We address this
problem by taking advantage of recent advances in scientific machine learning
and the dynamically orthogonal (DO) and bi-orthogonal (BO) methods for
representing stochastic processes. Specifically, we propose two new
Physics-Informed Neural Networks (PINNs) for solving time-dependent SPDEs,
namely the NN-DO/BO methods, which incorporate the DO/BO constraints into the
loss function with an implicit form instead of generating explicit expressions
for the temporal derivatives of the DO/BO modes. Hence, the proposed methods
overcome some of the drawbacks of the original DO/BO methods: we do not need
the assumption that the covariance matrix of the random coefficients is
invertible as in the original DO method, and we can remove the assumption of no
eigenvalue crossing as in the original BO method. Moreover, the NN-DO/BO
methods can be used to solve time-dependent stochastic inverse problems with
the same formulation and computational complexity as for forward problems. We
demonstrate the capability of the proposed methods via several numerical
examples: (1) A linear stochastic advection equation with deterministic initial
condition where the original DO/BO method would fail; (2) Long-time integration
of the stochastic Burgers' equation with many eigenvalue crossings during the
whole time evolution where the original BO method fails. (3) Nonlinear reaction
diffusion equation: we consider both the forward and the inverse problem,
including noisy initial data, to investigate the flexibility of the NN-DO/BO
methods in handling inverse and mixed type problems. Taken together, these
simulation results demonstrate that the NN-DO/BO methods can be employed to
effectively quantify uncertainty propagation in a wide range of physical
problems.">"Learning in Modal Space: Solving Time-Dependent Stochastic PDEs Using
  Physics-Informed Neural Networks"</a>,
Dongkun Zhang, Ling Guo, George Em Karniadakis,
arXiv: <a href="http://arxiv.org/abs/1905.01205v1">1905.01205</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.01023v1" title="Artificial Intelligence (AI), defined in its most simple form, is a
technological tool that makes machines intelligent. Since learning is at the
core of intelligence, machine learning poses itself as a core sub-field of AI.
Then there comes a subclass of machine learning, known as deep learning, to
address the limitations of their predecessors. AI has generally acquired its
prominence over the past few years due to its considerable progress in various
fields. AI has vastly invaded the realm of research. This has led physicists to
attentively direct their research towards implementing AI tools. Their central
aim has been to gain better understanding and enrich their intuition. This
review article is meant to supplement the previously presented efforts to
bridge the gap between AI and physics, and take a serious step forward to
filter out the &quot;Babelian&quot; clashes brought about from such gabs. This
necessitates first to have fundamental knowledge about common AI tools. To this
end, the review's primary focus shall be on deep learning models called
artificial neural networks. They are deep learning models which train
themselves through different learning processes. It discusses also the concept
of Markov decision processes. Finally, shortcut to the main goal, the review
thoroughly examines how these neural networks are capable to construct a
physical theory describing some observations without applying any previous
physical knowledge.">"Physicist's Journeys Through the AI World - A Topical Review. There is
  no royal road to unsupervised learning"</a>,
Imad Alhousseini, Wissam Chemissany, Fatima Kleit, Aly Nasrallah,
arXiv: <a href="http://arxiv.org/abs/1905.01023v1">1905.01023</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.13165v1" title="Determining phase diagrams and phase transitions semi-automatically using
machine learning has received a lot of attention recently, with results in good
agreement with more conventional approaches in most cases. When it comes to
more quantitative predictions, such as the identification of universality class
or precise determination of critical points, the task is more challenging. As
an exacting test-bed, we study the Heisenberg spin-1/2 chain in a random
external field that is known to display a transition from a many-body localized
to a thermalizing regime, which nature is not entirely characterized. We
introduce different neural network structures and dataset setups to achieve a
finite-size scaling analysis with the least possible physical bias (no assumed
knowledge on the phase transition and directly inputing wave-function
coefficients), using state-of-the-art input data simulating chains of sizes up
to L=24. In particular, we use domain adversarial techniques to ensure that the
network learns scale-invariant features. We find a variability of the output
results with respect to network and training parameters, resulting in
relatively large uncertainties on final estimates of critical point and
correlation length exponent which tend to be larger than the values obtained
from conventional approaches. We put the emphasis on interpretability
throughout the paper and discuss what the network appears to learn for the
various used architectures. Our findings show that a it quantitative analysis
of phase transitions of unknown nature remains a difficult task with neural
networks when using the minimally engineered physical input.">"Neural network setups for a precise detection of the many-body
  localization transition: finite-size scaling and limitations"</a>,
Hugo Théveniaut, Fabien Alet,
arXiv: <a href="http://arxiv.org/abs/1904.13165v1">1904.13165</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.12072v1" title="A Markov chain update scheme using a machine-learned flow-based generative
model is proposed for Monte Carlo sampling in lattice field theories. The
generative model may be optimized (trained) to produce samples from a
distribution approximating the desired Boltzmann distribution determined by the
lattice action of the theory being studied. Training the model systematically
improves autocorrelation times in the Markov chain, even in regions of
parameter space where standard Markov chain Monte Carlo algorithms exhibit
critical slowing down in producing decorrelated updates. Moreover, the model
may be trained without existing samples from the desired distribution. The
algorithm is compared with HMC and local Metropolis sampling for \(\phi^4\)
theory in two dimensions.">"Flow-based generative models for Markov chain Monte Carlo in lattice
  field theory"</a>,
M. S. Albergo, G. Kanwar, P. E. Shanahan,
arXiv: <a href="http://arxiv.org/abs/1904.12072v1">1904.12072</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.11298v1" title="Measurement and estimation of parameters are essential for science and
engineering, where one of the main quests is to find systematic and robust
schemes that can achieve high precision. While conventional schemes for quantum
parameter estimation focus on the optimization of the probe states and
measurements, it has been recently realized that control during the evolution
can significantly improve the precision. The identification of optimal
controls, however, is often computationally demanding, as typically the optimal
controls depend on the value of the parameter which then needs to be
re-calculated after the update of the estimation in each iteration. Here we
show that reinforcement learning provides an efficient way to identify the
controls that can be employed to improve the precision. We also demonstrate
that reinforcement learning is highly transferable, namely the neural network
trained under one particular value of the parameter can work for different
values within a broad range. These desired features make reinforcement learning
more efficient than conventional optimal quantum control methods.">"Transferable control for quantum parameter estimation through
  reinforcement learning"</a>,
Han Xu, Junning Li, Liqiang Liu, Yu Wang, Haidong Yuan, Xin Wang,
arXiv: <a href="http://arxiv.org/abs/1904.11298v1">1904.11298</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.10797v1" title="Machine learning can help us in solving problems in the context big data
analysis and classification, as well as in playing complex games such as Go.
But can it also be used to find novel protocols and algorithms for applications
such as large-scale quantum communication? Here we show that machine learning
can be used to identify central quantum protocols, including teleportation,
entanglement purification and the quantum repeater. These schemes are of
importance in long-distance quantum communication, and their discovery has
shaped the field of quantum information processing. However, the usefulness of
learning agents goes beyond the mere re-production of known protocols; the same
approach allows one to find improved solutions to long-distance communication
problems, in particular when dealing with asymmetric situations where channel
noise and segment distance are non-uniform. Our findings are based on the use
of projective simulation, a model of a learning agent that combines
reinforcement learning and decision making in a physically motivated framework.
The learning agent is provided with a universal gate set, and the desired task
is specified via a reward scheme. From a technical perspective, the learning
agent has to deal with stochastic environments and reactions. We utilize an
idea reminiscent of hierarchical skill acquisition, where solutions to
sub-problems are learned and re-used in the overall scheme. This is of
particular importance in the development of long-distance communication
schemes, and opens the way for using machine learning in the design and
implementation of quantum networks.">"Machine learning for long-distance quantum communication"</a>,
Julius Wallnöfer, Alexey A. Melnikov, Wolfgang Dür, Hans J. Briegel,
arXiv: <a href="http://arxiv.org/abs/1904.10797v1">1904.10797</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.10692v1" title="In large-scale scientific calculations, it often encounters the problem of
determining a multi-dimensional function, which can be time-consuming when
computing each point in this multi-dimensional space is already demanding. In
the letter, we will use a quantum three-boson problem to demonstrate that
active learning algorithm can speed up this calculation. The basic idea is to
fit this multi-dimensional function by neural networks, and the key point is to
make the query of labeled date economically by using a stratagem called &quot;query
by committee&quot;. We will present the general protocol of this fitting scheme, as
well as the procedure of how to further compute physical observable with the
fitted results. In the three-boson example we present, the multi-dimensional
function is a scattering kernel, and the physical observable is the atom-dimer
scattering length, which can exhibit the Efimov resonances. We show that this
algorithm can capture the Efimov resonance and predict the atom-dimer
scattering length with an error of a few thousandths, with only a few percents
of total data points required.">"Speeding up Quantum Few-Body Calculation with Active Learning"</a>,
Juan Yao, Yadong Wu, Hui Zhai,
arXiv: <a href="http://arxiv.org/abs/1904.10692v1">1904.10692</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.10251v2" title="Inspired by the universal approximation theorem and widespread adoption of
artificial neural network techniques in a diversity of fields, we propose
feed-forward neural networks as general purpose trial wave functions for
quantum monte carlo (QMC) simulations of many-body systems. To verify their
practical success, we employ various realizations of this ansatz in QMC
simulations of an exactly solvable model system of two trapped interacting
particles. Finally, we use the same technique to accurately predict the binding
curve of the hydrogen molecule, from first principles.">"Artificial Neural Networks as Trial Wave Functions for Quantum Monte
  Carlo"</a>,
Jan Kessler, Francesco Calcavecchia, Thomas D. Kühne,
arXiv: <a href="http://arxiv.org/abs/1904.10251v2">1904.10251</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.08441v1" title="We demonstrate quantum many-body state reconstruction from experimental data
generated by a programmable quantum simulator, by means of a neural network
model incorporating known experimental errors. Specifically, we extract
restricted Boltzmann machine (RBM) wavefunctions from data produced by a
Rydberg quantum simulator with eight and nine atoms in a single measurement
basis, and apply a novel regularization technique to mitigate the effects of
measurement errors in the training data. Reconstructions of modest complexity
are able to capture one- and two-body observables not accessible to
experimentalists, as well as more sophisticated observables such as the R\'enyi
mutual information. Our results open the door to integration of machine
learning architectures with intermediate-scale quantum hardware.">"Integrating Neural Networks with a Quantum Simulator for State
  Reconstruction"</a>,
Giacomo Torlai, Brian Timar, Evert P. L. van Nieuwenburg, Harry Levine, Ahmed Omran, Alexander Keesling, Hannes Bernien, Markus Greiner, Vladan Vuletić, Mikhail D. Lukin, Roger G. Melko, Manuel Endres,
arXiv: <a href="http://arxiv.org/abs/1904.08441v1">1904.08441</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.07637v1" title="We explore the capacity of neural networks to detect a symmetry with complex
local and non-local patterns: the gauge symmetry \(Z_2\). This symmetry is
present in physical problems from topological transitions to QCD, and controls
the computational hardness of instances of spin-glasses. Here, we show how to
design a neural network, and a dataset, able to learn this symmetry and to find
compressed latent representations of the gauge orbits. Our method pays special
attention to system-wrapping loops, the so-called Polyakov loops, known to be
particularly relevant for computational complexity.">"Learning a Gauge Symmetry with Neural Networks"</a>,
Aurélien Decelle, Victor Martin-Mayor, Beatriz Seoane,
arXiv: <a href="http://arxiv.org/abs/1904.07637v1">1904.07637</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.06032v1" title="We demonstrate that supervised machine learning (ML) with entanglement
spectrum can give useful information for constructing phase diagram in the
half-filled one-dimensional extended Hubbard model. Combining ML with
infinite-size density-matrix renormalization group, we confirm that
bond-order-wave phase remains stable in the thermodynamic limit.">"Machine learning phase diagram in the half-filled one-dimensional
  extended Hubbard model"</a>,
Kazuya Shinjo, Kakeru Sasaki, Satoru Hase, Shigetoshi Sota, Satoshi Ejima, Seiji Yunoki, Takami Tohyama,
arXiv: <a href="http://arxiv.org/abs/1904.06032v1">1904.06032</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.05903v1" title="We introduce two methods for estimating the density matrix for a quantum
system: Quantum Maximum Likelihood and Quantum Variational Inference. In these
methods, we construct a variational family to model the density matrix of a
mixed quantum state. We also introduce quantum flows, the quantum analog of
normalizing flows, which can be used to increase the expressivity of this
variational family. The eigenstates and eigenvalues of interest are then
derived by optimizing an appropriate loss function. The approach is
qualitatively different than traditional lattice techniques that rely on the
time dependence of correlation functions that summarize the lattice
configurations. The resulting estimate of the density matrix can then be used
to evaluate the expectation of an arbitrary operator, which opens the door to
new possibilities.">"Inferring the quantum density matrix with machine learning"</a>,
Kyle Cranmer, Siavash Golkar, Duccio Pappadopulo,
arXiv: <a href="http://arxiv.org/abs/1904.05903v1">1904.05903</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.05902v2" title="Quantum tomography is currently ubiquitous for testing any implementation of
a quantum information processing device. Various sophisticated procedures for
state and process reconstruction from measured data are well developed and
benefit from precise knowledge of the model describing state preparation and
the measurement apparatus. However, physical models suffer from intrinsic
limitations as actual measurement operators and trial states cannot be known
precisely. This scenario inevitably leads to state-preparation-and-measurement
(SPAM) errors degrading reconstruction performance. Here we develop and
experimentally implement a machine learning based protocol reducing SPAM
errors. We trained a supervised neural network to filter the experimental data
and hence uncovered salient patterns that characterize the measurement
probabilities for the original state and the ideal experimental apparatus free
from SPAM errors. We compared the neural network state reconstruction protocol
with a protocol treating SPAM errors by process tomography, as well as to a
SPAM-agnostic protocol with idealized measurements. The average reconstruction
fidelity is shown to be enhanced by 10\% and 27\%, respectively. The presented
methods apply to the vast range of quantum experiments which rely on
tomography.">"Experimental neural network enhanced quantum tomography"</a>,
Adriano Macarone Palmieri, Egor Kovlakov, Federico Bianchi, Dmitry Yudin, Stanislav Straupe, Jacob Biamonte, Sergei Kulik,
arXiv: <a href="http://arxiv.org/abs/1904.05902v2">1904.05902</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.05067v1" title="In many dynamical probes of a quantum system, quite often multiple eigenmodes
are excited. Therefore, the experimental data can be quite messy due to the
mixing of different modes, as well as the background noise, despite that each
mode manifests itself as a single frequency oscillation. Here we develop an
unsupervised machine learning algorithm to extract the frequencies of these
oscillations from such measurement data, that is, the eigenenergies of these
modes. This method is particularly useful when the measurement time is not long
enough to perform the Fourier transformation. Our method is inspired by the
independent component analysis method and its application to the cocktail party
problem. In that problem, the goal is to recover each voice from detectors that
detect signals of many mixed voices, and the principle is to find out signals
that possess features and are away from a Gaussian distribution. Instead, our
generalization is to find out signals that are close to a single frequency
oscillation. We demonstrate the advantage of our method by an example of
analyzing the collective mode of cold atoms. We believe this method can find
broad applications in analyzing data from dynamical experiments in quantum
systems.">"Generalized Independent Component Analysis for Extracting Eigen-Modes of
  a Quantum System"</a>,
Yadong Wu, Hui Zhai,
arXiv: <a href="http://arxiv.org/abs/1904.05067v1">1904.05067</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.04712v1" title="Machine learning techniques based on artificial neural networks have been
successfully applied to solve many problems in science. One of the most
interesting domains of machine learning, reinforcement learning, has natural
applicability for optimization problems in physics. In this work we use deep
reinforcement learning and Chopped Random Basis optimization, to solve an
optimization problem based on the insertion of an off-center barrier in a
quantum Szilard engine. We show that using designed protocols for the time
dependence of the barrier strength, we can achieve an equal splitting of the
wave function (1/2 probability to find the particle on either side of the
barrier) even for an asymmetric Szilard engine in such a way that no
information is lost when measuring which side the particle is found. This
implies that the asymmetric non-adiabatic Szilard engine can operate with the
same efficiency as the traditional Szilard engine, with adiabatic insertion of
a central barrier. We compare the two optimization methods, and demonstrate the
advantage of reinforcement learning when it comes to constructing robust and
noise-resistant protocols.">"Deep reinforcement learning for robust quantum optimization"</a>,
Vegard B. Sørdal, Joakim Bergli,
arXiv: <a href="http://arxiv.org/abs/1904.04712v1">1904.04712</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.02467v1" title="We develop an autonomous agent intended for interacting with dynamic
environment that is a noisy quantum computer and to approximate a ground state
of a spin Hamiltonian. During reinforcement learning on quantum simulator
including a noise generator our multi-neural-network agent develops different
strategies (from passive to active) depending on a random initial state and
length of the quantum circuit. In contrast to variational and phase estimation
methods proposed previously for simulating spin Hamiltonians on a quantum
computer we do not use quantum circuits with fixed sequences of gates. In our
approach a neural-network agent forms a quantum circuit in such a way to
decrease the estimated energy of the modelled system even in the presence of
decoherence and gate imperfection leading to fluctuations of measurements
results. The agent's performance is demonstrated on quantum simulator and real
quantum computer accessible via IBM Q cloud service.">"Neural network agent playing spin Hamiltonian games on a quantum
  computer"</a>,
Oleg M. Sotnikov, Vladimir V. Mazurenko,
arXiv: <a href="http://arxiv.org/abs/1904.02467v1">1904.02467</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.04058v1" title="We investigate the use of discrete and continuous versions of
physics-informed neural network methods for learning unknown dynamics or
constitutive relations of a dynamical system. For the case of unknown dynamics,
we represent all the dynamics with a deep neural network (DNN). When the
dynamics of the system are known up to the specification of constitutive
relations (that can depend on the state of the system), we represent these
constitutive relations with a DNN. The discrete versions combine classical
multistep discretization methods for dynamical systems with neural network
based machine learning methods. On the other hand, the continuous versions
utilize deep neural networks to minimize the residual function for the
continuous governing equations. We use the case of a fedbatch bioreactor system
to study the effectiveness of these approaches and discuss conditions for their
applicability. Our results indicate that the accuracy of the trained neural
network models is much higher for the cases where we only have to learn a
constitutive relation instead of the whole dynamics. This finding corroborates
the well-known fact from scientific computing that building as much structural
information is available into an algorithm can enhance its efficiency and/or
accuracy.">"A comparative study of physics-informed neural network models for
  learning unknown dynamics and constitutive relations"</a>,
Ramakrishna Tipireddy, Paris Perdikaris, Panos Stinis, Alexandre Tartakovsky,
arXiv: <a href="http://arxiv.org/abs/1904.04058v1">1904.04058</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.01486v1" title="The classification of phase transitions is a central and challenging task in
condensed matter physics. Typically, it relies on the identification of order
parameters and the analysis of singularities in the free energy and its
derivatives. Here, we propose an alternative framework to identify quantum
phase transitions, employing both unsupervised and supervised machine learning
techniques. Using the axial next-nearest neighbor Ising (ANNNI) model as a
benchmark, we show how unsupervised learning can detect three phases
(ferromagnetic, paramagnetic, and a cluster of the antiphase with the floating
phase) as well as two distinct regions within the paramagnetic phase. Employing
supervised learning we show that transfer learning becomes possible: a machine
trained only with nearest-neighbour interactions can learn to identify a new
type of phase occurring when next-nearest-neighbour interactions are
introduced. All our results rely on few and low dimensional input data (up to
twelve lattice sites), thus providing a computational friendly and general
framework for the study of phase transitions in many-body systems.">"Unveiling phase transitions with machine learning"</a>,
Askery Canabarro, Felipe Fernandes Fanchini, André Luiz Malvezzi, Rodrigo Pereira, Rafael Chaves,
arXiv: <a href="http://arxiv.org/abs/1904.01486v1">1904.01486</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.01305v2" title="Sampling complex free energy surfaces is one of the main challenges of modern
atomistic simulation methods. The presence of kinetic bottlenecks in such
surfaces often renders a direct approach useless. A popular strategy is to
identify a small number of key collective variables and to introduce a bias
potential that is able to favor their fluctuations in order to accelerate
sampling. Here we propose to use machine learning techniques in conjunction
with the recent variationally enhanced sampling method [Valsson and Parrinello,
Physical Review Letters 2014] to determine such potential. This is achieved by
expressing the bias as a neural network. The parameters are determined in a
reinforcement learning scheme aimed at minimizing the variationally enhanced
sampling functional. This required the development of a new and more efficient
minimization technique. The expressivity of neural networks allows accelerating
sampling in systems with rapidly varying free energy surfaces, removing
boundary effects artifacts, and making one more step towards being able to
handle several collective variables.">"Neural networks based variationally enhanced sampling"</a>,
Luigi Bonati, Yue-Yu Zhang, Michele Parrinello,
arXiv: <a href="http://arxiv.org/abs/1904.01305v2">1904.01305</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.00031v1" title="We introduce NetKet, a comprehensive open source framework for the study of
many-body quantum systems using machine learning techniques. The framework is
built around a general and flexible implementation of neural-network quantum
states, which are used as a variational ansatz for quantum wave functions.
NetKet provides algorithms for several key tasks in quantum many-body physics
and quantum technology, namely quantum state tomography, supervised learning
from wave-function data, and ground state searches for a wide range of
customizable lattice models. Our aim is to provide a common platform for open
research and to stimulate the collaborative development of computational
methods at the interface of machine learning and many-body physics.">"NetKet: A Machine Learning Toolkit for Many-Body Quantum Systems"</a>,
Giuseppe Carleo, Kenny Choo, Damian Hofmann, James E. T. Smith, Tom Westerhout, Fabien Alet, Emily J. Davis, Stavros Efthymiou, Ivan Glasser, Sheng-Hsuan Lin, Marta Mauri, Guglielmo Mazzola, Christian B. Mendl, Evert van Nieuwenburg, Ossian O'Reilly, Hugo Théveniaut, Giacomo Torlai, Alexander Wietek,
arXiv: <a href="http://arxiv.org/abs/1904.00031v1">1904.00031</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.11714v2" title="Large scale deep neural networks profited from an emerging class of AI
accelerators. Although the accelerators are specialized for machine learning,
some of their designs are general enough for other computing intensive
applications. Cloud TPU, as one of them, offers tremendous computing resources
and is easily accessible through TensorFlow by expressing the computation in a
graph. In this paper, we leverage this powerful hardware combined with the
expressiveness of TensorFlow to simulate the Ising model on a \(2\)-dimensional
lattice. We modify the computationally intensive part of the checkerboard
algorithm into matrix operations to exploit Cloud TPU's highly efficient matrix
unit. In our experiments, we demonstrate that our implementation outperforms
the best published benchmarks to our knowledge by 60% in single core and 250%
in multiple cores with linear scaling. We also show the performance improvement
of using low precision arithmetic---bfloat16 instead of float32---without
sacrificing any accuracy.">"High Performance Monte Carlo Simulation of Ising Model on TPU Clusters"</a>,
Kun Yang, Yi-Fan Chen, Georgios Roumpos, Chris Colby, John Anderson,
arXiv: <a href="http://arxiv.org/abs/1903.11714v2">1903.11714</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.10563v1" title="Machine learning encompasses a broad range of algorithms and modeling tools
used for a vast array of data processing tasks, which has entered most
scientific disciplines in recent years. We review in a selective way the recent
research on the interface between machine learning and physical sciences.This
includes conceptual developments in machine learning (ML) motivated by physical
insights, applications of machine learning techniques to several domains in
physics, and cross-fertilization between the two fields. After giving basic
notion of machine learning methods and principles, we describe examples of how
statistical physics is used to understand methods in ML. We then move to
describe applications of ML methods in particle physics and cosmology, quantum
many body physics, quantum computing, and chemical and material physics. We
also highlight research and development into novel computing architectures
aimed at accelerating ML. In each of the sections we describe recent successes
as well as domain-specific methodology and challenges.">"Machine learning and the physical sciences"</a>,
Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, Lenka Zdeborová,
arXiv: <a href="http://arxiv.org/abs/1903.10563v1">1903.10563</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.10474v1" title="Complex networks can be modeled as a probabilistic graphical model, where the
interactions between binary variables, &quot;spins&quot;, on nodes are described by a
coupling matrix that is inferred from observations. The inverse statistical
problem of finding direct interactions is difficult, especially for large
systems, because of the exponential growth in the possible number of states and
the possible number of networks. In the context of the experimental sciences,
well-controlled perturbations can be applied to a system, shedding light on the
internal structure of the network. Therefore, we propose a method to improve
the accuracy and efficiency of inference by iteratively applying perturbations
to a network that are advantageous under a Bayesian framework. The spectrum of
the empirical Fisher information can be used as a measure for the difficulty of
the inference during the training process. We significantly improve the
accuracy and efficiency of inference in medium-sized networks based on this
strategy with a reasonable number of experimental queries. Our method could be
powerful in the analysis of complex networks as well as in the rational design
of experiments.">"Active Learning of Spin Network Models"</a>,
Jialong Jiang, David A. Sivak, Matt Thomson,
arXiv: <a href="http://arxiv.org/abs/1903.10474v1">1903.10474</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.08543v1" title="Using a model heat engine we show that neural network-based reinforcement
learning can identify thermodynamic trajectories of maximal efficiency. We use
an evolutionary learning algorithm to evolve a population of neural networks,
subject to a directive to maximize the efficiency of a trajectory composed of a
set of elementary thermodynamic processes; the resulting networks learn to
carry out the maximally-efficient Carnot, Stirling, or Otto cycles. Given
additional irreversible processes this evolutionary scheme learns a hitherto
unknown thermodynamic cycle. Our results show how the reinforcement learning
strategies developed for game playing can be applied to solve physical problems
conditioned upon path-extensive order parameters.">"Optimizing thermodynamic trajectories using evolutionary reinforcement
  learning"</a>,
Chris Beeler, Uladzimir Yahorau, Rory Coles, Kyle Mills, Stephen Whitelam, Isaac Tamblyn,
arXiv: <a href="http://arxiv.org/abs/1903.08543v1">1903.08543</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.08061v2" title="We investigate theoretically the phase transition in three dimensional cubic
Ising model utilizing state-of-the-art machine learning algorithms. Supervised
machine learning models show high accuracies (~99\%) in phase classification
and very small relative errors (\(&lt; 10^{-4}\)) of the energies in different spin
configurations. Unsupervised machine learning models are introduced to study
the spin configuration reconstructions and reductions, and the phases of
reconstructed spin configurations can be accurately classified by a linear
logistic algorithm. Based on the comparison between various machine learning
models, we develop a few-shot strategy to predict phase transitions in larger
lattices from trained sample in smaller lattices. The few-shot machine learning
strategy for three dimensional(3D) Ising model enable us to study 3D ising
model efficiently and provides a new integrated and highly accurate approach to
other spin models.">"Few-shot machine learning in the three-dimensional Ising model"</a>,
Rui Zhang, Bin Wei, Dong Zhang, Jia-Ji Zhu, Kai Chang,
arXiv: <a href="http://arxiv.org/abs/1903.08061v2">1903.08061</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.08060v2" title="Experimental data are the source of understanding matter. However, measurable
quantities are limited and theoretically important quantities are often hidden.
Nonetheless, recent progress of machine-learning techniques opens possibilities
of exposing them only from available experimental data. In this article, the
Boltzmann-machine method is applied to the angle-resolved photoemission
spectroscopy spectra of cuprate superconductors. We find prominent peak
structures both in normal and anomalous self-energies, but they cancel in the
total self-energy making the structure apparently invisible, while the peaks
make dominant contributions to superconducting gap, hence providing a decisive
testimony for the origin of superconductivity. The relation between superfluid
density and critical temperature supports involvement of universal carrier
relaxation time associated with dissipative strange metals. The present
achievement opens avenues for innovative machine-learning spectroscopy method.">"Hidden self-energies as origin of cuprate superconductivity revealed by
  machine learning"</a>,
Youhei Yamaji, Teppei Yoshida, Atsushi Fujimori, Masatoshi Imada,
arXiv: <a href="http://arxiv.org/abs/1903.08060v2">1903.08060</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.06993v1" title="We introduce the use of neural networks as classifiers on classical
disordered systems with no spatial ordering. In this study, we implement a
convolutional neural network trained to identify the spin-glass state in the
three-dimensional Edwards-Anderson Ising spin-glass model from an input of
Monte Carlo sampled configurations at a given temperature. The neural network
is designed to be flexible with the input size and can accurately perform
inference over a small sample of the instances in the test set. Using the
neural network to classify instances of the three-dimensional Edwards-Anderson
Ising spin-glass in a (random) field we show that the inferred phase boundary
is consistent with the absence of an Almeida-Thouless line.">"Learning to find order in disorder"</a>,
Humberto Munoz-Bauza, Firas Hamze, Helmut G. Katzgraber,
arXiv: <a href="http://arxiv.org/abs/1903.06993v1">1903.06993</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.06713v1" title="The use of artificial neural networks to represent quantum wave-functions has
recently attracted interest as a way to solve complex many-body problems. The
potential of these variational parameterizations has been supported by
analytical and numerical evidence in controlled benchmarks. While approaching
the end of the early research phase in this field, it becomes increasingly
important to show how neural-network states perform for models and physical
problems that constitute a clear open challenge for other many-body
computational methods. In this paper we start addressing this aspect,
concentrating on a presently unsolved model describing two-dimensional
frustrated magnets. Using a fully convolutional neural network model as a
variational ansatz, we study the frustrated spin-1/2 J1-J2 Heisenberg model on
the square lattice. We demonstrate that the resulting predictions for both
ground-state energies and properties are competitive with, and often improve
upon, existing state-of-the-art methods. In a relatively small region in the
parameter space, corresponding to the maximally frustrated regime, our ansatz
exhibits comparatively good but not best performance. The gap between the
complexity of the models adopted here and those routinely adopted in deep
learning applications is, however, still substantial, such that further
improvements in future generations of neural-network quantum states are likely
to be expected.">"Study of the Two-Dimensional Frustrated J1-J2 Model with Neural Network
  Quantum States"</a>,
Kenny Choo, Titus Neupert, Giuseppe Carleo,
arXiv: <a href="http://arxiv.org/abs/1903.06713v1">1903.06713</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.04698v1" title="We present a deep reinforcement learning framework where a machine agent is
trained to search for a policy to generate a ground state for the square ice
model by exploring the physical environment. After training, the agent is
capable of proposing a sequence of local moves to achieve the goal. Analysis of
the trained policy and the state value function indicates that the ice rule and
loop-closing condition are learned without prior knowledge. We test the trained
policy as a sampler in the Markov chain Monte Carlo and benchmark against the
baseline loop algorithm. This framework can be generalized to other models with
topological constraints where generation of constraint-preserving states is
difficult.">"Generation of ice states through deep reinforcement learning"</a>,
Kai-Wen Zhao, Wen-Han Kao, Kai-Hsin Wu, Ying-Jer Kao,
arXiv: <a href="http://arxiv.org/abs/1903.04698v1">1903.04698</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.02175v1" title="Machine learning technologies are expected to be great tools for scientific
discoveries. In particular, materials development (which has brought a lot of
innovation by finding new and better functional materials) is one of the most
attractive scientific fields. To apply machine learning to actual materials
development, collaboration between scientists and machine learning is becoming
inevitable. However, such collaboration has been restricted so far due to black
box machine learning, in which it is difficult for scientists to interpret the
data-driven model from the viewpoint of material science and physics. Here, we
show a material development success story that was achieved by good
collaboration between scientists and one type of interpretable (explainable)
machine learning called factorized asymptotic Bayesian inference hierarchical
mixture of experts (FAB/HMEs). Based on material science and physics, we
interpreted the data-driven model constructed by the FAB/HMEs, so that we
discovered surprising correlation and knowledge about thermoelectric material.
Guided by this, we carried out actual material synthesis that led to
identification of a novel spin-driven thermoelectric material with the largest
thermopower to date.">"Materials development by interpretable machine learning"</a>,
Yuma Iwasaki, Ryoto Sawada, Valentin Stanev, Masahiko Ishida, Akihiro Kirihara, Yasutomo Omori, Hiroko Someya, Ichiro Takeuchi, Eiji Saitoh, Yorozu Shinichi,
arXiv: <a href="http://arxiv.org/abs/1903.02175v1">1903.02175</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.02146v1" title="Quantum steering is an important nonclassical resource for quantum
information processing. However, even lots of steering criteria exist, it is
still very difficult to efficiently determine whether an arbitrary two-qubit
state shared by Alice and Bob is steerable or not, because the optimal
measurement directions of Alice are unknown. In this work, we provide an
efficient quantum steering detection scheme for arbitrary 2-qubit states with
the help of machine learning, where Alice and Bob only need to measure in a few
fixed measurement directions. In order to prove the validity of this method, we
firstly realize a high performance quantum steering classifier with the whole
information. Furthermore, a high performance quantum steering classifier with
partial information is realized, where Alice and Bob only need to measure in
three fixed measurement directions. Our method outperforms the existing methods
in generic cases in terms of both speed and accuracy, opening up the avenues to
explore quantum steering via the machine learning approach.">"Steerability detection of arbitrary 2-qubit state via machine learning"</a>,
Changliang Ren, Changbo Chen,
arXiv: <a href="http://arxiv.org/abs/1903.02146v1">1903.02146</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.03516v1" title="The marriage of machine learning and quantum physics may give birth to a new
research frontier that could transform both.">"Machine learning meets quantum physics"</a>,
Sankar Das Sarma, Dong-Ling Deng, Lu-Ming Duan,
arXiv: <a href="http://arxiv.org/abs/1903.03516v1">1903.03516</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.00804v2" title="The exact holographic mapping (EHM) provides an explicit duality map between
a conformal field theory (CFT) configuration and a massive field propagating on
an emergent classical geometry. However, designing the optimal holographic
mapping is challenging. Here we introduce the neural network renormalization
group as a universal approach to design generic EHM for interacting field
theories. Given a field theory action that flows to CFT, we train a flow-based
hierarchical deep generative neural network to reproduce the boundary field
ensemble from uncorrelated bulk field fluctuations. In this way, the neural
network develops the optimal renormalization group transformations. Using the
machine-designed EHM to map the CFT back to a bulk effective action, we
determine the bulk geodesic distance from the residual mutual information. We
apply this approach to the complex \(\phi^4\) theory in two-dimensional Euclidian
spacetime in its critical phase, and show that the emergent bulk geometry
matches the three-dimensional hyperbolic geometry.">"Machine Learning Holographic Mapping by Neural Network Renormalization
  Group"</a>,
Hong-Ye Hu, Shuo-Hui Li, Lei Wang, Yi-Zhuang You,
arXiv: <a href="http://arxiv.org/abs/1903.00804v2">1903.00804</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.10104v1" title="We present a general variational approach to determine the steady state of
open quantum lattice systems via a neural network approach. The steady-state
density matrix of the lattice system is constructed via a purified neural
network ansatz in an extended Hilbert space with ancillary degrees of freedom.
The variational minimization of cost functions associated to the master
equation can be performed using a Markov chain Monte Carlo sampling. As a first
application and proof-of-principle, we apply the method to the dissipative
quantum transverse Ising model.">"Variational neural network ansatz for steady states in open quantum
  systems"</a>,
Filippo Vicentini, Alberto Biella, Nicolas Regnault, Cristiano Ciuti,
arXiv: <a href="http://arxiv.org/abs/1902.10104v1">1902.10104</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.09483v1" title="The possibility to simulate the properties of many-body open quantum systems
with a large number of degrees of freedom is the premise to the solution of
several outstanding problems in quantum science and quantum information. The
challenge posed by this task lies in the complexity of the density matrix
increasing exponentially with the system size. Here, we develop a variational
method to efficiently simulate the non-equilibrium steady state of Markovian
open quantum systems based on variational Monte Carlo and on a neural network
representation of the density matrix. Thanks to the stochastic reconfiguration
scheme, the application of the variational principle is translated into the
actual integration of the quantum master equation. We test the effectiveness of
the method by modeling the two-dimensional dissipative XYZ spin model on a
lattice.">"Variational quantum Monte Carlo with neural network ansatz for open
  quantum systems"</a>,
Alexandra Nagy, Vincenzo Savona,
arXiv: <a href="http://arxiv.org/abs/1902.09483v1">1902.09483</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.09216v1" title="Understanding the properties of quantum matter is an outstanding challenge in
science. In this work, we demonstrate how machine learning methods can be
successfully applied for the classification of various regimes in
single-particle and many-body systems. We realize neural network algorithms
that perform a classification between regular and chaotic behavior in quantum
billiard models with remarkably high accuracy. By taking this method further,
we show that machine learning techniques allow to pin down the transition from
integrability to many-body quantum chaos in Heisenberg XXZ spin chains. Our
results pave the way for exploring the power of machine learning tools for
revealing exotic phenomena in complex quantum many-body systems.">"Revealing quantum chaos with machine learning"</a>,
Y. A. Kharkov, V. E. Sotskov, A. A. Karazeev, E. O. Kiktenko, A. K. Fedorov,
arXiv: <a href="http://arxiv.org/abs/1902.09216v1">1902.09216</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.08418v1" title="How to implement multi-qubit gates efficiently with high precision is
essential for realizing universal fault tolerant computing. For a physical
system with some external controllable parameters, it is a great challenge to
control the time dependence of these parameters to achieve a target multi-qubit
gate efficiently and precisely. Here we construct a dueling double deep
Q-learning neural network (DDDQN) to find out the optimized time dependence of
controllable parameters to implement two typical quantum gates: a single-qubit
Hadamard gate and a two-qubit CNOT gate. Compared with traditional optimal
control methods, this deep reinforcement learning method can realize efficient
and precise gate control without requiring any gradient information during the
learning process. This work attempts to pave the way to investigate more
quantum control problems with deep reinforcement learning techniques.">"Deep Reinforcement Learning for Quantum Gate Control"</a>,
Zheng An, D. L. Zhou,
arXiv: <a href="http://arxiv.org/abs/1902.08418v1">1902.08418</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.07754v2" title="Designing and implementing algorithms for medium and large scale quantum
computers is not easy. In previous work we have suggested, and developed, the
idea of using machine learning techniques to train a quantum system such that
the desired process is &quot;learned,&quot; thus obviating the algorithm design
difficulty. This works quite well for small systems. But the goal is
macroscopic physical computation. Here, we implement our learned pairwise
entanglement witness on Microsoft's Q#, one of the commercially available gate
model quantum computer simulators; we perform statistical analysis to determine
reliability and reproduceability; and we show that using the machine learning
technique called &quot;bootstrapping&quot;, we can infer the pattern for mesoscopic N
from simulation results for three-, four-, five-, six-, and seven-qubit
systems. Our results suggest a fruitful pathway for general quantum computer
algorithm design and computation.">"Experimental pairwise entanglement estimation for an N-qubit system :A
  machine learning approach for programming quantum hardware"</a>,
N. L. Thompson, N. H. Nguyen, E. C. Behrman, J. E. Steck,
arXiv: <a href="http://arxiv.org/abs/1902.07754v2">1902.07754</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.08577v1" title="In quantum many-body problems, one of the main difficulties comes from the
description of non-negligible interactions which require, at least in
principle, an exponential amount of information. Recently, in the context of
spin glasses and Boltzmann machines, it has been demonstrated that systematic
machine learning of the wave function can reduce these issues to a tractable
computational problem. In this work, we apply this approach to a different
situation, i.e. the problem of finding the ground state of a given quantum
system made of electrons, entirely described by its Hamiltonian operator, and
by utilizing feedforward neural networks. Although still in the shape of a
proof of concept, one can already observe that this seminal idea is able to
substantially simplify the complexity of this peculiar, and important, problem.">"Could the Hilbert Space Be a Smaller Place? A Neural Network Perspective"</a>,
Jean Michel Sellier,
arXiv: <a href="http://arxiv.org/abs/1902.08577v1">1902.08577</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.07019v2" title="Machine learning methods have proved to be useful for the recognition of
patterns in statistical data. The measurement outcomes are intrinsically random
in quantum physics, however they do have a pattern when the measurements are
performed successively on an open quantum system. This pattern is due to the
system-environment interaction and contains information about the relaxation
rates as well as non-Markovian memory effects. Here we develop a method to
extract the information about the unknown environment from a series of
single-shot measurements on the system (without resorting to the process
tomography). The method is based on embedding the non-Markovian system dynamics
into a Markovian dynamics of the system and the effective reservoir of finite
dimension. The generator of Markovian embedding is learned by the maximum
likelihood estimation. We verify the method by comparing its prediction with an
exactly solvable non-Markovian dynamics. The developed algorithm to learn
unknown quantum environments enables one to efficiently control and manipulate
quantum systems.">"Machine learning non-Markovian quantum dynamics"</a>,
I. A. Luchnikov, S. V. Vintskevich, D. A. Grigoriev, S. N. Filippov,
arXiv: <a href="http://arxiv.org/abs/1902.07019v2">1902.07019</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.07006v3" title="We propose a new variational scheme based on the neural-network quantum
states to simulate the stationary states of open quantum many-body systems.
Using the high expressive power of the variational ansatz described by the
restricted Boltzmann machines, which we dub as the neural stationary state
ansatz, we compute the stationary states of quantum dynamics obeying the
Lindblad master equations. The mapping of the stationary-state search problem
into finding a zero-energy ground state of an appropriate Hermitian operator
allows us to apply the conventional variational Monte Carlo method for the
optimization. Our method is shown to simulate various spin systems efficiently,
i.e., the transverse-field Ising models in both one and two dimensions and the
XYZ model in one dimension.">"Constructing neural stationary states for open quantum many-body systems"</a>,
Nobuyuki Yoshioka, Ryusuke Hamazaki,
arXiv: <a href="http://arxiv.org/abs/1902.07006v3">1902.07006</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.05131v1" title="In experimentally realistic situations, quantum systems are never perfectly
isolated and the coupling to their environment needs to be taken into account.
Often, the effect of the environment can be well approximated by a Markovian
master equation. However, solving this master equation for quantum many-body
systems, becomes exceedingly hard due to the high dimension of the Hilbert
space. Here we present an approach to the effective simulation of the dynamics
of open quantum many-body systems based on machine learning techniques. We
represent the mixed many-body quantum states with neural networks in the form
of restricted Boltzmann machines and derive a variational Monte-Carlo algorithm
for their time evolution and stationary states. We document the accuracy of the
approach with numerical examples for a dissipative spin lattice system.">"Neural-Network Approach to Dissipative Quantum Many-Body Dynamics"</a>,
Michael J. Hartmann, Giuseppe Carleo,
arXiv: <a href="http://arxiv.org/abs/1902.05131v1">1902.05131</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.04079v1" title="While the non-perturbative interaction effects in the fractional quantum Hall
regime can be readily simulated through exact diagonalization, it has been
challenging to establish a suitable diagnostic that can label different phases
in the presence of competing interactions and disorder. Here we introduce a
multi-faceted framework using a simple artificial neural network (ANN) to
detect defining features of a fractional quantum Hall state, a charge density
wave state and a localized state using the entanglement spectra and charge
density as independent input. We consider the competing effects of a perturbing
interaction (\(l = 1\) pseudopotential \(\Delta V_1\)), a disorder potential \(W\),
and the Coulomb interaction to the system at filling fraction \({\nu} = 1/3\).
Our phase diagram benchmarks well against previous estimates of the phase
boundary using conventional measures along the \(\Delta V_1 = 0\) and \(W = 0\)
axes, the only regions where conventional approaches have been explored.
Moreover, exploring the entire two-dimensional phase diagram for the first
time, we establish the robustness of the fractional quantum Hall state and map
out the charge density wave micro-emulsion phase wherein droplets of charge
density wave region appear before the charge density wave is completely
disordered. Hence we establish that the ANN can access and learn the defining
traits of topological as well as broken symmetry phases using multi-faceted
inputs of entanglement spectra and charge density.">"Multi-faceted machine learning of competing orders in disordered
  interacting systems"</a>,
Michael Matty, Yi Zhang, Zlatko Papic, Eun-Ah Kim,
arXiv: <a href="http://arxiv.org/abs/1902.04079v1">1902.04079</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.04057v2" title="Artificial Neural Networks were recently shown to be an efficient
representation of highly-entangled many-body quantum states. In practical
applications, neural-network states inherit numerical schemes used in
Variational Monte Carlo, most notably the use of Markov-Chain Monte-Carlo
(MCMC) sampling to estimate quantum expectations. The local stochastic sampling
in MCMC caps the potential advantages of neural networks in two ways: (i) Its
intrinsic computational cost sets stringent practical limits on the width and
depth of the networks, and therefore limits their expressive capacity; (ii) Its
difficulty in generating precise and uncorrelated samples can result in
estimations of observables that are very far from their true value. Inspired by
the state-of-the-art generative models used in machine learning, we propose a
specialized Neural Network architecture that supports efficient and exact
sampling, completely circumventing the need for Markov Chain sampling. We
demonstrate our approach for two-dimensional interacting spin models,
showcasing the ability to obtain accurate results on larger system sizes than
those currently accessible to neural-network quantum states.">"Deep autoregressive models for the efficient variational simulation of
  many-body quantum systems"</a>,
Or Sharir, Yoav Levine, Noam Wies, Giuseppe Carleo, Amnon Shashua,
arXiv: <a href="http://arxiv.org/abs/1902.04057v2">1902.04057</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.02941v1" title="By applying a machine learning algorithm to extrapolations and the numerical
differentiations, we propose a method to obtain a continuous magnetization
curve out of discrete energy data. It gives an expression for the spin gap,
which converges faster to the thermodynamic limit. We check its validity for an
exactly solvable one-dimensional spin model and apply it to the kagome
antiferromagnet. Results of the kagome antiferromagnet obtained by the
exact-diagonalization data up to 30 sites were comparable to the DMRG results
for the 132 sites.">"Machine learning as an improved estimator for magnetization curve and
  spin gap"</a>,
Tota Nakamura,
arXiv: <a href="http://arxiv.org/abs/1902.02941v1">1902.02941</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.02354v1" title="A fundamental question in deep learning concerns the role played by
individual layers in a deep neural network (DNN) and the transferable
properties of the data representations which they learn. To the extent that
layers have clear roles one should be able to optimize them separately using
layer-wise loss functions. Such loss functions would describe what is the set
of good data representations at each depth of the network and provide a target
for layer-wise greedy optimization (LEGO). Here we introduce the Deep Gaussian
Layer-wise loss functions (DGLs) which, we believe, are the first supervised
layer-wise loss functions which are both explicit and competitive in terms of
accuracy. The DGLs have a solid theoretical foundation, they become exact for
wide DNNs, and we find that they can monitor standard end-to-end training.
Being highly structured and symmetric, the DGLs provide a promising analytic
route to understanding the internal representations generated by DNNs.">"The role of a layer in deep neural networks: a Gaussian Process
  perspective"</a>,
Oded Ben-David, Zohar Ringel,
arXiv: <a href="http://arxiv.org/abs/1902.02354v1">1902.02354</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.01845v1" title="Here, we consider a simple problem, demonstrating that neural networks can be
successfully used to give new insights in statistical physics. Specifically, we
consider 3D lattice dimer model, which consists of sites forming a lattice and
bonds connecting the neighboring sites, in such a way that every bond can be
either empty of filled with a dimer, and the total number of dimers ending at
one site is fixed to be one. Dimer configurations can be viewed as equivalent
if they are connected through a series of local flips, i.e. simultaneous
'rotation' of a pair of parallel neighboring dimers. It turns out that the
whole set of dimer configurations on a given 3D lattice can be split into
distinct topological classes, such that dimer configurations belonging to
different classes are not equivalent. In this paper we identify these classes
by using neural networks. More specifically, we train the neural networks to
distinguish dimer configurations from two known topological classes, and after
it, we test them on dimer configurations from unknown topological classes. We
demonstrate that 3D lattice dimer model on a bipartite lattice can be described
by an integer topological invariant ('Hopf number'), whereas lattice dimer
model on a non-bipartite lattice is described by \(Z_2\) invariant. Thus, we
demonstrate that neural networks can be successfully used to identify new
topological phases in condensed matter systems, whose existence can be later
verified by other (e.g. analytical) techniques.">"Probing topological properties of 3D lattice dimer model with neural
  networks"</a>,
Grigory Bednik,
arXiv: <a href="http://arxiv.org/abs/1902.01845v1">1902.01845</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.11103v3" title="Can physical concepts and laws emerge in a neural network as it learns to
predict the observation data of physical systems? As a benchmark and a
proof-of-principle study of this possibility, here we show an introspective
learning architecture that can automatically develop the concept of the quantum
wave function and discover the Schr\&quot;odinger equation from simulated
experimental data of the potential-to-density mappings of a quantum particle.
This introspective learning architecture contains a machine translator to
perform the potential to density mapping, and a knowledge distiller
auto-encoder to extract the essential information and its update law from the
hidden states of the translator, which turns out to be the quantum wave
function and the Schr\&quot;odinger equation. We envision that our introspective
learning architecture can enable machine learning to discover new physics in
the future.">"Emergent Quantum Mechanics in an Introspective Machine Learning
  Architecture"</a>,
Ce Wang, Hui Zhai, Yi-Zhuang You,
arXiv: <a href="http://arxiv.org/abs/1901.11103v3">1901.11103</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.10847v2" title="There has been a rise in decoding quantum error correction codes with neural
network based decoders, due to the good decoding performance achieved and
adaptability to any noise model. However, the main challenge is scalability to
larger code distances due to an exponential increase of the error syndrome
space. Note that, successfully decoding the surface code under realistic noise
assumptions will limit the size of the code to less than 100 qubits with
current neural network based decoders.
  Such a problem can be tackled by a distributed way of decoding, similar to
the Renormalization Group (RG) decoders. In this paper, we introduce a decoding
algorithm that combines the concept of RG decoding and neural network based
decoders. We tested the decoding performance under depolarizing noise with
noiseless error syndrome measurements for the rotated surface code and compared
against the Blossom algorithm and a neural network based decoder. We show that
similar level of decoding performance can be achieved between all tested
decoders while providing a solution to the scalability issues of neural network
based decoders.">"Decoding surface code with a distributed neural network based decoder"</a>,
Savvas Varsamopoulos, Koen Bertels, Carmen G. Almudever,
arXiv: <a href="http://arxiv.org/abs/1901.10847v2">1901.10847</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.08615v1" title="An artificial neural network (ANN) with the restricted Boltzmann machine
(RBM) architecture was recently proposed as a versatile variational quantum
many-body wave function. In this work we provide physical insights into the
performance of this ansatz. We uncover the connection between the structure of
RBM and perturbation series, which explains the excellent precision achieved by
RBM ansazt in certain simple models, demonstrated in the literature. Based on
this relation, we improve the numerical algorithm to achieve better performance
of RBM in cases where local minima complicate the convergence to the global
one. We introduce other classes of variational wave-functions, which are also
capable of reproducing the perturbative structure, and show that their
performance is comparable to that of RBM. Furthermore, we study the performance
of a few-layer RBM for approximating ground states of random,
translationally-invariant models in 1d, as well as random matrix-product states
(MPS). We find that the error in approximating such states exhibits a broad
distribution, and is largely determined by the entanglement properties of the
targeted state.">"Approximating power of machine-learning ansatz for quantum many-body
  states"</a>,
Artem Borin, Dmitry A. Abanin,
arXiv: <a href="http://arxiv.org/abs/1901.08615v1">1901.08615</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.07900v1" title="Photoinduced nonequilibrium states can provide new insights into the
dynamical properties in strongly correlated electron systems. One of the
typical and extensively studied systems is the half-filled one-dimensional
extended Hubbard model (1DEHM). Here, we propose that the supervised machine
learning (ML) can give useful information for characterizing the photoexcited
state in 1DEHM. Using entanglement spectra as a training dataset, we construct
the neural network. Judging from the trained network, we find that the quantum
state driven by driving pulse has bond-spin-density wave (BSDW) order for
\(U\lesssim2V\), where \(U\) (\(V\)) is the on-site (nearest-neighbor) Coulomb
interaction. We separately calculate the time evolution of order parameters and
find that the order parameters of BSDW are actually enhanced by photoexcitation
as predicted by ML. Interestingly, the enhancement of the BSDW order in the
photoexcited states for the 1DEHM has never been reported previously, despite
the extensive studies so far, thus demonstrating the advantage of ML to assist
characterizing photoexcited quantum states.">"Characterization of photoexcited states in the half-filled
  one-dimensional extended Hubbard model assisted by machine learning"</a>,
Kazuya Shinjo, Shigetoshi Sota, Seiji Yunoki, Takami Tohyama,
arXiv: <a href="http://arxiv.org/abs/1901.07900v1">1901.07900</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.05230v1" title="A probing scheme is considered with an accessible and controllable qubit,
used to probe an out-of equilibrium system consisting of a second qubit
interacting with an environment. Quantum spontaneous synchronization between
the probe and the system emerges in this model and, by tuning the probe
frequency, can occur both in-phase and in anti-phase. We analyze the capability
of machine learning in this probing scheme based on quantum synchronization. An
artificial neural network is used to infer, from a probe observable, main
dissipation features, such as the environment Ohmicity index. The efficiency of
the algorithm in the presence of some noise in the dataset is also considered.
We show that the performance in either classification and regression is
significantly improved due to the in/anti-phase synchronization transition.
This opens the way to the characterization of environments with arbitrary
spectral densities.">"Machine learning applied to quantum synchronization-assisted probing"</a>,
Gabriel Garau Estarellas, Gian Luca Giorgi, Miguel C. Soriano, Roberta Zambrini,
arXiv: <a href="http://arxiv.org/abs/1901.05230v1">1901.05230</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.03817v4" title="Recently, there have been many works on the deep learning of statistical
ensembles to determine the critical temperature of a possible phase transition.
We analyze the detailed structure of an optimized deep learning machine and
prove the basic equalities among the optimized machine parameters and the
physical quantities of the statistical system. According to these equalities,
we conclude that the bias parameters of the final full connection layer record
the free energy of the statistical system as a function of temperature. We
confirm these equalities in one- and two-dimensional Ising spin models and
actually demonstrate that the deep learning machine reveals the critical
temperature of the phase transition through the second difference of bias
parameters, which is equivalent to the specific heat. Our results disprove the
previous works claiming that the weight parameters of the full connection might
play a role of the order parameter such as the spin expectation.">"Logical Reasoning for Revealing the Critical Temperature through Deep
  Learning of Configuration Ensemble of Statistical Systems"</a>,
Ken-Ichi Aoki, Tatsuhiro Fujita, Tamao Kobayashi,
arXiv: <a href="http://arxiv.org/abs/1901.03817v4">1901.03817</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.03346v2" title="The continuous effort towards topological quantum devices calls for an
efficient and non-invasive method to assess the conformity of components in
different topological phases. Here, we show that machine learning paves the way
towards non-invasive topological quality control. To do so, we use a local
topological marker, able to discriminate between topological phases of
one-dimensional wires. The direct observation of this marker in solid state
systems is challenging, but we show that an artificial neural network can learn
to approximate it from the experimentally accessible local density of states.
Our method distinguishes different non-trivial phases, even for systems where
direct transport measurements are not available and for composite systems. This
new approach could find significant use in experiments, ranging from the study
of novel topological materials to high-throughput automated material design.">"Machine learning assisted measurement of local topological invariants"</a>,
Marcello D. Caio, Marco Caccin, Paul Baireuther, Timo Hyart, Michel Fruchart,
arXiv: <a href="http://arxiv.org/abs/1901.03346v2">1901.03346</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.01501v1" title="An acceleration of continuous time quantum Monte Carlo (CTQMC) methods is a
potentially interesting branch of work as they are matchless as impurity
solvers of a density functional theory in combination with a dynamical mean
field theory approach for the description of electronic structures of strongly
correlated materials. The inversion of the \(k \times k\) matrix given by the
diagram expansion order \(k\) in the CTQMC update and the multiplication of the
\(k \times k\) matrix and the non-interacting Green's function to measure the
impurity Green's function are computationally time-consuming. Here, we propose
the CTQMC method in combination with a machine learning technique, which would
eliminate the need for multiplication of the matrix with the non-interacting
Green's function. This method predicts the accurate impurity Green's function
and double occupancy at low temperature, and also considers the physical
properties of high Matsubara frequency in a much shorter computational time
than the conventional CTQMC method.">"Accelerated Continuous time quantum Monte Carlo method with Machine
  Learning"</a>,
Taegeun Song, Hunpyo Lee,
arXiv: <a href="http://arxiv.org/abs/1901.01501v1">1901.01501</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.00524v1" title="We present an approach to numerical homogenization of the elastic response of
microstructures. Our work uses deep neural network representations trained on
data obtained from direct numerical simulation (DNS) of martensitic phase
transformations. The microscopic model leading to the microstructures is based
on non-convex free energy density functions that give rise to martensitic
variants, and must be extended to gradient theories of elasticity at finite
strain. These strain gradients introduce interfacial energies as well as
coercify the model, enabling the admission of a large number of solutions, each
having finely laminated microstructures. The numerical stiffness of these DNS
solutions and the fine scales of response make the data expensive to obtain,
while also motivating the search for homogenized representations of their
response for the purpose of engineering design. The high-dimensionality of the
problem is reduced by training deep neural networks (DNNs) on the effective
response by using the scalar free energy density data. The novelty in our
approach is that the trained DNNs also return high-fidelity representations of
derivative data, specifically the stresses. This allows the recapitulation of
the classic hyperelastic response of continuum elasticity via the DNN
representation. Also included are detailed optimization studies over
hyperparameters, and convergence with size of datasets.">"Machine learning materials physics: Deep neural networks trained on
  elastic free energy data from martensitic microstructures predict homogenized
  stress fields with high accuracy"</a>,
K. Sagiyama, K. Garikipati,
arXiv: <a href="http://arxiv.org/abs/1901.00524v1">1901.00524</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.00774v1" title="In this Letter, we present a new strategy for applying the learning machine
to study phase transitions. We train the learning machine with samples only
obtained at a non-critical parameter point, aiming to establish intrinsic
correlations between the learning machine and the target system. Then, we find
that the accuracy of the learning machine, which is the most important
performance index in conventional learning machines, is no longer a key goal of
the training in our approach. Instead, relatively low accuracy of identifying
unlabeled data category can help to determine the critical point with greater
precision, manifesting the singularity around the critical point. It thus
provides a robust tool to study the phase transition. The classical
ferromagnetic and percolation phase transitions are employed as illustrative
examples.">"A New Strategy in Applying the Learning Machine to Study Phase
  Transitions"</a>,
Rongxing Xu, Weicheng Fu, Hong Zhao,
arXiv: <a href="http://arxiv.org/abs/1901.00774v1">1901.00774</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.00091v2" title="Given the ubiquity of lattice models in physics, it is imperative for
researchers to possess robust methods for quantifying clusters on the lattice
--- whether they be Ising spins or clumps of molecules. Inspired by biophysical
studies, we present Python code for handling clusters on a 2D periodic lattice.
Properties of individual clusters, such as their area, can be obtained with a
few function calls. Our code invokes an unsupervised machine learning method
called hierarchical clustering, which is simultaneously effective for the
present problem and simple enough for non-experts to grasp qualitatively.
Moreover, our code transparently merges clusters neighboring each other across
periodic boundaries using breadth-first search (BFS), an algorithm
well-documented in computer science pedagogy. The fact that our code is written
in Python --- instead of proprietary languages --- further enhances its value
for reproducible science.">"Identifying Clusters on a Discrete Periodic Lattice via Machine Learning"</a>,
Everest Law,
arXiv: <a href="http://arxiv.org/abs/1901.00091v2">1901.00091</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.00081v2" title="The free energy of a system is central to many material models. Although free
energy data is not generally found directly, its derivatives can be observed or
calculated. In this work, we present an Integrable Deep Neural Network (IDNN)
that can be trained to derivative data, then analytically integrated to recover
an accurate representation of the free energy. The IDNN is demonstrated by
training to the chemical potential values of a binary alloy with B2 ordering.
The resulting DNN representation of the free energy is used in a phase field
simulation and found to predict the appropriate formation of antiphase
boundaries in the material. In contrast, a B-spline representation of the same
data failed to represent the physics of the system with sufficient fidelity to
resolve the antiphase boundaries.">"Machine learning materials physics: Integrable deep neural networks
  enable scale bridging by learning free energy functions"</a>,
G. H. Teichert, A. R. Natarajan, A. Van der Ven, K. Garikipati,
arXiv: <a href="http://arxiv.org/abs/1901.00081v2">1901.00081</a>,
1/2019</p>
</li>
</ul>
<h2>Physics-Inspired Ideas Applied to Machine Learning</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1905.04271v1" title="Sequence models assign probabilities to variable-length sequences such as
natural language texts. The ability of sequence models to capture temporal
dependence can be characterized by the temporal scaling of correlation and
mutual information. In this paper, we study the mutual information of recurrent
neural networks (RNNs) including long short-term memories and self-attention
networks such as Transformers. Through a combination of theoretical study of
linear RNNs and empirical study of nonlinear RNNs, we find their mutual
information decays exponentially in temporal distance. On the other hand,
Transformers can capture long-range mutual information more efficiently, making
them preferable in modeling sequences with slow power-law mutual information,
such as natural languages and stock prices. We discuss the connection of these
results with statistical mechanics. We also point out the non-uniformity
problem in many natural language datasets. We hope this work provides a new
perspective in understanding the expressive power of sequence models and shed
new light on improving the architecture of them.">"Mutual Information Scaling and Expressive Power of Sequence Models"</a>,
Huitao Shen,
arXiv: <a href="http://arxiv.org/abs/1905.04271v1">1905.04271</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.13052v1" title="Permutation of any two hidden units yields invariant properties in typical
deep generative neural networks. This permutation symmetry plays an important
role in understanding the computation performance of a broad class of neural
networks with two or more hidden units. However, a theoretical study of the
permutation symmetry is still lacking. Here, we propose a minimal model with
only two hidden units in a restricted Boltzmann machine, which aims to address
how the permutation symmetry affects the critical learning data size at which
the concept-formation (or spontaneous symmetry breaking in physics language)
starts, and moreover semi-rigorously prove a conjecture that the critical data
size is independent of the number of hidden units once this number is finite.
Remarkably, we find that the embedded correlation between two receptive fields
of hidden units reduces the critical data size. In particular, the
weakly-correlated receptive fields have the benefit of significantly reducing
the minimal data size that triggers the transition, given less noisy data.
Inspired by the theory, we also propose an efficient fully-distributed
algorithm to infer the receptive fields of hidden units. Overall, our results
demonstrate that the permutation symmetry is an interesting property that
affects the critical data size for computation performances of related learning
algorithms. All these effects can be analytically probed based on the minimal
model, providing theoretical insights towards understanding unsupervised
learning in a more general context.">"Minimal model of permutation symmetry in unsupervised learning"</a>,
Tianqi Hou, K. Y. Michael Wong, Haiping Huang,
arXiv: <a href="http://arxiv.org/abs/1904.13052v1">1904.13052</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.10996v1" title="Convolution operations designed for graph-structured data usually utilize the
graph Laplacian, which can be seen as message passing between the adjacent
neighbors through a generic random walk. In this paper, we propose PAN, a new
graph convolution framework that involves every path linking the message sender
and receiver with learnable weights depending on the path length, which
corresponds to the maximal entropy random walk. PAN generalizes the graph
Laplacian to a new transition matrix we call \emph{maximal entropy transition}
(MET) matrix derived from a path integral formalism. Most previous graph
convolutional network architectures can be adapted to our framework, and many
variations and derivatives based on the path integral idea can be developed.
Experimental results show that the path integral based graph neural networks
have great learnability and fast convergence rate, and achieve state-of-the-art
performance on benchmark tasks.">"PAN: Path Integral Based Convolution for Deep Graph Neural Networks"</a>,
Zheng Ma, Ming Li, Yuguang Wang,
arXiv: <a href="http://arxiv.org/abs/1904.10996v1">1904.10996</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.10869v1" title="Deep belief networks are used extensively for unsupervised stochastic
learning on large datasets. Compared to other deep learning approaches their
layer-by-layer learning makes them highly scalable. Unfortunately, the
principles by which they achieve efficient learning are not well understood.
Numerous attempts have been made to explain their efficiency and applicability
to a wide class of learning problems in terms of principles drawn from
cognitive psychology, statistics, information theory, and more recently
physics, but quite often these imported principles lack strong scientific
foundation. Here we demonstrate how one can arrive at convolutional deep belief
networks as potential solution to unsupervised learning problems without making
assumptions about the underlying framework. To do this, we exploit the notion
of symmetry that is fundamental in machine learning, physics and other fields,
utilizing the particular form of the functional renormalization group in
physics.">"Variational approach to unsupervised learning"</a>,
Swapnil Nitin Shah,
arXiv: <a href="http://arxiv.org/abs/1904.10869v1">1904.10869</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.10819v1" title="Artificial neural networks (ANNs) have now been widely used for industry
applications and also played more important roles in fundamental researches.
Although most ANN hardware systems are electronically based, optical
implementation is particularly attractive because of its intrinsic parallelism
and low energy consumption. Here, we propose and demonstrate fully-functioned
all optical neural networks (AONNs), in which linear operations are programmed
by spatial light modulators and Fourier lenses, and optical nonlinear
activation functions are realized with electromagnetically induced transparency
in laser-cooled atoms. Moreover, all the errors from different optical neurons
here are independent, thus the AONN could scale up to a larger system size with
final error still maintaining in a similar level of a single neuron. We confirm
its capability and feasibility in machine learning by successfully classifying
the order and disorder phases of a typical statistic Ising model. The
demonstrated AONN scheme can be used to construct various ANNs of different
architectures with the intrinsic parallel computation at the speed of light.">"All Optical Neural Network with Nonlinear Activation Functions"</a>,
Ying Zuo, Bohan Li, Yujun Zhao, Yue Jiang, You-Chiuan Chen, Peng Chen, Gyu-Boong Jo, Junwei Liu, Shengwang Du,
arXiv: <a href="http://arxiv.org/abs/1904.10819v1">1904.10819</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.08991v1" title="Neural networks are a central technique in machine learning. Recent years
have seen a wave of interest in applying neural networks to physical systems
for which the governing dynamics are known and expressed through differential
equations. Two fundamental challenges facing the development of neural networks
in physics applications is their lack of interpretability and their
physics-agnostic design. The focus of the present work is to embed physical
constraints into the structure of the neural network to address the second
fundamental challenge. By constraining tunable parameters (such as weights and
biases) and adding special layers to the network, the desired constraints are
guaranteed to be satisfied without the need for explicit regularization terms.
This is demonstrated on supervised and unsupervised networks for two basic
symmetries: even/odd symmetry of a function and energy conservation. In the
supervised case, the network with embedded constraints is shown to perform well
on regression problems while simultaneously obeying the desired constraints
whereas a traditional network fits the data but violates the underlying
constraints. Finally, a new unsupervised neural network is proposed that
guarantees energy conservation through an embedded symplectic structure. The
symplectic neural network is used to solve a system of energy-conserving
differential equations and outperforms an unsupervised, non-symplectic neural
network.">"Physical Symmetries Embedded in Neural Networks"</a>,
M. Mattheakis, P. Protopapas, D. Sondak, M. Di Giovanni, E. Kaxiras,
arXiv: <a href="http://arxiv.org/abs/1904.08991v1">1904.08991</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.06194v1" title="A deep neural network is a parameterization of a multi-layer mapping of
signals in terms of many alternatively arranged linear and nonlinear
transformations. The linear transformations, which are generally used in the
fully-connected as well as convolutional layers, contain most of the
variational parameters that are trained and stored. Compressing a deep neural
network to reduce its number of variational parameters but not its prediction
power is an important but challenging problem towards the establishment of an
optimized scheme in training efficiently these parameters and in lowering the
risk of overfitting. Here we show that this problem can be effectively solved
by representing linear transformations with matrix product operators (MPO). We
have tested this approach in five main neural networks, including FC2, LeNet-5,
VGG, ResNet, and DenseNet on two widely used datasets, namely MNIST and
CIFAR-10, and found that this MPO representation indeed sets up a faithful and
efficient mapping between input and output signals, which can keep or even
improve the prediction accuracy with dramatically reduced number of parameters.">"Compressing deep neural networks by matrix product operators"</a>,
Ze-Feng Gao, Song Cheng, Rong-Qiang He, Z. Y. Xie, Hui-Hai Zhao, Zhong-Yi Lu, Tao Xiang,
arXiv: <a href="http://arxiv.org/abs/1904.06194v1">1904.06194</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.04154v1" title="We recapitulate the Bayesian formulation of neural network based classifiers
and show that, while sampling from the posterior does indeed lead to better
generalisation than is obtained by standard optimisation of the cost function,
even better performance can in general be achieved by sampling finite
temperature (\(T\)) distributions derived from the posterior. Taking the example
of two different deep (3 hidden layers) classifiers for MNIST data, we find
quite different \(T\) values to be appropriate in each case. In particular, for a
typical neural network classifier a clear minimum of the test error is observed
at \(T&gt;0\). This suggests an early stopping criterion for full batch simulated
annealing: cool until the average validation error starts to increase, then
revert to the parameters with the lowest validation error. As \(T\) is increased
classifiers transition from accurate classifiers to classifiers that have
higher training error than assigning equal probability to each class. Efficient
studies of these temperature-induced effects are enabled using a
replica-exchange Hamiltonian Monte Carlo simulation technique. Finally, we show
how thermodynamic integration can be used to perform model selection for deep
neural networks. Similar to the Laplace approximation, this approach assumes
that the posterior is dominated by a single mode. Crucially, however, no
assumption is made about the shape of that mode and it is not required to
precisely compute and invert the Hessian.">"Bayesian Neural Networks at Finite Temperature"</a>,
Robert J. N. Baldock, Nicola Marzari,
arXiv: <a href="http://arxiv.org/abs/1904.04154v1">1904.04154</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.12632v1" title="The &quot;edge of chaos&quot; phase transition in artificial neural networks is of
renewed interest in the light of recent evidence for criticality in brain
dynamics. Statistical mechanics traditionally studied this transition with
connectivity \(k\) as control parameter and an exactly balanced
excitation/inhibition ratio. While critical connectivity has been found to be
low in these model systems, typically around \(k=2\), which is unrealistic for
natural systems, a recent study utilizing the excitation/inhibition ratio as
control parameter found a new, nearly degree-independent critical point when
connectivity is large. However, the new phase transition is accompanied by an
unnaturally high level of activity in the network.
  Here we study random neural networks with the additional properties of (i) a
high clustering coefficient and (ii) neurons that are solely either excitatory
or inhibitory, a prominent property of natural neurons. As a result we observe
an additional critical point for networks with large connectivity, regardless
of degree distribution, which exhibits low activity levels that compare well
with neuronal brain networks.">"Critical excitation/inhibition balance in dense neural networks"</a>,
Lorenz Baumgarten, Stefan Bornholdt,
arXiv: <a href="http://arxiv.org/abs/1903.12632v1">1903.12632</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.10742v1" title="Tensor network (TN) has recently triggered extensive interests in developing
machine-learning models in quantum many-body Hilbert space. Here we purpose a
generative TN classification (GTNC) approach for supervised learning. The
strategy is to train the generative TN for each class of the samples to
construct the classifiers. The classification is implemented by comparing the
distance in the many-body Hilbert space. The numerical experiments by GTNC show
impressive performance on the MNIST and Fashion-MNIST dataset. The testing
accuracy is competitive to the state-of-the-art convolutional neural network
while higher than the naive Bayes classifier (a generative classifier) and
support vector machine. Moreover, GTNC is more efficient than the existing TN
models that are in general discriminative. By investigating the distances in
the many-body Hilbert space, we find that (a) the samples are naturally
clustering in such a space; and (b) bounding the bond dimensions of the TN's to
finite values corresponds to removing redundant information in the image
recognition. These two characters make GTNC an adaptive and universal model of
excellent performance.">"Generative Tensor Network Classification Model for Supervised Machine
  Learning"</a>,
Zheng-Zhi Sun, Cheng Peng, Ding Liu, Shi-Ju Ran, Gang Su,
arXiv: <a href="http://arxiv.org/abs/1903.10742v1">1903.10742</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.09562v1" title="The ultimate goal of physics is finding a unique equation capable of
describing the evolution of any observable quantity in a self-consistent way.
Within the field of statistical physics, such an equation is known as the
generalized Langevin equation (GLE). Nevertheless, the formal and exact GLE is
not particularly useful, since it depends on the complete history of the
observable at hand, and on hidden degrees of freedom typically inaccessible
from a theoretical point of view. In this work, we propose the use of deep
neural networks as a new avenue for learning the intricacies of the unknowns
mentioned above. By using machine learning to eliminate the unknowns from GLEs,
our methodology outperforms previous approaches (in terms of efficiency and
robustness) where general fitting functions were postulated. Finally, our work
is tested against several prototypical examples, from a colloidal systems and
particle chains immersed in a thermal bath, to climatology and financial
models. In all cases, our methodology exhibits an excellent agreement with the
actual dynamics of the observables under consideration.">"Deep learning as closure for irreversible processes: A data-driven
  generalized Langevin equation"</a>,
Antonio Russo, Miguel A. Durán-Olivencia, Ioannis G. Kevrekidis, Serafim Kalliadasis,
arXiv: <a href="http://arxiv.org/abs/1903.09562v1">1903.09562</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.07378v1" title="We introduce exact macroscopic on-line learning dynamics of two-layer neural
networks with ReLU units in the form of a system of differential equations,
using techniques borrowed from statistical physics. For the first experiments,
numerical solutions reveal similar behavior compared to sigmoidal activation
researched in earlier work. In these experiments the theoretical results show
good correspondence with simulations. In ove-rrealizable and unrealizable
learning scenarios, the learning behavior of ReLU networks shows distinctive
characteristics compared to sigmoidal networks.">"On-line learning dynamics of ReLU neural networks using statistical
  physics techniques"</a>,
Michiel Straat, Michael Biehl,
arXiv: <a href="http://arxiv.org/abs/1903.07378v1">1903.07378</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.04951v1" title="We provide a deep Boltzmann machine (DBM) for the AdS/CFT correspondence.
Under the philosophy that the bulk spacetime is a neural network, we give a
dictionary between those, and obtain a restricted DBM as a discretized bulk
scalar field theory in curved geometries. The probability distribution as
training data is the generating functional of the boundary quantum field
theory, and it trains neural network weights which are the metric of the bulk
geometry. The deepest layer implements black hole horizons, and an employed
regularization for the weights is an Einstein action. A large \(N_c\) limit in
holography reduces the DBM to a folded feed-forward architecture. We also
neurally implement holographic renormalization into an autoencoder. The DBM for
the AdS/CFT may serve as a platform for studying mechanisms of spacetime
emergence in holography.">"AdS/CFT as a deep Boltzmann machine"</a>,
Koji Hashimoto,
arXiv: <a href="http://arxiv.org/abs/1903.04951v1">1903.04951</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.02606v1" title="Batch Normalization (BatchNorm) is an extremely useful component of modern
neural network architectures, enabling optimization using higher learning rates
and achieving faster convergence. In this paper, we use mean-field theory to
analytically quantify the impact of BatchNorm on the geometry of the loss
landscape for multi-layer networks consisting of fully-connected and
convolutional layers. We show that it has a flattening effect on the loss
landscape, as quantified by the maximum eigenvalue of the Fisher Information
Matrix. These findings are then used to justify the use of larger learning
rates for networks that use BatchNorm, and we provide quantitative
characterization of the maximal allowable learning rate to ensure convergence.
Experiments support our theoretically predicted maximum learning rate, and
furthermore suggest that networks with smaller values of the BatchNorm
parameter achieve lower loss after the same number of epochs of training.">"Mean-field Analysis of Batch Normalization"</a>,
Mingwei Wei, James Stokes, David J Schwab,
arXiv: <a href="http://arxiv.org/abs/1903.02606v1">1903.02606</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.08129v2" title="We develop a mean field theory for batch normalization in fully-connected
feedforward neural networks. In so doing, we provide a precise characterization
of signal propagation and gradient backpropagation in wide batch-normalized
networks at initialization. Our theory shows that gradient signals grow
exponentially in depth and that these exploding gradients cannot be eliminated
by tuning the initial weight variances or by adjusting the nonlinear activation
function. Indeed, batch normalization itself is the cause of gradient
explosion. As a result, vanilla batch-normalized networks without skip
connections are not trainable at large depths for common initialization
schemes, a prediction that we verify with a variety of empirical simulations.
While gradient explosion cannot be eliminated, it can be reduced by tuning the
network close to the linear regime, which improves the trainability of deep
batch-normalized networks without residual connections. Finally, we investigate
the learning dynamics of batch-normalized networks and observe that after a
single step of optimization the networks achieve a relatively stable
equilibrium in which gradients have dramatically smaller dynamic range. Our
theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new
identities that may be of independent interest.">"A Mean Field Theory of Batch Normalization"</a>,
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, Samuel S. Schoenholz,
arXiv: <a href="http://arxiv.org/abs/1902.08129v2">1902.08129</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.06015v1" title="We consider learning two layer neural networks using stochastic gradient
descent. The mean-field description of this learning dynamics approximates the
evolution of the network weights by an evolution in the space of probability
distributions in \(R^D\) (where \(D\) is the number of parameters associated to
each neuron). This evolution can be defined through a partial differential
equation or, equivalently, as the gradient flow in the Wasserstein space of
probability distributions. Earlier work shows that (under some regularity
assumptions), the mean field description is accurate as soon as the number of
hidden units is much larger than the dimension \(D\). In this paper we establish
stronger and more general approximation guarantees. First of all, we show that
the number of hidden units only needs to be larger than a quantity dependent on
the regularity properties of the data, and independent of the dimensions. Next,
we generalize this analysis to the case of unbounded activation functions,
which was not covered by earlier bounds. We extend our results to noisy
stochastic gradient descent.
  Finally, we show that kernel ridge regression can be recovered as a special
limit of the mean field analysis.">"Mean-field theory of two-layers neural networks: dimension-free bounds
  and kernel limit"</a>,
Song Mei, Theodor Misiakiewicz, Andrea Montanari,
arXiv: <a href="http://arxiv.org/abs/1902.06015v1">1902.06015</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.04760v2" title="Several recent trends in machine learning theory and practice, from the
design of state-of-the-art Gaussian Process to the convergence analysis of deep
neural nets (DNNs) under stochastic gradient descent (SGD), have found it
fruitful to study wide random neural networks. Central to these approaches are
certain scaling limits of such networks. We unify these results by introducing
a notion of a straightline \emph{tensor program} that can express most neural
network computations, and we characterize its scaling limit when its tensors
are large and randomized. From our framework follows (1) the convergence of
random neural networks to Gaussian processes for architectures such as
recurrent neural networks, convolutional neural networks, residual networks,
attention, and any combination thereof, with or without batch normalization;
(2) conditions under which the \emph{gradient independence assumption} -- that
weights in backpropagation can be assumed to be independent from weights in the
forward pass -- leads to correct computation of gradient dynamics, and
corrections when it does not; (3) the convergence of the Neural Tangent Kernel,
a recently proposed kernel used to predict training dynamics of neural networks
under gradient descent, at initialization for all architectures in (1) without
batch normalization. Mathematically, our framework is general enough to
rederive classical random matrix results such as the semicircle and the
Marchenko-Pastur laws, as well as recent results in neural network Jacobian
singular values. We hope our work opens a way toward design of even stronger
Gaussian Processes, initialization schemes to avoid gradient
explosion/vanishing, and deeper understanding of SGD dynamics in modern
architectures.">"Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian
  Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"</a>,
Greg Yang,
arXiv: <a href="http://arxiv.org/abs/1902.04760v2">1902.04760</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.02880v1" title="Can multilayer neural networks -- typically constructed as highly complex
structures with many nonlinearly activated neurons across layers -- behave in a
non-trivial way that yet simplifies away a major part of their complexities? In
this work, we uncover a phenomenon in which the behavior of these complex
networks -- under suitable scalings and stochastic gradient descent dynamics --
becomes independent of the number of neurons as this number grows sufficiently
large. We develop a formalism in which this many-neurons limiting behavior is
captured by a set of equations, thereby exposing a previously unknown operating
regime of these networks. While the current pursuit is mathematically
non-rigorous, it is complemented with several experiments that validate the
existence of this behavior.">"Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks"</a>,
Phan-Minh Nguyen,
arXiv: <a href="http://arxiv.org/abs/1902.02880v1">1902.02880</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.10416v1" title="These notes attempt a self-contained introduction into statistical field
theory applied to neural networks of rate units and binary spins. The
presentation consists of three parts: First, the introduction of fundamental
notions of probabilities, moments, cumulants, and their relation by the linked
cluster theorem, of which Wick's theorem is the most important special case;
followed by the diagrammatic formulation of perturbation theory, reviewed in
the statistical setting. Second, dynamics described by stochastic differential
equations in the Ito-formulation, treated in the Martin-Siggia-Rose-De
Dominicis-Janssen path integral formalism. With concepts from disordered
systems, we then study networks with random connectivity and derive their
self-consistent dynamic mean-field theory, explaining the statistics of
fluctuations and the emergence of different phases with regular and chaotic
dynamics. Third, we introduce the effective action, vertex functions, and the
loopwise expansion. These tools are illustrated by systematic derivations of
self-consistency equations, going beyond the mean-field approximation. These
methods are applied to the pairwise maximum entropy (Ising spin) model,
including the recently-found diagrammatic derivation of the
Thouless-Anderson-Palmer mean field theory.">"Statistical field theory for neural networks"</a>,
Moritz Helias, David Dahmen,
arXiv: <a href="http://arxiv.org/abs/1901.10416v1">1901.10416</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.09566v1" title="Various existing quantum supervised learning (SL) schemes rely on quantum
random access memories to store quantum-encoded data given a priori in a
classical description. The data acquisition process, however, has not been
accounted for, while it sets the ultimate limit of the usefulness of the data
for different SL tasks, as constrained by the quantum Cramer-Rao bound. We
introduce supervised learning enhanced by an entangled sensor network (SLEEN)
as a means to carry out SL tasks at the physical layer where a quantum
advantage is achieved. The entanglement shared by different sensors boosts the
performance of extracting global features of the object under investigation. We
leverage SLEEN to construct an entanglement-enhanced support-vector machine for
quantum data classification and entanglement-enhanced principal component
analyzer for quantum data compression. In both schemes, variational circuits
are employed to seek the optimum entangled probe state and measurement settings
to maximize the entanglement-enabled quantum advantage. We compare the
performance of SLEEN with separable-state SL schemes and observe an appreciable
entanglement-enabled performance gain even in the presence of loss. SLEEN is
realizable with available technology, opening a viable route toward building
near-term quantum devices that offer unmatched performance beyond what the
optimum classical device is able to afford.">"Supervised Learning Enhanced by an Entangled Sensor Network"</a>,
Quntao Zhuang, Zheshen Zhang,
arXiv: <a href="http://arxiv.org/abs/1901.09566v1">1901.09566</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.09085v1" title="Deep neural networks achieve stellar generalisation on a variety of problems,
despite often being large enough to easily fit all their training data. Here we
study the generalisation dynamics of two-layer neural networks in a
teacher-student setup, where one network, the student, is trained using
stochastic gradient descent (SGD) on data generated by another network, called
the teacher. We show how for this problem, the dynamics of SGD are captured by
a set of differential equations. In particular, we demonstrate analytically
that the generalisation error of the student increases linearly with the
network size, with other relevant parameters held constant. Our results
indicate that achieving good generalisation in neural networks depends on the
interplay of at least the algorithm, its learning rate, the model architecture,
and the data set.">"Generalisation dynamics of online learning in over-parameterised neural
  networks"</a>,
Sebastian Goldt, Madhu S. Advani, Andrew M. Saxe, Florent Krzakala, Lenka Zdeborová,
arXiv: <a href="http://arxiv.org/abs/1901.09085v1">1901.09085</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.06314v1" title="Surrogate modeling and uncertainty quantification tasks for PDE systems are
most often considered as supervised learning problems where input and output
data pairs are used for training. The construction of such emulators is by
definition a small data problem which poses challenges to deep learning
approaches that have been developed to operate in the big data regime. Even in
cases where such models have been shown to have good predictive capability in
high dimensions, they fail to address constraints in the data implied by the
PDE model. This paper provides a methodology that incorporates the governing
equations of the physical model in the loss/likelihood functions. The resulting
physics-constrained, deep learning models are trained without any labeled data
(e.g. employing only input data) and provide comparable predictive responses
with data-driven models while obeying the constraints of the problem at hand.
This work employs a convolutional encoder-decoder neural network approach as
well as a conditional flow-based generative model for the solution of PDEs,
surrogate model construction, and uncertainty quantification tasks. The
methodology is posed as a minimization problem of the reverse Kullback-Leibler
(KL) divergence between the model predictive density and the reference
conditional density, where the later is defined as the Boltzmann-Gibbs
distribution at a given inverse temperature with the underlying potential
relating to the PDE system of interest. The generalization capability of these
models to out-of-distribution input is considered. Quantification and
interpretation of the predictive uncertainty is provided for a number of
problems.">"Physics-Constrained Deep Learning for High-dimensional Surrogate
  Modeling and Uncertainty Quantification without Labeled Data"</a>,
Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, Paris Perdikaris,
arXiv: <a href="http://arxiv.org/abs/1901.06314v1">1901.06314</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.02217v1" title="Matrix product states (MPS), a tensor network designed for one-dimensional
quantum systems, has been recently proposed for generative modeling of natural
data (such as images) in terms of `Born machine'. However, the exponential
decay of correlation in MPS restricts its representation power heavily for
modeling complex data such as natural images. In this work, we push forward the
effort of applying tensor networks to machine learning by employing the Tree
Tensor Network (TTN) which exhibits balanced performance in expressibility and
efficient training and sampling. We design the tree tensor network to utilize
the 2-dimensional prior of the natural images and develop sweeping learning and
sampling algorithms which can be efficiently implemented utilizing Graphical
Processing Units (GPU). We apply our model to random binary patterns and the
binary MNIST datasets of handwritten digits. We show that TTN is superior to
MPS for generative modeling in keeping correlation of pixels in natural images,
as well as giving better log-likelihood scores in standard datasets of
handwritten digits. We also compare its performance with state-of-the-art
generative models such as the Variational AutoEncoders, Restricted Boltzmann
machines, and PixelCNN. Finally, we discuss the future development of Tensor
Network States in machine learning problems.">"Tree Tensor Networks for Generative Modeling"</a>,
Song Cheng, Lei Wang, Tao Xiang, Pan Zhang,
arXiv: <a href="http://arxiv.org/abs/1901.02217v1">1901.02217</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1812.08778v1" title="Hybrid variational quantum algorithms have been proposed for simulating
many-body quantum systems with shallow quantum circuits, and are therefore
relevant to Noisy Intermediate Scale Quantum devices. These algorithms are
often discussed as a means to solve static energy spectra and simulate the
dynamics of real and imaginary time evolutions. Here we consider broader uses
of the variational method to simulate general processes. We first show a
variational algorithm for simulating the generalised time evolution with a
non-hermitian Hamiltonian. Then we consider matrix multiplication, a vital
component of diverse applications in many fields including machine learning and
optimisation. We first convert matrix multiplication into a matrix evolution
problem and show how it can be simulated with the algorithm for generalised
time evolution. Meanwhile, when considering matrices that are products of small
matrices, we propose an alternative variational algorithm that can realise
matrix multiplication with a simpler circuit. Finally, we focus on open quantum
systems and apply the developed methods to the variational simulation of
stochastic master equations. We numerically test our theory with a single qubit
system suffering dephasing noise.">"Variational quantum simulation of general processes"</a>,
Suguru Endo, Ying Li, Simon Benjamin, Xiao Yuan,
arXiv: <a href="http://arxiv.org/abs/1812.08778v1">1812.08778</a>,
12/2018</p>
</li>
</ul>
<h2>Quantum Computation and Quantum Algorithms for Machine Learning</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1905.04286v1" title="High-dimensional quantum systems are vital for quantum technologies and are
essential in demonstrating practical quantum advantage in quantum computing,
simulation and sensing. Since dimensionality grows exponentially with the
number of qubits, the potential power of noisy intermediate-scale quantum
(NISQ) devices over classical resources also stems from entangled states in
high dimensions. An important family of quantum protocols that can take
advantage of high-dimensional Hilbert space are classification tasks. These
include quantum machine learning algorithms, witnesses in quantum information
processing and certain decision problems. However, due to counter-intuitive
geometrical properties emergent in high dimensions, classification problems are
vulnerable to adversarial attacks. We demonstrate that the amount of
perturbation needed for an adversary to induce a misclassification scales
inversely with dimensionality. This is shown to be a fundamental feature
independent of the details of the classification protocol. Furthermore, this
leads to a trade-off between the security of the classification algorithm
against adversarial attacks and quantum advantages we expect for
high-dimensional problems. In fact, protection against these adversarial
attacks require extra resources that scale at least polynomially with the
Hilbert space dimension of the system, which can erase any significant quantum
advantage that we might expect from a quantum protocol. This has wide-ranging
implications in the use of both near-term and future quantum technologies for
classification.">"Vulnerability of quantum classification to adversarial perturbations"</a>,
Nana Liu, Peter Wittek,
arXiv: <a href="http://arxiv.org/abs/1905.04286v1">1905.04286</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.00365v1" title="Generalized linear models (GLM) are link function based statistical models.
Many supervised learning algorithms are extensions of GLMs and have link
functions built into the algorithm to model different outcome distributions.
There are two major drawbacks when using this approach in applications using
real world datasets. One is that none of the link functions available in the
popular packages is a good fit for the data. Second, it is computationally
inefficient and impractical to test all the possible distributions to find the
optimum one. In addition, many GLMs and their machine learning extensions
struggle on problems of overdispersion in Tweedie distributions. In this paper
we propose a quantum extension to GLM that overcomes these drawbacks. A quantum
gate with non-Gaussian transformation can be used to continuously deform the
outcome distribution from known results. In doing so, we eliminate the need for
a link function. Further, by using an algorithm that superposes all possible
distributions to collapse to fit a dataset, we optimize the model in a
computationally efficient way. We provide an initial proof-of-concept by
testing this approach on both a simulation of overdispersed data and then on a
benchmark dataset, which is quite overdispersed, and achieved state of the art
results. This is a game changer in several applied fields, such as part failure
modeling, medical research, actuarial science, finance and many other fields
where Tweedie regression and overdispersion are ubiquitous.">"Quantum Generalized Linear Models"</a>,
Colleen M. Farrelly, Srikanth Namuduri, Uchenna Chukwu,
arXiv: <a href="http://arxiv.org/abs/1905.00365v1">1905.00365</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.00293v1" title="We introduce a binary temperature classifier quantum model operates in a
thermal environment. Proper measurement and sensing of temperature are of
central importance to the realization of nanoscale quantum devices. More
significantly, minimal classifiers may constitute the basic units for the
physical quantum neural networks. In the present study, first, the mathematical
model was introduced through a two-level quantum system weakly coupled to the
thermal reservoirs and demonstrate that the model faithfully classifies the
temperature information of the reservoirs in the thermal steady state limit. We
also suggest a physical model implements the proposed thermal classifier by
superconducting circuits composed of transmon qubits and discuss the
feasibility by realistic parameters. It's shown that the physical model
operates three orders of magnitude faster than the current binary classifiers.">"A thermal quantum classifier"</a>,
Ufuk Korkmaz, Deniz Türkpençe, Tahir Çetin Akıncı, Serhat Şeker,
arXiv: <a href="http://arxiv.org/abs/1905.00293v1">1905.00293</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1905.00159v1" title="A D-Wave quantum annealer (QA) having a 2048 qubit lattice, with no missing
qubits and couplings, allowed embedding of a complete graph of a Restricted
Boltzmann Machine (RBM). A handwritten digit OptDigits data set having 8x7
pixels of visible units was used to train the RBM using a classical Contrastive
Divergence. Embedding of the classically-trained RBM into the D-Wave lattice
was used to demonstrate that the QA offers a high-efficiency alternative to the
classical Markov Chain Monte Carlo (MCMC) for reconstructing missing labels of
the test images as well as a generative model. At any training iteration, the
D-Wave-based classification had classification error more than two times lower
than MCMC. The main goal of this study was to investigate the quality of the
sample from the RBM model distribution and its comparison to a classical MCMC
sample. For the OptDigits dataset, the states in the D-Wave sample belonged to
about two times more local valleys compared to the MCMC sample. All the
lowest-energy (the highest joint probability) local minima in the MCMC sample
were also found by the D-Wave. The D-Wave missed many of the higher-energy
local valleys, while finding many &quot;new&quot; local valleys consistently missed by
the MCMC. It was established that the &quot;new&quot; local valleys that the D-Wave finds
are important for the model distribution in terms of the energy of the
corresponding local minima, the width of the local valleys, and the height of
the escape barrier.">"Towards Sampling from Nondirected Probabilistic Graphical models using a
  D-Wave Quantum Annealer"</a>,
Yaroslav Koshka, M. A. Novotny,
arXiv: <a href="http://arxiv.org/abs/1905.00159v1">1905.00159</a>,
5/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.12697v1" title="Artificial neural network, consisting of many neurons in different layers, is
an important method to simulate humain brain. Usually, one neuron has two
operations: one is linear, the other is nonlinear. The linear operation is
inner product and the nonlinear operation is represented by an activation
function. In this work, we introduce a kind of quantum neuron whose inputs and
outputs are quantum states. The inner product and activation operator of the
quantum neurons can be realized by quantum circuits. Based on the quantum
neuron, we propose a model of quantum neural network in which the weights
between neurons are all quantum states. We also construct a quantum circuit to
realize this quantum neural network model. A learning algorithm is proposed
meanwhile. We show the validity of learning algorithm theoretically and
demonstrate the potential of the quantum neural network numerically.">"Building quantum neural networks based on swap test"</a>,
Jian Zhao, Yuan-Hang Zhang, Chang-Peng Shao, Yu-Chun Wu, Guang-Can Guo, Guo-Ping Guo,
arXiv: <a href="http://arxiv.org/abs/1904.12697v1">1904.12697</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.10573v1" title="We present an algorithm for learning a latent variable generative model via
generative adversarial learning where the canonical uniform noise input is
replaced by samples from a graphical model. This graphical model is learned by
a Boltzmann machine which learns low-dimensional feature representation of data
extracted by the discriminator. A quantum annealer, the D-Wave 2000Q, is used
to sample from this model. This algorithm joins a growing family of algorithms
that use a quantum annealing subroutine in deep learning, and provides a
framework to test the advantages of quantum-assisted learning in GANs. Fully
connected, symmetric bipartite and Chimera graph topologies are compared on a
reduced stochastically binarized MNIST dataset, for both classical and quantum
annealing sampling methods. The quantum-assisted associative adversarial
network successfully learns a generative model of the MNIST dataset for all
topologies, and is also applied to the LSUN dataset bedrooms class for the
Chimera topology. Evaluated using the Fr\'{e}chet inception distance and
inception score, the quantum and classical versions of the algorithm are found
to have equivalent performance for learning an implicit generative model of the
MNIST dataset.">"Quantum-assisted associative adversarial network: Applying quantum
  annealing in deep learning"</a>,
Max Wilson, Thomas Vandal, Tad Hogg, Eleanor Rieffel,
arXiv: <a href="http://arxiv.org/abs/1904.10573v1">1904.10573</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.09602v1" title="The exploration of quantum algorithms that possess quantum advantages is a
central topic in quantum computation and quantum information processing. One
potential candidate in this area is quantum generative adversarial learning
(QuGAL), which conceptually has exponential advantages over classical
adversarial networks. However, the corresponding learning algorithm remains
obscured. In this paper, we propose the first quantum generative adversarial
learning algorithm-- the quantum multiplicative matrix weight algorithm
(QMMW)-- which enables the efficient processing of fundamental tasks. The
computational complexity of QMMW is polynomially proportional to the number of
training rounds and logarithmically proportional to the input size. The core
concept of the proposed algorithm combines QuGAL with online learning. We
exploit the implementation of QuGAL with parameterized quantum circuits, and
numerical experiments for the task of entanglement test for pure state are
provided to support our claims.">"Efficient Online Quantum Generative Adversarial Learning Algorithms with
  Applications"</a>,
Yuxuan Du, Min-Hsiu Hsieh, Dacheng Tao,
arXiv: <a href="http://arxiv.org/abs/1904.09602v1">1904.09602</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.06411v2" title="The cocktail party problem refers to the famous selective attention problem
of how to find out the signal of each individual sources from signals of a
number of detectors. In the classical cocktail party problem, the signal of
each source is a sequence of data such as the voice from a speaker, and each
detector detects signal as a linear combination of all sources. This problem
can be solved by a unsupervised machine learning algorithm known as the
independent component analysis. In this work we propose a quantum analog of the
cocktail party problem. Here each source is a density matrix of a pure state
and each detector detects a density matrix as a linear combination of all pure
state density matrix. The quantum cocktail party problem is to recover the pure
state density matrix from a number observed mixed state density matrices. We
propose the physical realization of this problem, and how to solve this problem
through either classical Newton's optimization method or by mapping the problem
to the ground state of an Ising type of spin Hamiltonian.">"The Quantum Cocktail Party Problem"</a>,
Xiao Liang, Yadong Wu, Hui Zhai,
arXiv: <a href="http://arxiv.org/abs/1904.06411v2">1904.06411</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.02276v1" title="We investigate quantum algorithms for classification, a fundamental problem
in machine learning, with provable guarantees. Given \(n\) \(d\)-dimensional data
points, the state-of-the-art (and optimal) classical algorithm for training
classifiers with constant margin runs in \(\tilde{O}(n+d)\) time. We design
sublinear quantum algorithms for the same task running in \(\tilde{O}(\sqrt{n}
+\sqrt{d})\) time, a quadratic improvement in both \(n\) and \(d\). Moreover, our
algorithms use the standard quantization of the classical input and generate
the same classical output, suggesting minimal overheads when used as
subroutines for end-to-end applications. We also demonstrate a tight lower
bound (up to poly-log factors) and discuss the possibility of implementation on
near-term quantum machines. As a side result, we also give sublinear quantum
algorithms for approximating the equilibria of \(n\)-dimensional matrix zero-sum
games with optimal complexity \(\tilde{\Theta}(\sqrt{n})\).">"Sublinear quantum algorithms for training linear and kernel-based
  classifiers"</a>,
Tongyang Li, Shouvanik Chakrabarti, Xiaodi Wu,
arXiv: <a href="http://arxiv.org/abs/1904.02276v1">1904.02276</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.02214v1" title="The search for an application of near-term quantum devices is widespread.
Quantum Machine Learning is touted as a potential utilisation of such devices,
particularly those which are out of the reach of the simulation capabilities of
classical computers. In this work, we propose a generative Quantum Machine
Learning Model, called the Ising Born Machine (IBM), which we show cannot, in
the worst case, and up to suitable notions of error, be simulated efficiently
by a classical device. We also show this holds for all the circuit families
encountered during training. In particular, we explore quantum circuit learning
using non-universal circuits derived from Ising Model Hamiltonians, which are
implementable on near term quantum devices.
  We propose two novel training methods for the IBM by utilising the Stein
Discrepancy and the Sinkhorn Divergence cost functions. We show numerically,
both using a simulator within Rigetti's Forest platform and on the Aspen-1 16Q
chip, that the cost functions we suggest outperform the more commonly used
Maximum Mean Discrepancy (MMD) for differentiable training. We also propose an
improvement to the MMD by proposing a novel utilisation of quantum kernels
which we demonstrate provides improvements over its classical counterpart. We
discuss the potential of these methods to learn hard' quantum distributions, a
feat which would demonstrate the advantage of quantum over classical computers,
and provide the first formal definitions for what we callQuantum Learning
Supremacy'. Finally, we propose a novel view on the area of quantum circuit
compilation by using the IBM to `mimic' target quantum circuits using classical
output data only.">"The Born Supremacy: Quantum Advantage and Training of an Ising Born
  Machine"</a>,
Brian Coyle, Daniel Mills, Vincent Danos, Elham Kashefi,
arXiv: <a href="http://arxiv.org/abs/1904.02214v1">1904.02214</a>,
4/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1904.00043v1" title="Quantum algorithms have the potential to outperform their classical
counterparts in a variety of tasks. The realization of the advantage often
requires the ability to load classical data efficiently into quantum states.
However, the best known methods for loading generic data into an \(n\)-qubit
state require \(\mathcal{O}\left(2^n\right)\) gates. This scaling can easily
predominate the complexity of a quantum algorithm and, thereby, impair
potential quantum advantage. Our work demonstrates that quantum Generative
Adversarial Networks (qGANs) facilitate efficient loading of generic
probability distributions -- implicitly given by data samples -- into quantum
states. More specifically, the qGAN scheme employs the interplay of a quantum
channel, a variational quantum circuit, and a classical neural network to learn
the probability distribution underlying training data samples and load it into
the quantum channel. Effectively, the scheme results in a quantum channel that
loads the learned distribution with
\(\mathcal{O}\left(poly\left(n\right)\right)\) gates. This distribution loading
method can, thus, enable the exploitation of quantum advantage induced by other
quantum algorithms, such as Quantum Amplitude Estimation. We implement the qGAN
distribution learning and loading method with Qiskit and test it using a
quantum simulation as well as actual quantum processors provided by the IBM Q
Experience. Furthermore, we employ quantum simulation to demonstrate the use of
the trained quantum channel in a quantum finance application.">"Quantum Generative Adversarial Networks for Learning and Loading Random
  Distributions"</a>,
Christa Zoufal, Aurélien Lucchi, Stefan Woerner,
arXiv: <a href="http://arxiv.org/abs/1904.00043v1">1904.00043</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.08699v2" title="As a ubiquitous aspect of modern information technology, data compression has
a wide range of applications. Therefore, quantum autoencoder which can compress
quantum information into a reduced space is fundamentally important to achieve
atomatical data compression in the field of quantum information. Such a quantum
autoencoder can be implemented through training the parameters of a quantum
device using machine learning. In this paper, we experimentally realize a
universal two-qubit unitary gate and achieve a quantum autoencoder by applying
machine learning. Also, this quantum autoencoder can be used to discriminate
two groups of nonorthogonal states.">"Experimental realization of a quantum autoencoder via a universal
  two-qubit unitary gate"</a>,
Chang-Jiang Huang, Qi Yin, Jun-Feng Tang, Daoyi Dong, Guo-Yong Xiang, Chuan-Feng Li, Guang-Can Guo,
arXiv: <a href="http://arxiv.org/abs/1903.08699v2">1903.08699</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1903.03999v2" title="Principal component analysis is an important dimension reduction technique in
machine learning. In [S. Lloyd, M. Mohseni and P. Rebentrost, Nature Physics
10, 631-633, (2014)], a quantum algorithm to implement principal component
analysis on quantum computer was obtained by computing the Hamiltonian
simulation of unknown density operators. The complexity is \(O((\log
d)t^2/\epsilon)\), where \(d\) is the dimension, \(t\) is the evolution time and
\(\epsilon\) is the precision. We improve this result into \(O((\log
d)t^{1+\frac{1}{k}}/\epsilon^{\frac{1}{k}})\) for arbitrary constant integer
\(k\geq 1\). As a result, we show that the Hamiltonian simulation of low-rank
dense Hermitian matrices can be implemented in the same time.">"An Improved Algorithm for Quantum Principal Component Analysis"</a>,
Changpeng Shao,
arXiv: <a href="http://arxiv.org/abs/1903.03999v2">1903.03999</a>,
3/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.10445v1" title="Neural networks enjoy widespread success in both research and industry and,
with the imminent advent of quantum technology, it is now a crucial challenge
to design quantum neural networks for fully quantum learning tasks. Here we
propose the use of quantum neurons as a building block for quantum feed-forward
neural networks capable of universal quantum computation. We describe the
efficient training of these networks using the fidelity as a cost function and
provide both classical and efficient quantum implementations. Our method allows
for fast optimisation with reduced memory requirements: the number of qudits
required scales with only the width, allowing the optimisation of deep
networks. We benchmark our proposal for the quantum task of learning an unknown
unitary and find remarkable generalisation behaviour and a striking robustness
to noisy training data.">"Efficient Learning for Deep Quantum Neural Networks"</a>,
Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, Ramona Wolf,
arXiv: <a href="http://arxiv.org/abs/1902.10445v1">1902.10445</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.08907v1" title="In this work, we investigate how to speed up machine learning tasks based on
quantum computation. We show an important classifier in machine learning, the
twin support vector machine, can be exponentially speeded up on quantum
computers. Specifically, for a training set with \(m\) samples which are
represented by \(n\)-dimensional vectors, the proposed quantum algorithm can
learn two non-parallel hyperplanes in \(O(\mathrm{log}\ mn)\) time, and then
classify a new sample in \(O(\mathrm{log}\ n)\) time by comparing the distances
from the simple to the two hyperplanes. Note that the classical algorithm
requires polynomial time both in the training and classification procedures.">"Quantum speedup for twin support vector machine"</a>,
Zekun Ye, Lvzhou Li, Haozhen Situ,
arXiv: <a href="http://arxiv.org/abs/1902.08907v1">1902.08907</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.08753v1" title="The problem of learning Boolean linear functions from quantum examples w.r.t.
the uniform distribution can be solved on a quantum computer using the
Bernstein-Vazirani algorithm. A similar strategy can be applied in the case of
noisy quantum training data, as was observed in arXiv:1702.08255v2 [quant-ph].
We employ the biased quantum Fourier transform introduced in arXiv:1802.05690v2
[quant-ph] to develop quantum algorithms for learning Boolean linear functions
from quantum examples w.r.t. a biased product distribution. Here, one procedure
is applicable to any (except full) bias, the other gives a better performance
but is applicable only for small bias. Moreover, we discuss the stability of
the second procedure w.r.t. noisy training data and w.r.t. faulty quantum
gates. The latter also enables us to solve a version of the problem where the
underlying distribution is not known in advance. Finally, we prove lower bounds
on the classical and quantum sample complexities of the learning problem and
compare these to the upper bounds implied by our algorithms.">"Quantum Learning Boolean Linear Functions w.r.t. Product Distributions"</a>,
Matthias C. Caro,
arXiv: <a href="http://arxiv.org/abs/1902.08753v1">1902.08753</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.06573v1" title="Complexity of materials designed by machine learning is currently limited by
the inefficiency of classical computers. We show how quantum annealing can be
incorporated into automated materials discovery and conduct a
proof-of-principle study on designing complex thermofunctional metamaterials
consisting of SiO2, SiC, and Poly(methyl methacrylate). Empirical computing
time of our quantum-classical hybrid algorithm involving a factorization
machine, a rigorous coupled wave analysis, and a D-Wave 2000Q quantum annealer
was insensitive to the problem size, while a classical counterpart experienced
rapid increase. Our method was used to design complex structures of wavelength
selective radiators showing much better concordance with the thermal
atmospheric transparency window in comparison to existing human-designed
alternatives. Our result shows that quantum annealing provides scientists
gigantic computational power that may change how materials are designed.">"Expanding the horizon of automated metamaterials discovery via quantum
  annealing"</a>,
Koki Kitai, Jiang Guo, Shenghong Ju, Shu Tanaka, Koji Tsuda, Junichiro Shiomi, Ryo Tamura,
arXiv: <a href="http://arxiv.org/abs/1902.06573v1">1902.06573</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.05162v1" title="We present a representation for linguistic structure that we call a
Fock-space representation, which allows us to embed problems in language
processing into small quantum devices. We further develop a formalism for
understanding both classical as well as quantum linguistic problems and phrase
them both as a Harmony optimization problem that can be solved on a quantum
computer which we show is related to classifying vectors using quantum
Boltzmann machines. We further provide a new training method for learning
quantum Harmony operators that describe a language. This also provides a new
algorithm for training quantum Boltzmann machines that requires no
approximations and works in the presence of hidden units. We additionally show
that quantum language processing is BQP-complete, meaning that it is
polynomially equivalent to the circuit model of quantum computing which implies
that quantum language models are richer than classical models unless BPP=BQP.
It also implies that, under certain circumstances, quantum Boltzmann machines
are more expressive than classical Boltzmann machines. Finally, we examine the
performance of our approach numerically for the case of classical harmonic
grammars and find that the method is capable of rapidly parsing even
non-trivial grammars. This suggests that the work may have value as a quantum
inspired algorithm beyond its initial motivation as a new quantum algorithm.">"Quantum Language Processing"</a>,
Nathan Wiebe, Alex Bocharov, Paul Smolensky, Matthias Troyer, Krysta M Svore,
arXiv: <a href="http://arxiv.org/abs/1902.05162v1">1902.05162</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.01879v2" title="We present a quantum machine learning algorithm for training Sparse Support
Vector Machine, a linear classifier that minimizes the hinge loss and the \(L_1\)
norm of the feature weights vector. Sparse SVM results in a classifier that
uses only a small fraction of the input features in making decisions, and is
especially suitable for cases where the total number of features is at the same
order, or larger, than the number of training samples. The algorithm utilizes
recently proposed quantum solvers for semidefinite programming and linear
programming problems. We show that while for an arbitrary binary classification
problem no quantum speedup is achieved by using quantum SDP/LP solvers during
training, there are realistic scenarios in which using a sparse linear
classifier makes sense in terms of the expected accuracy of predictions, and
polynomial quantum speedup compared to classical methods can be achieved.">"Quantum Sparse Support Vector Machines"</a>,
Tomasz Arodz, Seyran Saeedi,
arXiv: <a href="http://arxiv.org/abs/1902.01879v2">1902.01879</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1902.00869v1" title="In classical machine learning, a set of weak classifiers can be adaptively
combined to form a strong classifier for improving the overall performance, a
technique called adaptive boosting (or AdaBoost). However, constructing the
strong classifier for a large data set is typically resource consuming. Here we
propose a quantum extension of AdaBoost, demonstrating a quantum algorithm that
can output the optimal strong classifier with a quadratic speedup in the number
of queries of the weak classifiers. Our results also include a generalization
of the standard AdaBoost to the cases where the output of each classifier may
be probabilistic even for the same input. We prove that the update rules and
the query complexity of the non-deterministic classifiers are the same as those
of deterministic classifiers, which may be of independent interest to the
classical machine-learning community. Furthermore, the AdaBoost algorithm can
also be applied to data encoded in the form of quantum states; we show how the
training set can be simplified by using the tools of t-design. Our approach
describes a model of quantum machine learning where quantum speedup is achieved
in finding the optimal classifier, which can then be applied for classical
machine-learning applications.">"Quantum Speedup in Adaptive Boosting of Binary Classification"</a>,
Ximing Wang, Yuechi Ma, Min-Hsiu Hsieh, Manhong Yung,
arXiv: <a href="http://arxiv.org/abs/1902.00869v1">1902.00869</a>,
2/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.11434v1" title="The topic area of this paper parameterized quantum circuits (quantum neural
networks) which are trained to estimate a given function, specifically the type
of circuits proposed by Mitarai et al. (Phys. Rev. A, 2018). The input is
encoded into amplitudes of states of qubits. The no-cloning principle of
quantum mechanics suggests that there is an advantage in redundantly encoding
the input value several times. We follow this suggestion and prove lower bounds
on the number of redundant copies for two types of input encoding. We draw
conclusions for the architecture design of QNNs.">"Input Redundancy for Parameterized Quantum Circuits"</a>,
Javier Gil Vidal, Dirk Oliver Theis,
arXiv: <a href="http://arxiv.org/abs/1901.11434v1">1901.11434</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.10632v1" title="Quantum walks are at the heart of modern quantum technologies. They allow to
deal with quantum transport phenomena and are an advanced tool for constructing
novel quantum algorithms. Quantum walks on graphs are fundamentally different
from classical random walks analogs, in particular, they walk faster than
classical ones on certain graphs, enabling in these cases quantum algorithmic
applications and quantum-enhanced energy transfer. However, little is known
about the possible speedups on arbitrary graphs not having explicit symmetries.
For these graphs one would need to perform simulations of classical and quantum
walk dynamics to check if the speedup occurs, which could take a long
computational time. Here we present a new approach for the solution of the
quantum speedup problem, which is based on a machine learning algorithm that
detects the quantum advantage by just looking at a graph. The convolutional
neural network, which we designed specifically to learn from graphs, observes
simulated examples and learns complex features of graphs that lead to a quantum
speedup, allowing to identify graphs that exhibit quantum speedup without
performing any quantum walk or random walk simulations. Our findings pave the
way to an automated elaboration of novel large-scale quantum circuits utilizing
quantum walk based algorithms, and to simulating high-efficiency energy
transfer in biophotonics and material science.">"Detecting quantum speedup by quantum walk with convolutional neural
  networks"</a>,
Alexey A. Melnikov, Leonid E. Fedichkin, Alexander Alodjants,
arXiv: <a href="http://arxiv.org/abs/1901.10632v1">1901.10632</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.09133v1" title="Deep learning is a modern approach to realize artificial intelligence. Many
frameworks exist to implement the machine learning task; however, performance
is limited by computing resources. Using a quantum computer to accelerate
training is a promising approach. The variational quantum circuit (VQC) has
gained a great deal of attention because it can be run on near-term quantum
computers. In this paper, we establish a new framework that merges traditional
machine learning tasks with the VQC. Users can implement a trainable quantum
operation into a neural network. This framework enables the training of a
quantum-classical hybrid task and may lead to a new area of quantum machine
learning.">"VQNet: Library for a Quantum-Classical Hybrid Neural Network"</a>,
Zhao-Yun Chen, Cheng Xue, Si-Ming Chen, Guo-Ping Guo,
arXiv: <a href="http://arxiv.org/abs/1901.09133v1">1901.09133</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.07653v2" title="An efficient way to compute Hamiltonian ground-states on a quantum computer
stands to impact many problems in the physical and computer sciences, ranging
from quantum simulation to machine learning. Unfortunately, existing
techniques, such as phase estimation and variational algorithms, display formal
and practical disadvantages, such as requirements for deep circuits and
high-dimensional optimization. We describe the quantum imaginary time evolution
and quantum Lanczos algorithms, analogs of classical algorithms for ground (and
excited) states, but with exponentially reduced space and time requirements per
iteration, and without deep circuits, ancillae, or high-dimensional non-linear
optimization. We further discuss quantum imaginary time evolution as a natural
subroutine to generate Gibbs averages through an analog of minimally entangled
typical thermal states. We implement these algorithms with exact classical
emulation as well as in prototype circuits on the Rigetti quantum virtual
machine and Aspen-1 quantum processing unit, demonstrating the power of quantum
elevations of classical algorithms.">"Quantum Imaginary Time Evolution, Quantum Lanczos, and Quantum Thermal
  Averaging"</a>,
Mario Motta, Chong Sun, Adrian Teck Keng Tan, Matthew J. O' Rourke, Erika Ye, Austin J. Minnich, Fernando G. S. L. Brandao, Garnet Kin-Lic Chan,
arXiv: <a href="http://arxiv.org/abs/1901.07653v2">1901.07653</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.06901v2" title="Quantum annealing (QA) is a generic method for solving optimization problems
using fictitious quantum fluctuation. The current device performing QA involves
controlling the transverse field; it is classically simulatable by using the
standard technique for mapping the quantum spin systems to the classical ones.
In this sense, the current system for QA is not powerful despite utilizing
quantum fluctuation. Hence, we developed a system with a time-dependent
Hamiltonian consisting of a combination of the formulated Ising model and the
&quot;driver&quot; Hamiltonian with only quantum fluctuation. In the previous study, for
a fully connected spin model, quantum fluctuation can be addressed in a
relatively simple way. We proved that the fully connected antiferromagnetic
interaction can be transformed into a fluctuating transverse field and is thus
classically simulatable at sufficiently low temperatures. Using the fluctuating
transverse field, we established several ways to simulate part of the
nonstoquastic Hamiltonian on classical computers. We formulated a
message-passing algorithm in the present study. This algorithm is capable of
assessing the performance of QA with part of the nonstoquastic Hamiltonian
having a large number of spins. In other words, we developed a different
approach for simulating the nonstoquastic Hamiltonian without using the quantum
Monte Carlo technique. Our results were validated by comparison to the results
obtained by the replica method.">"Message-passing algorithm of quantum annealing with nonstoquastic
  Hamiltonian"</a>,
Masayuki Ohzeki,
arXiv: <a href="http://arxiv.org/abs/1901.06901v2">1901.06901</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.07535v2" title="Recent developments in the field of deep learning have motivated many
researchers to apply these methods to problems in quantum information. Torlai
and Melko first proposed a decoder for surface codes based on neural networks.
Since then, many other researchers have applied neural networks to study a
variety of problems in the context of decoding. An important development in
this regard was due to Varsamopoulos et al. who proposed a two-step decoder
using neural networks. Subsequent work of Maskara et al. used the same concept
for decoding for various noise models. We propose a similar two-step neural
decoder using inverse parity-check matrix for topological color codes. We show
that it outperforms the state-of-the-art performance of non-neural decoders for
independent Pauli errors noise model on a 2D hexagonal color code. Our final
decoder is independent of the noise model and achieves a threshold of \(10 \%\).
Our result is comparable to the recent work on neural decoder for quantum error
correction by Maskara et al.. It appears that our decoder has significant
advantages with respect to training cost and complexity of the network for
higher lengths when compared to that of Maskara et al.. Our proposed method can
also be extended to arbitrary dimension and other stabilizer codes.">"Neural Decoder for Topological Codes using Pseudo-Inverse of Parity
  Check Matrix"</a>,
Chaitanya Chinni, Abhishek Kulkarni, Dheeraj M. Pai, Kaushik Mitra, Pradeep Kiran Sarvepalli,
arXiv: <a href="http://arxiv.org/abs/1901.07535v2">1901.07535</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.05374v1" title="A broad class of hybrid quantum-classical algorithms known as &quot;variational
algorithms&quot; have been proposed in the context of quantum simulation, machine
learning, and combinatorial optimization as a means of potentially achieving a
quantum speedup on a near-term quantum device for a problem of practical
interest. Such algorithms use the quantum device only to prepare parameterized
quantum states and make simple measurements. A classical controller uses the
measurement results to perform an optimization of a classical function induced
by a quantum observable which defines the problem. While most prior works have
considered optimization strategies based on estimating the objective function
and doing a derivative-free or finite-difference-based optimization, some
recent proposals involve directly measuring observables corresponding to the
gradient of the objective function. The measurement procedure needed requires
coherence time barely longer than that needed to prepare a trial state. We
prove that strategies based on such gradient measurements can admit
substantially faster rates of convergence to the optimum in some contexts. We
first introduce a natural black-box setting for variational algorithms which we
prove our results with respect to. We define a simple class of problems for
which a variational algorithm based on low-depth gradient measurements and
stochastic gradient descent converges to the optimum substantially faster than
any possible strategy based on estimating the objective function itself, and
show that stochastic gradient descent is essentially optimal for this problem.
Importing known results from the stochastic optimization literature, we also
derive rigorous upper bounds on the cost of variational optimization in a
convex region when using gradient measurements in conjunction with certain
stochastic gradient descent or stochastic mirror descent algorithms.">"Low-depth gradient measurements can improve convergence in variational
  hybrid quantum-classical algorithms"</a>,
Aram Harrow, John Napp,
arXiv: <a href="http://arxiv.org/abs/1901.05374v1">1901.05374</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.02462v2" title="With the increasing crossover between quantum information and machine
learning, quantum simulation of neural networks has drawn unprecedentedly
strong attention, especially for the simulation of associative memory in
Hopfield neural networks due to their wide applications and relatively simple
structures that allow for easier mapping to the quantum regime. Quantum
stochastic walk, a strikingly powerful tool to analyze quantum dynamics, has
been recently proposed to simulate the firing pattern and associative memory
with a dependence on Hamming Distance. We successfully map the theoretical
scheme into a three-dimensional photonic quantum chip and realize quantum
stochastic walk evolution through well-controlled detunings of the propagation
constant. We demonstrate a good match rate of the associative memory between
the experimental quantum scheme and the expected result for Hopfield neural
networks. The ability of quantum simulation for an important feature of a
neural network, combined with the scalability of our approach through low-loss
integrated chip and straightforward Hamiltonian engineering, provides a primary
but steady step towards photonic artificial intelligence devices for
optimization and computation tasks of greatly improved efficiencies.">"Experimental quantum stochastic walks simulating associative memory of
  Hopfield neural networks"</a>,
Hao Tang, Zhen Feng, Ying-Han Wang, Peng-Cheng Lai, Chao-Yue Wang, Zhuo-Yang Ye, Cheng-Kai Wang, Zi-Yu Shi, Tian-Yu Wang, Yuan Chen, Jun Gao, Xian-Min Jin,
arXiv: <a href="http://arxiv.org/abs/1901.02462v2">1901.02462</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.01653v2" title="In this paper, we develop a theory of learning nonlinear input-output maps
with fading memory by dissipative quantum systems, as a quantum counterpart of
the theory of approximating such maps using classical dynamical systems. The
theory identifies the properties required for a class of dissipative quantum
systems to be {\em universal}, in that any input-output map with fading memory
can be approximated arbitrarily closely by an element of this class. We then
introduce an example class of dissipative quantum systems that is provably
universal. Numerical experiments illustrate that with a small number of qubits,
this class can achieve comparable performance to classical learning schemes
with a large number of tunable parameters. Further numerical analysis suggests
that the exponentially increasing Hilbert space presents a potential resource
for dissipative quantum systems to surpass classical learning schemes for
input-output maps.">"Learning Nonlinear Input-Output Maps with Dissipative Quantum Systems"</a>,
Jiayin Chen, Hendra I. Nurdin,
arXiv: <a href="http://arxiv.org/abs/1901.01653v2">1901.01653</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.01027v1" title="Conditional random field (CRF) is an important probabilistic machine learning
model for labeling sequential data, which is widely utilized in natural
language processing, bioinformatics and computer vision. However, training the
CRF model is computationally intractable when large scale training samples are
processed. Since little work has been done for labeling sequential data in the
quantum settings, we in this paper construct a quantum CRF (QCRF) model by
introducing well-defined Hamiltonians and measurements, and present a quantum
algorithm to train this model. It is shown that the algorithm achieves an
exponential speed-up over its classical counterpart. Furthermore, we also
demonstrate that the QCRF model possesses higher Vapnik Chervonenkis dimension
than the classical CRF model, which means QCRF is equipped with a higher
learning ability.">"Quantum Conditional Random Field"</a>,
Yusen Wu, Chao-Hua Yu, Binbin Cai, Sujuan Qin, Fei Gao, Qiaoyan Wen,
arXiv: <a href="http://arxiv.org/abs/1901.01027v1">1901.01027</a>,
1/2019</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1901.00848v1" title="We propose a hybrid quantum-classical approach to model continuous classical
probability distributions using a variational quantum circuit. The architecture
of the variational circuit consists of two parts: a quantum circuit employed to
encode a classical random variable into a quantum state, called the quantum
encoder, and a variational circuit whose parameters are optimized to mimic a
target probability distribution. Samples are generated by measuring the
expectation values of a set of operators chosen at the beginning of the
calculation. Our quantum generator can be complemented with a classical
function, such as a neural network, as part of the classical post-processing.
We demonstrate the application of the quantum variational generator using a
generative adversarial learning approach, where the quantum generator is
trained via its interaction with a discriminator model that compares the
generated samples with those coming from the real data distribution. We show
that our quantum generator is able to learn target probability distributions
using either a classical neural network or a variational quantum circuit as the
discriminator. Our implementation takes advantage of automatic differentiation
tools to perform the optimization of the variational circuits employed. The
framework presented here for the design and implementation of variational
quantum generators can serve as a blueprint for designing hybrid
quantum-classical architectures for other machine learning tasks on near-term
quantum devices.">"Variational quantum generators: Generative adversarial quantum machine
  learning for continuous distributions"</a>,
Jonathan Romero, Alan Aspuru-Guzik,
arXiv: <a href="http://arxiv.org/abs/1901.00848v1">1901.00848</a>,
1/2019</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></p>
        </section>
      </div>

  </section>

  <!-- Two: Including columns with links to most recent articles -->
  <!--
    <section class="wrapper style1 container special">
        <div class="row">
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/machine-learning-for-quantum-design-at-pi.html" rel='bookmark'><h3>Machine Learning for Quantum Design at PI</h3></a>
              </header>
              <p>The Perimeter Institute for Theoretical Physics in Waterloo, Canada, hosts "</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/machine-learning-for-quantum-design-at-pi.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" rel='bookmark'><h3>Workshop Machine Learning for Quantum Technology at MPL Erlangen</h3></a>
              </header>
              <p>The Max Planck Institute for the Science of Light in Erlangen, Germany, hosts <a href="https://www.mpl.mpg.de/divisions/marquardt-division/workshops/2019-machi</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/DL-theory.html" rel='bookmark'><h3>The Theory of Deep Learning - Part I</h3></a>
              </header>
              <p>Why do modern deep neural networks (DNNs) perform so well on previously unseen test data, even when their number of weights is much larger than the number of data points</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/DL-theory.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
        </div>
    </section>  -->
  </article>

<!-- Footer -->
<footer id="footer">

    <ul class="icons">
    </ul>

    <span class="copyright">&copy; physicsml. All rights reserved. <br> Powered by <a href="https://blog.getpelican.com">Pelican</a>. Theme Twenty, Design: <a href="http://html5up.net">HTML5 UP</a>. Implemented and maintained by Anna Go and Danny Kong.</span>

</footer>
</body>
</html>
