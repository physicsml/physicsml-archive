<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.dropotron.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/skel-layers.min.js"></script>
    <script src="/js/init.js"></script>
    <link rel="stylesheet" href="/css/pygment.css" />
    <noscript>
        <link rel="stylesheet" href="/css/skel.css" />
        <link rel="stylesheet" href="/css/style.css" />
        <link rel="stylesheet" href="/css/style-noscript.css" />
    </noscript>
    <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
    <!-- <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" integrity="sha384-3AB7yXWz4OeoZcPbieVW64vVXEwADiYyAEhwilzWsLw+9FgqpyjjStpPnpBO8o8S" crossorigin="anonymous">
    <link  href="http://fonts.googleapis.com/css?family=Anonymous+Pro:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" >
    <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie/v8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie/v9.css" /><![endif]-->

    <title>Papers | &#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297; </title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width">
</head>

<body class=" loading">



    <!-- Header -->
    <header id="header" >
        <h1 class="logo">
            <a href="..">&#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297;</a>
        </h1>
        <nav id="nav">
            <ul>
                <!-- <li class="current"><a href="index.html">Welcome</a></li> -->
                    <li><a href="/category/news.html">News</a></li>
                    <li><a href="/category/articles.html">Blog</a></li>
                    <li class="submenu">
                            <a href="#">Papers</a>
                            <ul style="list-style: none;">
                                    <!-- <li><a href="/pages/papers-19.html">2019</a></li> -->
                                    <li><a href="/pages/papers-18.html">2018</a></li>
                                    <li><a href="/pages/papers-17.html">2017</a></li>
                                    <li><a href="/pages/papers-16.html">2016</a></li>
                                    <li><a href="/pages/papers-15.html">2015</a></li>
                            </ul>
                    </li>
                <!--
                <li class="submenu">
                    <a href="../">No Blog</a>
                    <ul>
                            <li >
                                <a href="../category/articles/">Articles</a>
                            </li>
                            <li >
                                <a href="../category/news/">News</a>
                            </li>
                    </ul>
                </li>
                -->
                <!--
                <li><a href="#" class="button special">Nothing to Sign Up to</a></li>
                -->
            </ul>
        </nav>
    </header>

<!-- Main -->
<article id="main">

  <header class="special container">
    <span class="icon fas fa-paperclip"></span>
    <h2>Papers</h2>
    <!-- add page sub title here -->
    <p>The following are recent papers combining the fields of physics - especially quantum mechanics - and machine learning. </br> Please email <a href="mailto:agolubeva@pitp.ca">Anna Go</a> if you would like to see a paper added to this page.</p>
  </header>

  <!-- One -->
  <section class="wrapper style4 container">

    <!-- Content -->
      <div class="content">
        <section>
          <!-- <a href="#" class="image feature"><img src="images/pic04.jpg" alt="" /></a> -->
          <p><h2>Applying Machine Learning to Physics</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1712.06863v1" title="The difficulty of validating large-scale quantum devices, such as Boson
Samplers, poses a major challenge for any research program that aims to show
quantum advantages over classical hardware. To address this problem, we propose
a novel data-driven approach wherein models are trained to identify common
pathologies using unsupervised machine learning methods. We illustrate this
idea by training a classifier that exploits K-means clustering to distinguish
between Boson Samplers that use indistinguishable photons from those that do
not. We train the model on numerical simulations of small-scale Boson Samplers
and then validate the pattern recognition technique on larger numerical
simulations as well as on photonic chips in both traditional Boson Sampling and
scattershot experiments. The effectiveness of such method relies on
particle-type-dependent internal correlations present in the output
distributions. This approach performs substantially better on the test data
than previous methods and underscores the ability to further generalize its
operation beyond the scope of the examples that it was trained on.">"Pattern recognition techniques for Boson Sampling validation"</a>,
Iris Agresti, Niko Viggianiello, Fulvio Flamini, Nicolò Spagnolo, Andrea Crespi, Roberto Osellame, Nathan Wiebe, Fabio Sciarrino,
arXiv: <a href="http://arxiv.org/abs/1712.06863v1">1712.06863</a>,
12/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1712.03893v1" title="In this paper we propose new algorithm to reduce autocorrelation in Markov
chain Monte-Carlo algorithms for euclidean field theories on the lattice. Our
proposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted
Boltzmann machine. We examine the validity of the algorithm by employing the
phi-fourth theory in three dimension. We observe reduction of the
autocorrelation both in symmetric and broken phase as well. Our proposing
algorithm provides consistent central values of expectation values of the
action density and one-point Green's function with ones from the original HMC
in both the symmetric phase and broken phase within the statistical error. On
the other hand, two-point Green's functions have slight difference between one
calculated by the HMC and one by our proposing algorithm in the symmetric
phase. Furthermore, near the criticality, the distribution of the one-point
Green's function differs from the one from HMC. We discuss the origin of
discrepancies and its improvement.">"Towards reduction of autocorrelation in HMC by machine learning"</a>,
Akinori Tanaka, Akio Tomiya,
arXiv: <a href="http://arxiv.org/abs/1712.03893v1">1712.03893</a>,
12/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1712.00371v1" title="We detect the quantum phase transition of a quantum many-body system by
mapping the observed results of the quantum state onto a neural network. In the
present study, we utilized the simplest case of a quantum many-body system,
namely a one-dimensional chain of Ising spins with the transverse Ising model.
We prepared several spin configurations, which were obtained using repeated
observations of the model for a particular strength of the transverse field, as
input data for the neural network. Although the proposed method can be employed
using experimental observations of quantum many-body systems, we tested our
technique with spin configurations generated by a quantum Monte Carlo
simulation without initial relaxation. The neural network successfully
classified the strength of transverse field only from the spin configurations,
leading to consistent estimations of the critical point of our model \(\Gamma_c
=J\).">"Deep Neural Network Detects Quantum Phase Transition"</a>,
Shunta Arai, Masayuki Ohzeki, Kazuyuki Tanaka,
arXiv: <a href="http://arxiv.org/abs/1712.00371v1">1712.00371</a>,
12/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1712.00127v1" title="The number of parameters describing a quantum state is well known to grow
exponentially with the number of particles. This scaling clearly limits our
ability to do tomography to systems with no more than a few qubits and has been
used to argue against the universal validity of quantum mechanics itself.
However, from a computational learning theory perspective, it can be shown
that, in a probabilistic setting, quantum states can be approximately learned
using only a linear number of measurements. Here we experimentally demonstrate
this linear scaling in optical systems with up to 6 qubits. Our results
highlight the power of computational learning theory to investigate quantum
information, provide the first experimental demonstration that quantum states
can be &quot;probably approximately learned&quot; with access to a number of copies of
the state that scales linearly with the number of qubits, and pave the way to
probing quantum states at new, larger scales.">"Experimental learning of quantum states"</a>,
Andrea Rocchetto, Scott Aaronson, Simone Severini, Gonzalo Carvacho, Davide Poderini, Iris Agresti, Marco Bentivegna, Fabio Sciarrino,
arXiv: <a href="http://arxiv.org/abs/1712.00127v1">1712.00127</a>,
11/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.09842v1" title="Efficient and automated classification of phases from minimally processed
data is one goal of machine learning in condensed matter and statistical
physics. Supervised algorithms trained on raw samples of microstates can
successfully detect conventional phase transitions via learning a bulk feature
such as an order parameter. In this paper, we investigate whether neural
networks can learn to classify phases based on topological defects. We address
this question on the two-dimensional classical XY model which exhibits a
Kosterlitz-Thouless transition. We find significant feature engineering of the
raw spin states is required to convincingly claim that features of the vortex
configurations are responsible for learning the transition temperature. We
further show a single-layer network does not correctly classify the phases of
the XY model, while a convolutional network easily performs classification by
learning the global magnetization. Finally, we design a deep network capable of
learning vortices without feature engineering. We demonstrate the detection of
vortices does not necessarily result in the best classification accuracy,
especially for lattices of less than approximately 1000 spins. For larger
systems, it remains a difficult task to learn vortices.">"Machine learning vortices at the Kosterlitz-Thouless transition"</a>,
Matthew J. S. Beach, Anna Golubeva, Roger G. Melko,
arXiv: <a href="http://arxiv.org/abs/1710.09842v1">1710.09842</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1711.00020v1" title="Neural network based machine learning is emerging as a powerful tool for
obtaining phase diagrams when traditional regression schemes using local
equilibrium order parameters are not available, as in many-body localized or
topological phases. Nevertheless, instances of machine learning offering new
insights have been rare up to now. Here we show that a single feed-forward
neural network can decode the defining structures of two distinct MBL phases
and a thermalizing phase, using entanglement spectra obtained from individual
eigenstates. For this, we introduce a simplicial geometry based method for
extracting multi-partite phase boundaries. We find that this method outperforms
conventional metrics (like the entanglement entropy) for identifying MBL phase
transitions, revealing a sharper phase boundary and shedding new insight into
the topology of the phase diagram. Furthermore, the phase diagram we acquire
from a single disorder configuration confirms that the machine-learning based
approach we establish here can enable speedy exploration of large phase spaces
that can assist with the discovery of new MBL phases. To our knowledge this
work represents the first example of a machine learning approach revealing new
information beyond conventional knowledge.">"Machine learning out-of-equilibrium phases of matter"</a>,
Jordan Venderley, Vedika Khemani, Eun-Ah Kim,
arXiv: <a href="http://arxiv.org/abs/1711.00020v1">1711.00020</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.00725v1" title="Studying general quantum many-body systems is one of the major challenges in
modern physics because it requires an amount of computational resources that
scales exponentially with the size of the system.Simulating the evolution of a
state, or even storing its description, rapidly becomes intractable for exact
classical algorithms. Recently, machine learning techniques, in the form of
restricted Boltzmann machines, have been proposed as a way to efficiently
represent certain quantum states with applications in state tomography and
ground state estimation. Here, we introduce a new representation of states
based on variational autoencoders. Variational autoencoders are a type of
generative model in the form of a neural network. We probe the power of this
representation by encoding probability distributions associated with states
from different classes. Our simulations show that deep networks give a better
representation for states that are hard to sample from, while providing no
benefit for random states. This suggests that the probability distributions
associated to hard quantum states might have a compositional structure that can
be exploited by layered neural networks. Specifically, we consider the
learnability of a class of quantum states introduced by Fefferman and Umans.
Such states are provably hard to sample for classical computers, but not for
quantum ones, under plausible computational complexity assumptions. The good
level of compression achieved for hard states suggests these methods can be
suitable for characterising states of the size expected in first generation
quantum hardware.">"Learning hard quantum distributions with variational autoencoders"</a>,
Andrea Rocchetto, Edward Grant, Sergii Strelchuk, Giuseppe Carleo, Simone Severini,
arXiv: <a href="http://arxiv.org/abs/1710.00725v1">1710.00725</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.08015v1" title="Our understanding of supercooled liquids and glasses has lagged significantly
behind that of simple liquids and crystalline solids. This is in part due to
the many possibly relevant degrees of freedom that are present due to the
disorder inherent to these systems and in part to non-equilibrium effects which
are difficult to treat in the standard context of statistical physics. Together
these issues have resulted in a field whose theories are under-constrained by
experiment and where fundamental questions are still unresolved. Mean field
results have been successful in infinite dimensions but it is unclear to what
extent they apply to realistic systems and assume uniform local structure. At
odds with this are theories premised on the existence of structural defects.
However, until recently it has been impossible to find structural signatures
that are predictive of dynamics. Here we summarize and recast the results from
several recent papers offering a data driven approach to building a
phenomenological theory of disordered materials by combining machine learning
with physical intuition.">"Combining Machine Learning and Physics to Understand Glassy Systems"</a>,
Samuel S. Schoenholz,
arXiv: <a href="http://arxiv.org/abs/1709.08015v1">1709.08015</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.05790v2" title="In this letter, we apply the artificial neural network in a supervised manner
to map out the quantum phase diagram of disordered topological superconductor
in class DIII. Given the disorder that keeps the discrete symmetries of the
ensemble as a whole, translational symmetry which is broken in the
quasiparticle distribution individually is recovered statistically by taking an
ensemble average. By using this, we classify the phases by the artificial
neural network that learned the quasiparticle distribution in the clean limit,
and show that the result is totally consistent with the calculation by the
transfer matrix method or noncommutative geometry approach. If all three
phases, namely the \(\mathbb{Z}_2\), trivial, and the thermal metal phases appear
in the clean limit, the machine can classify them with high confidence over the
entire phase diagram. If only the former two phases are present, we find that
the machine remains confused in the certain region, leading us to conclude the
detection of the unknown phase which is eventually identified as the thermal
metal phase. In our method, only the first moment of the quasiparticle
distribution is used for input, but application to a wider variety of systems
is expected by the inclusion of higher moments.">"Learning Disordered Topological Phases by Statistical Recovery of
  Symmetry"</a>,
Nobuyuki Yoshioka, Yutaka Akagi, Hosho Katsura,
arXiv: <a href="http://arxiv.org/abs/1709.05790v2">1709.05790</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.06475v2" title="We develop a machine learning method to construct accurate ground-state wave
functions of strongly interacting and entangled quantum spin as well as
fermionic models on lattices. A restricted Boltzmann machine algorithm in the
form of an artificial neural network is combined with a conventional
variational Monte Carlo method with pair product (geminal) wave functions and
quantum number projections. The combination allows an application of the
machine learning scheme to interacting fermionic systems. The combined method
substantially improves the accuracy beyond that ever achieved by each method
separately, in the Heisenberg as well as Hubbard models on square lattices,
thus proving its power as a highly accurate quantum many-body solver.">"Restricted-Boltzmann-Machine Learning for Solving Strongly Correlated
  Quantum Systems"</a>,
Yusuke Nomura, Andrew S. Darmawan, Youhei Yamaji, Masatoshi Imada,
arXiv: <a href="http://arxiv.org/abs/1709.06475v2">1709.06475</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.02597v1" title="Unsupervised machine learning via a restricted Boltzmann machine is an useful
tool in distinguishing an ordered phase from a disordered phase. Here we study
its application on the two-dimensional Ashkin-Teller model, which features a
partially ordered product phase. We train the neural network with spin
configuration data generated by Monte Carlo simulations and show that distinct
features of the product phase can be learned from non-ergodic samples resulting
from symmetry breaking. Careful analysis of the weight matrices inspires us to
define a nontrivial machine-learning motivated quantity of the product form,
which resembles the conventional product order parameter.">"Identifying Product Order with Restricted Boltzmann Machines"</a>,
Wen-Jia Rao, Zhenyu Li, Qiong Zhu, Mingxing Luo, Xin Wan,
arXiv: <a href="http://arxiv.org/abs/1709.02597v1">1709.02597</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.02779v1" title="Quantum information technologies, and intelligent learning systems, are both
emergent technologies that will likely have a transforming impact on our
society. The respective underlying fields of research -- quantum information
(QI) versus machine learning (ML) and artificial intelligence (AI) -- have
their own specific challenges, which have hitherto been investigated largely
independently. However, in a growing body of recent work, researchers have been
probing the question to what extent these fields can learn and benefit from
each other. QML explores the interaction between quantum computing and ML,
investigating how results and techniques from one field can be used to solve
the problems of the other. Recently, we have witnessed breakthroughs in both
directions of influence. For instance, quantum computing is finding a vital
application in providing speed-ups in ML, critical in our &quot;big data&quot; world.
Conversely, ML already permeates cutting-edge technologies, and may become
instrumental in advanced quantum technologies. Aside from quantum speed-up in
data analysis, or classical ML optimization used in quantum experiments,
quantum enhancements have also been demonstrated for interactive learning,
highlighting the potential of quantum-enhanced learning agents. Finally, works
exploring the use of AI for the very design of quantum experiments, and for
performing parts of genuine research autonomously, have reported their first
successes. Beyond the topics of mutual enhancement, researchers have also
broached the fundamental issue of quantum generalizations of ML/AI concepts.
This deals with questions of the very meaning of learning and intelligence in a
world that is described by quantum mechanics. In this review, we describe the
main ideas, recent developments, and progress in a broad spectrum of research
investigating machine learning and artificial intelligence in the quantum
domain.">"Machine learning &amp; artificial intelligence in the quantum domain"</a>,
Vedran Dunjko, Hans J. Briegel,
arXiv: <a href="http://arxiv.org/abs/1709.02779v1">1709.02779</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.00812v2" title="The three-dimensional Anderson model is a well-studied model of disordered
electron systems that shows the delocalization--localization transition. As in
our previous papers on two- and three-dimensional (2D, 3D) quantum phase
transitions [J. Phys. Soc. Jpn. {\bf 85}, 123706 (2016), {\bf 86}, 044708
(2017)], we used an image recognition algorithm based on a multilayered
convolutional neural network. However, in contrast to previous papers in which
2D image recognition was used, we applied 3D image recognition to analyze
entire 3D wave functions. We show that a full phase diagram of the
disorder-energy plane is obtained once the 3D convolutional neural network has
been trained at the band center. We further demonstrate that the full phase
diagram for 3D quantum bond and site percolations can be drawn by training the
3D Anderson model at the band center.">"Phase Diagrams of Three-Dimensional Anderson and Quantum Percolation
  Models using Deep Three-Dimensional Convolutional Neural Network"</a>,
Tomohiro Mano, Tomi Ohtsuki,
arXiv: <a href="http://arxiv.org/abs/1709.00812v2">1709.00812</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.01223v2" title="Motivated by the close relations of the renormalization group with both the
holography duality and the deep learning, we propose that the holographic
geometry can emerge from deep learning the entanglement feature of a quantum
many-body state. We develop a concrete algorithm, call the entanglement feature
learning (EFL), based on the random tensor network (RTN) model for the tensor
network holography. We show that each RTN can be mapped to a Boltzmann machine,
trained by the entanglement entropies over all subregions of a given quantum
many-body state. The goal is to construct the optimal RTN that best reproduce
the entanglement feature. The RTN geometry can then be interpreted as the
emergent holographic geometry. We demonstrate the EFL algorithm on 1D free
fermion system and observe the emergence of the hyperbolic geometry (AdS\(_3\)
spatial geometry) as we tune the fermion system towards the gapless critical
point (CFT\(_2\) point).">"Machine Learning Spatial Geometry from Entanglement Features"</a>,
Yi-Zhuang You, Zhao Yang, Xiao-Liang Qi,
arXiv: <a href="http://arxiv.org/abs/1709.01223v2">1709.01223</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1708.09401v3" title="In this Letter we supervisedly train neural networks to distinguish different
topological phases in the context of topological band insulators. After
training with Hamiltonians of one-dimensional insulators with chiral symmetry,
the neural network can predict their topological winding numbers with nearly
100% accuracy, even for Hamiltonians with larger winding numbers that are not
included in the training data. These results show a remarkable success that the
neural network can capture the global and nonlinear topological features of
quantum phases from local inputs. By opening up the neural network, we confirm
that the network does learn the discrete version of the winding number formula.
We also make a couple of remarks regarding the role of the symmetry and the
opposite effect of regularization techniques when applying machine learning to
physical systems.">"Machine Learning Topological Invariants with Neural Networks"</a>,
Pengfei Zhang, Huitao Shen, Hui Zhai,
arXiv: <a href="http://arxiv.org/abs/1708.09401v3">1708.09401</a>,
8/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1708.06686v1" title="We present a procedure for training and evaluating a deep neural network
which can efficiently infer extensive parameters of arbitrarily large systems,
doing so with O(N) complexity. We use a form of domain decomposition for
training and inference, where each sub-domain (tile) is comprised of a
non-overlapping focus region surrounded by an overlapping context region. The
relative sizes of focus and context are physically motivated and depend on the
locality length scale of the problem. Extensive deep neural networks (EDNN) are
a formulation of convolutional neural networks which provide a flexible and
general approach, based on physical constraints, to describe multi-scale
interactions. They are well suited to massively parallel inference, as no
inter-thread communication is necessary during evaluation. Example uses for
learning simple spin models, Laplacian (derivative) operator, and approximating
many-body quantum mechanical operators (within the density functional theory
approach) are demonstrated.">"Extensive deep neural networks"</a>,
Iryna Luchak, Kyle Mills, Kevin Ryczko, Adam Domurad, Isaac Tamblyn,
arXiv: <a href="http://arxiv.org/abs/1708.06686v1">1708.06686</a>,
8/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1708.04762v2" title="We use determinant Quantum Monte Carlo (DQMC), in combination with the
principal component analysis (PCA) approach to unsupervised learning, to
extract information about phase transitions in several of the most fundamental
Hamiltonians describing strongly correlated materials. We first explore the
zero temperature antiferromagnet to singlet transition in the Periodic Anderson
Model, the Mott insulating transition in the Hubbard model on a honeycomb
lattice, and the magnetic transition in the 1/6-filled Lieb lattice. We then
discuss the prospects for learning finite temperature superconducting
transitions in the attractive Hubbard model, for which there is no sign
problem. Finally, we investigate finite temperature CDW transitions in the
Holstein model, where the electrons are coupled to phonon degrees of freedom.
We examine the different behaviors associated with providing
Hubbard-Stratonovich auxiliary fields configurations on both the entire
space-time lattice and on a single imaginary time slice, or other quantities,
such as equal-time Green's and pair-pair correlation functions.">"Learning Fermionic Critical Points"</a>,
Natanael C. Costa, Wenjian Hu, Z. J. Bai, Richard T. Scalettar, Rajiv R. P. Singh,
arXiv: <a href="http://arxiv.org/abs/1708.04762v2">1708.04762</a>,
8/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1708.04622v1" title="It is well established that neural networks with deep architectures perform
better than shallow networks for many tasks in machine learning. In statistical
physics, while there has been recent interest in representing physical data
with generative modelling, the focus has been on shallow neural networks. A
natural question to ask is whether deep neural networks hold any advantage over
shallow networks in representing such data. We investigate this question by
using unsupervised, generative graphical models to learn the probability
distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep
belief networks, and deep restricted Boltzmann networks are trained on thermal
spin configurations from this system, and compared to the shallow architecture
of the restricted Boltzmann machine. We benchmark the models, focussing on the
accuracy of generating energetic observables near the phase transition, where
these quantities are most difficult to approximate. Interestingly, after
training the generative networks, we observe that the accuracy essentially
depends only on the number of neurons in the first hidden layer of the network,
and not on other model details such as network depth or model type. This is
evidence that shallow networks are more efficient than deep networks at
representing physical probability distributions associated with Ising systems
near criticality.">"Deep Learning the Ising Model Near Criticality"</a>,
Alan Morningstar, Roger G. Melko,
arXiv: <a href="http://arxiv.org/abs/1708.04622v1">1708.04622</a>,
8/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1708.02917v2" title="The Restricted Boltzmann Machine (RBM), an important tool used in machine
learning in particular for unsupervized learning tasks, is investigated from
the perspective of its spectral properties. Starting from empirical
observations, we propose a generic statistical ensemble for the weight matrix
of the RBM and characterize its mean evolution. This let us show how in the
linear regime, in which the RBM is found to operate at the beginning of the
training, the statistical properties of the data drive the selection of the
unstable modes of the weight matrix. A set of equations characterizing the
non-linear regime is then derived, unveiling in some way how the selected modes
interact in later stages of the learning procedure and defining a deterministic
learning curve for the RBM.">"Spectral Dynamics of Learning Restricted Boltzmann Machines"</a>,
Aurélien Decelle, Giancarlo Fissore, Cyril Furtlehner,
arXiv: <a href="http://arxiv.org/abs/1708.02917v2">1708.02917</a>,
8/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1707.09723v1" title="Motivated by the recent successful application of artificial neural networks
to quantum many-body problems [G. Carleo and M. Troyer, Science {\bf 355}, 602
(2017)], a method to calculate the ground state of the Bose-Hubbard model using
a feedforward neural network is proposed. The results are in good agreement
with those obtained by exact diagonalization and the Gutzwiller approximation.
The method of neural-network quantum states is promising for solving quantum
many-body problems of ultracold atoms in optical lattices.">"Solving the Bose-Hubbard model with machine learning"</a>,
Hiroki Saito,
arXiv: <a href="http://arxiv.org/abs/1707.09723v1">1707.09723</a>,
7/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1707.06656v4" title="The efficient representation of quantum many-body states with classical
resources is a key challenge in quantum many-body theory. In this work we
analytically construct classical networks for the description of the quantum
dynamics in transverse-field Ising models that can be solved efficiently using
Monte Carlo techniques. Our perturbative construction encodes time-evolved
quantum states of spin-1/2 systems in a network of classical spins with local
couplings and can be directly generalized to other spin systems and higher
spins. Using this construction we compute the transient dynamics in one, two,
and three dimensions including local observables, entanglement production, and
Loschmidt amplitudes using Monte Carlo algorithms and demonstrate the accuracy
of this approach by comparisons to exact results. We include a mapping to
equivalent artificial neural networks, which were recently introduced to
provide a universal structure for classical network wave functions.">"Quantum dynamics in transverse-field Ising models from classical
  networks"</a>,
Markus Schmitt, Markus Heyl,
arXiv: <a href="http://arxiv.org/abs/1707.06656v4">1707.06656</a>,
7/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1707.03114v2" title="We construct a hidden variable model for the EPR correlations using a
Restricted Boltzmann Machine. The model reproduces the expected correlations
and thus violates the Bell inequality, as required by Bell's theorem. Unlike
most hidden-variable models, this model does not violate the \(locality\)
assumption in Bell's argument. Rather, it violates \(measurement\)
\(independence\), albeit in a decidedly non-conspiratorial way.">"Learning the Einstein-Podolsky-Rosen correlations on a Restricted
  Boltzmann Machine"</a>,
Steven Weinstein,
arXiv: <a href="http://arxiv.org/abs/1707.03114v2">1707.03114</a>,
7/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1707.00663v1" title="The application of state-of-the-art machine learning techniques to
statistical physic problems has seen a surge of interest for their ability to
discriminate phases of matter by extracting essential features in the many-body
wavefunction or the ensemble of correlators sampled in Monte Carlo simulations.
Here we introduce a gener- alization of supervised machine learning approaches
that allows to accurately map out phase diagrams of inter- acting many-body
systems without any prior knowledge, e.g. of their general topology or the
number of distinct phases. To substantiate the versatility of this approach,
which combines convolutional neural networks with quantum Monte Carlo sampling,
we map out the phase diagrams of interacting boson and fermion models both at
zero and finite temperatures and show that first-order, second-order, and
Kosterlitz-Thouless phase transitions can all be identified. We explicitly
demonstrate that our approach is capable of identifying the phase transition to
non-trivial many-body phases such as superfluids or topologically ordered
phases without supervision.">"Quantum phase recognition via unsupervised machine learning"</a>,
Peter Broecker, Fakher F. Assaad, Simon Trebst,
arXiv: <a href="http://arxiv.org/abs/1707.00663v1">1707.00663</a>,
7/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1706.09779v1" title="We train a deep convolutional neural network to accurately predict the
energies and magnetizations of Ising model configurations, using both the
traditional nearest-neighbour Hamiltonian, as well as a long-range screened
Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep
neural network in predicting the nearest-neighbour energy of the 4x4 Ising
model. Using its success at this task, we motivate the study of the larger 8x8
Ising model, showing that the deep neural network can learn the
nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small
fraction of configuration space. Additionally, we show that the neural network
has learned both the energy and magnetization operators with sufficient
accuracy to replicate the low-temperature Ising phase transition. Finally, we
teach the convolutional deep neural network to accurately predict a long-range
interaction through a screened Coulomb Hamiltonian. In this case, the benefits
of the neural network become apparent; it is able to make predictions with a
high degree of accuracy, 1600 times faster than a CUDA-optimized &quot;exact&quot;
calculation.">"Deep neural networks for direct, featureless learning through
  observation: the case of 2d spin models"</a>,
K. Mills, I. Tamblyn,
arXiv: <a href="http://arxiv.org/abs/1706.09779v1">1706.09779</a>,
6/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1706.08466v1" title="Inferring a generative model from data is a fundamental problem in machine
learning. It is well-known that the Ising model is the maximum entropy model
for binary variables which reproduces the sample mean and pairwise
correlations. Learning the parameters of the Ising model from data is the
challenge. We establish an analogy between the inverse Ising problem and the
Ornstein-Zernike formalism in liquid state physics. Rather than analytically
deriving the closure relation, we use a deep neural network to learn the
closure from simulations of the Ising model. We show, using simulations as well
as biochemical datasets, that the deep neural network model outperforms
systematic field-theoretic expansions and can generalize well beyond the
parameter regime of the training data. The neural network is able to learn from
synthetic data, which can be generated with relative ease, to give accurate
predictions on real world datasets.">"Inverse Ising inference by combining Ornstein-Zernike theory with deep
  learning"</a>,
Alpha A. Lee,
arXiv: <a href="http://arxiv.org/abs/1706.08466v1">1706.08466</a>,
6/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1706.07977v2" title="This work aims at the goal whether the artificial intelligence can recognize
phase transition without the prior human knowledge. If this becomes successful,
it can be applied to, for instance, analyze data from quantum simulation of
unsolved physical models. Toward this goal, we first need to apply the machine
learning algorithm to well-understood models and see whether the outputs are
consistent with our prior knowledge, which serves as the benchmark of this
approach. In this work, we feed the compute with data generated by the
classical Monte Carlo simulation for the XY model in frustrated triangular and
union jack lattices, which has two order parameters and exhibits two phase
transitions. We show that the outputs of the principle component analysis agree
very well with our understanding of different orders in different phases, and
the temperature dependences of the major components detect the nature and the
locations of the phase transitions. Our work offers promise for using machine
learning techniques to study sophisticated statistical models, and our results
can be further improved by using principle component analysis with kernel
tricks and the neural network method.">"Unsupervised Learning of Frustrated Classical Spin Models I: Principle
  Component Analysis"</a>,
Ce Wang, Hui Zhai,
arXiv: <a href="http://arxiv.org/abs/1706.07977v2">1706.07977</a>,
6/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1706.08111v1" title="We introduce a fully automatic self-learning scheme for detecting phase
boundaries. This method extends the previously introduced confusion scheme for
learning phase transitions, by using a cooperative network that learns to
optimize the guess for the transition point. The networks together are capable
of finding transition points for fully unlabeled data. This improvement allows
us to efficiently study 1D and 2D parameter spaces, where for the latter we
utilize an active contour model -- the snake -- from computer vision as a
representation of the learned phase boundary. The snakes, equipped with neural
networks, can learn while they move in the parameter space and thereby detect
phase boundaries automatically.">"Self-Learning Phase Boundaries by Active Contours"</a>,
Ye-Hua Liu, Evert P. L. van Nieuwenburg,
arXiv: <a href="http://arxiv.org/abs/1706.08111v1">1706.08111</a>,
6/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1706.00868v3" title="How useful can machine learning be in a quantum laboratory? Here we raise the
question of the potential of intelligent machines in the context of scientific
research. A major motivation for the present work is the unknown reachability
of various entanglement classes in quantum experiments. We investigate this
question by using the projective simulation model, a physics-oriented approach
to artificial intelligence. In our approach, the projective simulation system
is challenged to design complex photonic quantum experiments that produce
high-dimensional entangled multiphoton states, which are of high interest in
modern quantum experiments. The artificial intelligence system learns to create
a variety of entangled states, and improves the efficiency of their
realization. In the process, the system autonomously (re)discovers experimental
techniques which are only now becoming standard in modern quantum optical
experiments - a trait which was not explicitly demanded from the system but
emerged through the process of learning. Such features highlight the
possibility that machines could have a significantly more creative role in
future research.">"Active learning machine learns to create new quantum experiments"</a>,
Alexey A. Melnikov, Hendrik Poulsen Nautrup, Mario Krenn, Vedran Dunjko, Markus Tiersch, Anton Zeilinger, Hans J. Briegel,
arXiv: <a href="http://arxiv.org/abs/1706.00868v3">1706.00868</a>,
6/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.07855v3" title="A fault-tolerant quantum computation requires an efficient means to detect
and correct errors that accumulate in encoded quantum information. In the
context of machine learning, neural networks are a promising new approach to
quantum error correction. Here we show that a recurrent neural network can be
trained, using only experimentally accessible data, to detect errors in a
widely used topological code, the surface code, with a performance above that
of the established minimum-weight perfect matching (or blossom) decoder. The
performance gain is achieved because the neural network decoder can detect
correlations between bit-flip (X) and phase-flip (Z) errors. The machine
learning algorithm adapts to the physical system, hence no noise model is
needed. The long short-term memory layers of the recurrent neural network
maintain their performance over a large number of quantum error correction
cycles, making it a practical decoder for forthcoming experimental realizations
of the surface code.">"Machine-learning-assisted correction of correlated qubit errors in a
  topological code"</a>,
P. Baireuther, T. E. O'Brien, B. Tarasinski, C. W. J. Beenakker,
arXiv: <a href="http://arxiv.org/abs/1705.07855v3">1705.07855</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.06724v2" title="The recently-introduced self-learning Monte Carlo method is a general-purpose
numerical method that speeds up Monte Carlo simulations by training an
effective model to propose uncorrelated configurations in the Markov chain. We
implement this method in the framework of continuous time Monte Carlo method
with auxiliary field in quantum impurity models. We introduce and train a
diagram generating function (DGF) to model the probability distribution of
auxiliary field configurations in continuous imaginary time, at all orders of
diagrammatic expansion. By using DGF to propose global moves in configuration
space, we show that the self-learning continuous-time Monte Carlo method can
significantly reduce the computational complexity of the simulation.">"Self-Learning Monte Carlo Method: Continuous-Time Algorithm"</a>,
Yuki Nagai, Huitao Shen, Yang Qi, Junwei Liu, Liang Fu,
arXiv: <a href="http://arxiv.org/abs/1705.06724v2">1705.06724</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.11023v1" title="Guided by critical systems found in nature we develop a novel mechanism
consisting of inhomogeneous polynomial regularisation via which we can induce
scale invariance in deep learning systems. Technically, we map our deep
learning (DL) setup to a genuine field theory, on which we act with the
Renormalisation Group (RG) in momentum space and produce the flow equations of
the couplings; those are translated to constraints and consequently interpreted
as &quot;critical regularisation&quot; conditions in the optimiser; the resulting
equations hence prove to be sufficient conditions for - and serve as an elegant
and simple mechanism to induce scale invariance in any deep learning setup.">"Criticality &amp; Deep Learning II: Momentum Renormalisation Group"</a>,
Dan Oprisa, Peter Toth,
arXiv: <a href="http://arxiv.org/abs/1705.11023v1">1705.11023</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.05372v4" title="Correlated many-body problems ubiquitously appear in various fields of
physics such as condensed matter physics, nuclear physics, and statistical
physics. However, due to the interplay of the large number of degrees of
freedom, it is generically impossible to treat these problems from first
principles. Thus the construction of a proper model, namely effective
Hamiltonian, is essential. Here, we propose a simple scheme of constructing
Hamiltonians from given energy or entanglement spectra with machine learning.
We apply the proposed scheme to the Hubbard model at the half-filling, and
compare the obtained effective low-energy spin-1/2 model with several analytic
results based on the high order perturbation theory which have been
inconsistent with each other. We also show that our approach can be used to
construct the entanglement Hamiltonian of a quantum many-body state from its
entanglement spectrum as well. We exemplify this using the ground states of the
\(S=1/2\) two-leg Heisenberg ladders. We observe a qualitative difference between
the entanglement Hamiltonians of the two phases (the Haldane phase and the Rung
Singlet phase) of the model due to the different origin of the entanglement. In
the Haldane phase, we find that the entanglement Hamiltonian is non-local by
nature, and the locality can be restored by introducing the anisotropy and
turning the system into the large-\(D\) phase. Possible applications to the study
of strongly-correlated systems and the model construction from experimental
data are discussed.">"Construction of Hamiltonians by supervised learning of energy and
  entanglement spectra"</a>,
Hiroyuki Fujita, Yuya O. Nakagawa, Sho Sugiura, Masaki Oshikawa,
arXiv: <a href="http://arxiv.org/abs/1705.05372v4">1705.05372</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.05582v1" title="We present a procedure for reconstructing the decision function of an
artificial neural network as a simple function of the input, provided the
decision function is sufficiently symmetric. In this case one can easily deduce
the quantity by which the neural network classifies the input. The procedure is
embedded into a pipeline of machine learning algorithms able to detect the
existence of different phases of matter, to determine the position of phase
transitions and to find explicit expressions of the physical quantities by
which the algorithm distinguishes between phases. We assume no prior knowledge
about the Hamiltonian or the order parameters except Monte Carlo-sampled
configurations. The method is applied to the Ising Model and SU(2) lattice
gauge theory. In both systems we deduce the explicit expressions of the known
order parameters from the decision functions of the neural networks.">"Machine Learning of Explicit Order Parameters: From the Ising Model to
  SU(2) Lattice Gauge Theory"</a>,
Sebastian Johann Wetzel, Manuel Scherzer,
arXiv: <a href="http://arxiv.org/abs/1705.05582v1">1705.05582</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.01947v2" title="After decades of progress and effort, obtaining a phase diagram for a
strongly-correlated topological system still remains a challenge. Although in
principle one could turn to Wilson loops and long-range entanglement,
evaluating these non-local observables at many points in phase space can be
prohibitively costly. With growing excitement over topological quantum
computation comes the need for an efficient approach for obtaining topological
phase diagrams. Here we turn to machine learning using quantum loop topography
(QLT), a notion we have recently introduced. Specifically, we propose a
construction of QLT that is sensitive to quasi-particle statistics. We then use
mutual statistics between the spinons and visons to detect a \(Z_2\)
quantum spin liquid in a multi-parameter phase space. We successfully obtain
the quantum phase boundary between the topological and trivial phases using a
simple feed-forward neural network. Furthermore, we demonstrate advantages of
our approach for the evaluation of phase diagrams relating to speed and
storage. Such statistics-based machine learning of topological phases opens new
efficient routes to studying topological phase diagrams in strongly correlated
systems.">"Machine Learning <span class="math">\(Z_2\)</span> Quantum Spin Liquids with
  Quasi-particle Statistics"</a>,
Yi Zhang, Roger G. Melko, Eun-Ah Kim,
arXiv: <a href="http://arxiv.org/abs/1705.01947v2">1705.01947</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.00857v1" title="Surface codes reach high error thresholds when decoded with known algorithms,
but the decoding time will likely exceed the available time budget, especially
for near-term implementations. To decrease the decoding time, we reduce the
decoding problem to a classification problem that a feedforward neural network
can solve. We investigate quantum error correction and fault tolerance at small
code distances using neural network-based decoders, demonstrating that the
neural network can generalize to inputs that were not provided during training
and that they can reach similar or better decoding performance compared to
previous algorithms. We conclude by discussing the time required by a
feedforward neural network decoder in hardware.">"Decoding Small Surface Codes with Feedforward Neural Networks"</a>,
Savvas Varsamopoulos, Ben Criger, Koen Bertels,
arXiv: <a href="http://arxiv.org/abs/1705.00857v1">1705.00857</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.01523v3" title="The problem of determining whether a given quantum state is entangled lies at
the heart of quantum information processing, which is known to be an NP-hard
problem in general. Despite the proposed many methods such as the positive
partial transpose (PPT) criterion and the k-symmetric extendibility criterion
to tackle this problem in practice, none of them enables a general, effective
solution to the problem even for small dimensions. Explicitly, separable states
form a high-dimensional convex set, which exhibits a vastly complicated
structure. In this work, we build a new separability-entanglement classifier
underpinned by machine learning techniques. Our method outperforms the existing
methods in generic cases in terms of both speed and accuracy, opening up the
avenues to explore quantum entanglement via the machine learning approach.">"A Separability-Entanglement Classifier via Machine Learning"</a>,
Sirui Lu, Shilin Huang, Keren Li, Jun Li, Jianxin Chen, Dawei Lu, Zhengfeng Ji, Yi Shen, Duanlu Zhou, Bei Zeng,
arXiv: <a href="http://arxiv.org/abs/1705.01523v3">1705.01523</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1705.00565v2" title="The ability to prepare a physical system in a desired quantum state is
central to many areas of physics such as nuclear magnetic resonance, cold
atoms, and quantum computing. Preparing states quickly and with high fidelity
remains a formidable challenge. In this work we implement cutting-edge
Reinforcement Learning (RL) techniques and show that their performance rivals
gradient-based optimal control methods at the task of finding short,
high-fidelity driving protocols from an initial to a target state in
non-integrable many-body quantum systems of interacting qubits. The quantum
state preparation problem, viewed as an optimization problem, is shown to
exhibit a spin-glass-like phase transition in the space of protocols as a
function of the protocol duration. Our RL-aided approach helps identify
variational protocols with nearly optimal fidelity, even in the glassy phase,
where optimal state preparation appears exponentially hard. This study
highlights the usefulness of RL for applications in out-of-equilibrium quantum
physics.">"Reinforcement Learning in Different Phases of Quantum Control"</a>,
Marin Bukov, Alexandre G. R. Day, Dries Sels, Phillip Weinberg, Anatoli Polkovnikov, Pankaj Mehta,
arXiv: <a href="http://arxiv.org/abs/1705.00565v2">1705.00565</a>,
5/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.05822v1" title="Maximum likelihood estimation (MLE) is one of the most important methods in
machine learning, and the expectation-maximization (EM) algorithm is often used
to obtain maximum likelihood estimates. However, EM heavily depends on initial
configurations and fails to find the global optimum. On the other hand, in the
field of physics, quantum annealing (QA) was proposed as a novel optimization
approach. Motivated by QA, we propose a quantum annealing extension of EM,
which we call the deterministic quantum annealing expectation-maximization
(DQAEM) algorithm. We also discuss its advantage in terms of the path integral
formulation. Furthermore, by employing numerical simulations, we illustrate how
it works in MLE and show that DQAEM outperforms EM.">"Deterministic Quantum Annealing Expectation-Maximization Algorithm"</a>,
Hideyuki Miyahara, Koji Tsumura, Yuki Sughiyama,
arXiv: <a href="http://arxiv.org/abs/1704.05822v1">1704.05822</a>,
4/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.08724v1" title="In this work, we analyze the nonequilibrium thermodynamics of a class of
neural networks known as Restricted Boltzmann Machines (RBMs) in the context of
unsupervised learning. We show how the network is described as a discrete
Markov process and how the detailed balance condition and the Maxwell-Boltzmann
equilibrium distribution are sufficient conditions for a complete
thermodynamics description, including nonequilibrium fluctuation theorems.
Numerical simulations in a fully trained RBM are performed and the heat
exchange fluctuation theorem is verified with excellent agreement to the
theory. We observe how the contrastive divergence functional, mostly used in
unsupervised learning of RBMs, is closely related to nonequilibrium
thermodynamic quantities. We also use the framework to interpret the estimation
of the partition function of RBMs with the Annealed Importance Sampling method
from a thermodynamics standpoint. Finally, we argue that unsupervised learning
of RBMs is equivalent to a work protocol in a system driven by the laws of
thermodynamics in the absence of labeled data.">"Nonequilibrium Thermodynamics of Restricted Boltzmann Machines"</a>,
Domingos S. P. Salazar,
arXiv: <a href="http://arxiv.org/abs/1704.08724v1">1704.08724</a>,
4/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.06279v1" title="Physical systems differing in their microscopic details often display
strikingly similar behaviour when probed at low energies. Those universal
properties, largely determining their physical characteristics, are revealed by
the powerful renormalization group (RG) procedure, which systematically retains
&quot;slow&quot; degrees of freedom and integrates out the rest. However, the important
degrees of freedom may be difficult to identify. Here we demonstrate a machine
learning (ML) algorithm capable of identifying the relevant degrees of freedom
without any prior knowledge about the system. We introduce an artificial neural
network based on a model-independent, information-theoretic characterization of
a real-space RG procedure, performing this task. We apply the algorithm to
classical statistical physics problems in two dimensions.">"Mutual Information, Neural Networks and the Renormalization Group"</a>,
Maciej Koch-Janusz, Zohar Ringel,
arXiv: <a href="http://arxiv.org/abs/1704.06279v1">1704.06279</a>,
4/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.05848v1" title="Machine learning is capable of discriminating phases of matter, and finding
associated phase transitions, directly from large data sets of raw state
configurations. In the context of condensed matter physics, most progress in
the field of supervised learning has come from employing neural networks as
classifiers. Although very powerful, such algorithms suffer from a lack of
interpretability, which is usually desired in scientific applications in order
to associate learned features with physical phenomena. In this paper, we
explore support vector machines (SVMs) which are a class of supervised kernel
methods that provide interpretable decision functions. We find that SVMs can
learn the mathematical form of physical discriminators, such as order
parameters and Hamiltonian constraints, for a set of two-dimensional spin
models: the ferromagnetic Ising model, a conserved-order-parameter Ising model,
and the Ising gauge theory. The ability of SVMs to provide interpretable
classification highlights their potential for automating feature detection in
both synthetic and experimental data sets for condensed matter and other
many-body systems.">"Kernel methods for interpretable machine learning of order parameters"</a>,
Pedro Ponte, Roger G. Melko,
arXiv: <a href="http://arxiv.org/abs/1704.05848v1">1704.05848</a>,
4/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.05148v4" title="In this paper, we demonstrate the expressibility of artificial neural
networks (ANNs) in quantum many-body physics by showing that a feed-forward
neural network with a small number of hidden layers can be trained to
approximate with high precision the ground states of some notable quantum
many-body systems. We consider the one-dimensional free bosons and fermions,
spinless fermions on a square lattice away from half-filling, as well as
frustrated quantum magnetism with a rapidly oscillating ground-state
characteristic function. In the latter case, an ANN with a standard
architecture fails, while that with a slightly modified one successfully learns
the frustration-induced complex sign rule in the ground state and approximates
the ground states with high precisions. As an example of practical use of our
method, we also perform the variational method to explore the ground state of
an anti-ferromagnetic \(J_1-J_2\) Heisenberg model.">"Approximating quantum many-body wave-functions using artificial neural
  networks"</a>,
Zi Cai, Jinguo Liu,
arXiv: <a href="http://arxiv.org/abs/1704.05148v4">1704.05148</a>,
4/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.01578v2" title="We show that a simple artificial neural network trained on entanglement
spectra of individual states of a many-body quantum system can be used to
determine the transition between a many-body localized and a thermalizing
regime. Specifically, we study the Heisenberg spin-1/2 chain in a random
external field. We employ a multilayer perceptron with a single hidden layer,
which is trained on labeled entanglement spectra pertaining to the fully
localized and fully thermal regimes. We then apply this network to classify
spectra belonging to states in the transition region. For training, we use a
cost function that contains, in addition to the usual error and regularization
parts, a term that favors a confident classification of the transition region
states. The resulting phase diagram is in good agreement with the one obtained
by more conventional methods and can be computed for small systems. In
particular, the neural network outperforms conventional methods in classifying
individual eigenstates pertaining to a single disorder realization. It allows
us to map out the structure of these eigenstates across the transition with
spatial resolution. Furthermore, we analyze the network operation using the
dreaming technique to show that the neural network correctly learns by itself
the power-law structure of the entanglement spectra in the many-body localized
regime.">"Probing many-body localization with neural networks"</a>,
Frank Schindler, Nicolas Regnault, Titus Neupert,
arXiv: <a href="http://arxiv.org/abs/1704.01578v2">1704.01578</a>,
4/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.00080v2" title="We apply unsupervised machine learning techniques, mainly principal component
analysis (PCA), to compare and contrast the phase behavior and phase
transitions in several classical spin models - the square and
triangular-lattice Ising models, the Blume-Capel model, a highly degenerate
biquadratic-exchange spin-one Ising (BSI) model, and the 2D XY model, and
examine critically what machine learning is teaching us. We find that
quantified principal components from PCA not only allow exploration of
different phases and symmetry-breaking, but can distinguish phase transition
types and locate critical points. We show that the corresponding weight vectors
have a clear physical interpretation, which is particularly interesting in the
frustrated models such as the triangular antiferromagnet, where they can point
to incipient orders. Unlike the other well-studied models, the properties of
the BSI model are less well known. Using both PCA and conventional Monte Carlo
analysis, we demonstrate that the BSI model shows an absence of phase
transition and macroscopic ground-state degeneracy. The failure to capture the
charge' correlations (vorticity) in the BSI model (XY model) from raw spin
configurations points to some of the limitations of PCA. Finally, we employ a
nonlinear unsupervised machine learning procedure, theantoencoder method',
and demonstrate that it too can be trained to capture phase transitions and
critical points.">"Discovering Phases, Phase Transitions and Crossovers through
  Unsupervised Machine Learning: A critical examination"</a>,
Wenjian Hu, Rajiv R. P. Singh, Richard T. Scalettar,
arXiv: <a href="http://arxiv.org/abs/1704.00080v2">1704.00080</a>,
3/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1703.05334v2" title="The experimental realization of increasingly complex synthetic quantum
systems calls for the development of general theoretical methods, to validate
and fully exploit quantum resources. Quantum-state tomography (QST) aims at
reconstructing the full quantum state from simple measurements, and therefore
provides a key tool to obtain reliable analytics. Brute-force approaches to
QST, however, demand resources growing exponentially with the number of
constituents, making it unfeasible except for small systems. Here we show that
machine learning techniques can be efficiently used for QST of highly-entangled
states, in both one and two dimensions. Remarkably, the resulting approach
allows one to reconstruct traditionally challenging many-body quantities - such
as the entanglement entropy - from simple, experimentally accessible
measurements. This approach can benefit existing and future generations of
devices ranging from quantum computers to ultra-cold atom quantum simulators.">"Many-body quantum state tomography with neural networks"</a>,
Giacomo Torlai, Guglielmo Mazzola, Juan Carrasquilla, Matthias Troyer, Roger Melko, Giuseppe Carleo,
arXiv: <a href="http://arxiv.org/abs/1703.05334v2">1703.05334</a>,
3/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1703.02435v2" title="We employ unsupervised machine learning techniques to learn latent parameters
which best describe states of the two-dimensional Ising model and the
three-dimensional XY model. These methods range from principal component
analysis to artificial neural network based variational autoencoders. The
states are sampled using a Monte-Carlo simulation above and below the critical
temperature. We find that the predicted latent parameters correspond to the
known order parameters. The latent representation of the states of the models
in question are clustered, which makes it possible to identify phases without
prior knowledge of their existence or the underlying Hamiltonian. Furthermore,
we find that the reconstruction loss function can be used as a universal
identifier for phase transitions.">"Unsupervised learning of phase transitions: from principal component
  analysis to variational autoencoders"</a>,
Sebastian Johann Wetzel,
arXiv: <a href="http://arxiv.org/abs/1703.02435v2">1703.02435</a>,
3/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1701.06246v1" title="We study the representational power of a Boltzmann machine (a type of neural
network) in quantum many-body systems. We prove that any (local) tensor network
state has a (local) neural network representation. The construction is almost
optimal in the sense that the number of parameters in the neural network
representation is almost linear in the number of nonzero parameters in the
tensor network representation. Despite the difficulty of representing (gapped)
chiral topological states with local tensor networks, we construct a
quasi-local neural network representation for a chiral p-wave superconductor.
This demonstrates the power of Boltzmann machines.">"Neural network representation of tensor network and chiral states"</a>,
Yichen Huang, Joel E. Moore,
arXiv: <a href="http://arxiv.org/abs/1701.06246v1">1701.06246</a>,
1/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1701.04831v2" title="The restricted Boltzmann machine (RBM) is one of the fundamental building
blocks of deep learning. RBM finds wide applications in dimensional reduction,
feature extraction, and recommender systems via modeling the probability
distributions of a variety of input data including natural images, speech
signals, and customer ratings, etc. We build a bridge between RBM and tensor
network states (TNS) widely used in quantum many-body physics research. We
devise efficient algorithms to translate an RBM into the commonly used TNS.
Conversely, we give sufficient and necessary conditions to determine whether a
TNS can be transformed into an RBM of given architectures. Revealing these
general and constructive connections can cross-fertilize both deep learning and
quantum many-body physics. Notably, by exploiting the entanglement entropy
bound of TNS, we can rigorously quantify the expressive power of RBM on complex
data sets. Insights into TNS and its entanglement capacity can guide the design
of more powerful deep learning architectures. On the other hand, RBM can
represent quantum many-body states with fewer parameters compared to TNS, which
may allow more efficient classical simulations.">"Equivalence of restricted Boltzmann machines and tensor network states"</a>,
Jing Chen, Song Cheng, Haidong Xie, Lei Wang, Tao Xiang,
arXiv: <a href="http://arxiv.org/abs/1701.04831v2">1701.04831</a>,
1/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1701.05039v1" title="The challenge of quantum many-body problems comes from the difficulty to
represent large-scale quantum states, which in general requires an
exponentially large number of parameters. Recently, a connection has been made
between quantum many-body states and the neural network representation
(\textit{arXiv:1606.02318}). An important open question is what characterizes
the representational power of deep and shallow neural networks, which is of
fundamental interest due to popularity of the deep learning methods. Here, we
give a rigorous proof that a deep neural network can efficiently represent most
physical states, including those generated by any polynomial size quantum
circuits or ground states of many body Hamiltonians with polynomial-size gaps,
while a shallow network through a restricted Boltzmann machine cannot
efficiently represent those states unless the polynomial hierarchy in
computational complexity theory collapses.">"Efficient Representation of Quantum Many-body States with Deep Neural
  Networks"</a>,
Xun Gao, Lu-Ming Duan,
arXiv: <a href="http://arxiv.org/abs/1701.05039v1">1701.05039</a>,
1/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1701.04844v3" title="Machine learning, one of today's most rapidly growing interdisciplinary
fields, promises an unprecedented perspective for solving intricate quantum
many-body problems. Understanding the physical aspects of the representative
artificial neural-network states is recently becoming highly desirable in the
applications of machine learning techniques to quantum many-body physics. Here,
we study the quantum entanglement properties of neural-network states, with a
focus on the restricted-Boltzmann-machine (RBM) architecture. We prove that the
entanglement of all short-range RBM states satisfies an area law for arbitrary
dimensions and bipartition geometry. For long-range RBM states we show by using
an exact construction that such states could exhibit volume-law entanglement,
implying a notable capability of RBM in representing efficiently quantum states
with massive entanglement. We further examine generic RBM states with random
weight parameters. We find that their averaged entanglement entropy obeys
volume-law scaling and meantime strongly deviates from the Page-entropy of the
completely random pure states. We show that their entanglement spectrum has no
universal part associated with random matrix theory and bears a Poisson-type
level statistics. Using reinforcement learning, we demonstrate that RBM is
capable of finding the ground state (with power-law entanglement) of a model
Hamiltonian with long-range interaction. In addition, we show, through a
concrete example of the one-dimensional symmetry-protected topological cluster
states, that the RBM representation may also be used as a tool to analytically
compute the entanglement spectrum. Our results uncover the unparalleled power
of artificial neural networks in representing quantum many-body states, which
paves a novel way to bridge computer science based machine learning techniques
to outstanding quantum condensed matter physics problems.">"Quantum Entanglement in Neural Network States"</a>,
Dong-Ling Deng, Xiaopeng Li, S. Das Sarma,
arXiv: <a href="http://arxiv.org/abs/1701.04844v3">1701.04844</a>,
1/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1701.00246v1" title="We set up Restricted Boltzmann Machines (RBM) to reproduce the Long Range
Ising (LRI) models of the Ohmic type in one dimension. The RBM parameters are
tuned by using the standard machine learning procedure with an additional
method of Configuration with Probability (CwP). The quality of resultant RBM
are evaluated through the susceptibility with respect to the magnetic external
field. We compare the results with those by Block Decimation Renormalization
Group (BDRG) method, and our RBM clear the test with satisfactory precision.">"Restricted Boltzmann Machines for the Long Range Ising Models"</a>,
Ken-Ichi Aoki, Tamao Kobayashi,
arXiv: <a href="http://arxiv.org/abs/1701.00246v1">1701.00246</a>,
1/2017</p>
</li>
</ul>
<h2>Physics-Inspired Ideas Applied to Machine Learning</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1801.00315v1" title="Inspired by coarse-graining approaches used in physics, we show how similar
algorithms can be adapted for data. The resulting algorithms are based on
layered tree tensor networks and scale linearly with both the dimension of the
input and the training set size. Computing most of the layers with an
unsupervised algorithm, then optimizing just the top layer for supervised
classification of the MNIST and fashion-MNIST data sets gives very good
results. We also discuss mixing a prior guess for supervised weights together
with an unsupervised representation of the data, yielding a smaller number of
features nevertheless able to give good performance.">"Learning Relevant Features of Data with Multi-scale Tensor Networks"</a>,
E. M. Stoudenmire,
arXiv: <a href="http://arxiv.org/abs/1801.00315v1">1801.00315</a>,
12/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1712.04144v1" title="We compare and contrast the statistical physics and quantum physics inspired
approaches for unsupervised generative modeling of classical data. The two
approaches represent probabilities of observed data using energy-based models
and quantum states respectively.Classical and quantum information patterns of
the target datasets therefore provide principled guidelines for structural
design and learning in these two approaches. Taking the restricted Boltzmann
machines (RBM) as an example, we analyze the information theoretical bounds of
the two approaches. We verify our reasonings by comparing the performance of
RBMs of various architectures on the standard MNIST datasets.">"Information Perspective to Probabilistic Modeling: Boltzmann Machines
  versus Born Machines"</a>,
Song Cheng, Jing Chen, Lei Wang,
arXiv: <a href="http://arxiv.org/abs/1712.04144v1">1712.04144</a>,
12/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.11029v2" title="Stochastic gradient descent (SGD) is widely believed to perform implicit
regularization when used to train deep neural networks, but the precise manner
in which this occurs has thus far been elusive. We prove that SGD minimizes an
average potential over the posterior distribution of weights along with an
entropic regularization term. This potential is however not the original loss
function in general. So SGD does perform variational inference, but for a
different loss than the one used to compute the gradients. Even more
surprisingly, SGD does not even converge in the classical sense: we show that
the most likely trajectories of SGD for deep networks do not behave like
Brownian motion around critical points. Instead, they resemble closed loops
with deterministic components. We prove that such &quot;out-of-equilibrium&quot; behavior
is a consequence of highly non-isotropic gradient noise in SGD; the covariance
matrix of mini-batch gradients for deep networks has a rank as small as 1% of
its dimension. We provide extensive empirical validation of these claims,
proven in the appendix.">"Stochastic gradient descent performs variational inference, converges to
  limit cycles for deep networks"</a>,
Pratik Chaudhari, Stefano Soatto,
arXiv: <a href="http://arxiv.org/abs/1710.11029v2">1710.11029</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.06570v1" title="A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.">"A Correspondence Between Random Neural Networks and Statistical Field
  Theory"</a>,
Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein,
arXiv: <a href="http://arxiv.org/abs/1710.06570v1">1710.06570</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.05520v1" title="The success of deep convolutional neural network (CNN) in computer vision
especially image classification problems requests a new information theory for
function of image, instead of image itself. In this article, after establishing
a deep mathematical connection between image classification problem and quantum
spin model, we propose to use entanglement entropy, a generalization of
classical Boltzmann-Shannon entropy, as a powerful tool to characterize the
information needed for representation of general function of image. We prove
that there is a sub-volume-law bound for entanglement entropy of target
functions of reasonable image classification problems. Therefore target
functions of image classification only occupy a small subspace of the whole
Hilbert space. As a result, a neural network with polynomial number of
parameters is efficient for representation of such target functions of image.
The concept of entanglement entropy can also be useful to characterize the
expressive power of different neural networks. For example, we show that to
maintain the same expressive power, number of channels \(D\) in a convolutional
neural network should scale with the number of convolution layers \(n_c\) as
\(D\sim D_0^{\frac{1}{n_c}}\). Therefore, deeper CNN with large \(n_c\) is more
efficient than shallow ones.">"Entanglement Entropy of Target Functions for Image Classification and
  Convolutional Neural Network"</a>,
Ya-Hui Zhang,
arXiv: <a href="http://arxiv.org/abs/1710.05520v1">1710.05520</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.04833v1" title="The resemblance between the methods used in studying quantum-many body
physics and in machine learning has drawn considerable attention. In
particular, tensor networks (TNs) and deep learning architectures bear striking
similarities to the extent that TNs can be used for machine learning. Previous
results used one-dimensional TNs in image recognition, showing limited
scalability and a high bond dimension. In this work, we train two-dimensional
hierarchical TNs to solve image recognition problems, using a training
algorithm derived from the multipartite entanglement renormalization ansatz
(MERA). This approach overcomes scalability issues and implies novel
mathematical connections among quantum many-body physics, quantum information
theory, and machine learning. While keeping the TN unitary in the training
phase, TN states can be defined, which optimally encodes each class of the
images into a quantum many-body state. We study the quantum features of the TN
states, including quantum entanglement and fidelity. We suggest these
quantities could be novel properties that characterize the image classes, as
well as the machine learning tasks. Our work could be further applied to
identifying possible quantum properties of certain artificial intelligence
methods.">"Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A
  Quantum Information Theoretic Perspective on Deep Architectures"</a>,
Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng, Raul Blázquez García, Gang Su, Maciej Lewenstein,
arXiv: <a href="http://arxiv.org/abs/1710.04833v1">1710.04833</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.04045v2" title="Neural Networks Quantum States have been recently introduced as an Ansatz for
describing the wave function of quantum many-body systems. We show that there
are strong connections between Neural Networks Quantum States in the form of
Restricted Boltzmann Machines and some classes of Tensor Network states in
arbitrary dimension. In particular we demonstrate that short-range Restricted
Boltzmann Machines are Entangled Plaquette States, while fully connected
Restricted Boltzmann Machines are String-Bond States with a non-local geometry
and low bond dimension. These results shed light on the underlying architecture
of Restricted Boltzmann Machines and their efficiency at representing many-body
quantum states. String-Bond States also provide a generic way of enhancing the
power of Neural Networks Quantum States and a natural generalization to systems
with larger local Hilbert space. We compare the advantages and drawbacks of
these different classes of states and present a method to combine them
together. This allows us to benefit from both the entanglement structure of
Tensor Networks and the efficiency of Neural Network Quantum States into a
single Ansatz capable of targeting the wave function of strongly correlated
systems. While it remains a challenge to describe states with chiral
topological order using traditional Tensor Networks, we show that Neural
Networks Quantum States and their String-Bond States extension can describe a
lattice Fractional Quantum Hall state exactly. In addition, we provide
numerical evidence that Neural Networks Quantum States can approximate a chiral
spin liquid with better accuracy than Entangled Plaquette States and local
String-Bond States. Our results demonstrate the efficiency of neural networks
to describe complex quantum wave functions and pave the way towards the use of
String-Bond States as a tool in more traditional machine learning applications.">"Neural Networks Quantum States, String-Bond States and chiral
  topological states"</a>,
Ivan Glasser, Nicola Pancotti, Moritz August, Ivan D. Rodriguez, J. Ignacio Cirac,
arXiv: <a href="http://arxiv.org/abs/1710.04045v2">1710.04045</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.01467v2" title="Deep neural networks as powerful tools are widely used in various domains.
However, the nature of computations at each layer of the deep networks is far
from being well understood. Increasing the interpretability of deep neural
networks is thus important. Here, we construct a mean-field framework to
understand how compact representations are developed across layers, not only in
deterministic random deep networks but also in generative deep networks where
network parameters are learned from input data. Our theory shows that the deep
computation implements a dimensionality reduction while maintaining a finite
level of weak correlations between neurons for possible feature extraction.
This work may pave the way for understanding how a sensory hierarchy works in
general.">"Mean-field theory of input dimensionality reduction in unsupervised deep
  neural networks"</a>,
Haiping Huang,
arXiv: <a href="http://arxiv.org/abs/1710.01467v2">1710.01467</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.04833v1" title="The resemblance between the methods used in studying quantum-many body
physics and in machine learning has drawn considerable attention. In
particular, tensor networks (TNs) and deep learning architectures bear striking
similarities to the extent that TNs can be used for machine learning. Previous
results used one-dimensional TNs in image recognition, showing limited
scalability and a high bond dimension. In this work, we train two-dimensional
hierarchical TNs to solve image recognition problems, using a training
algorithm derived from the multipartite entanglement renormalization ansatz
(MERA). This approach overcomes scalability issues and implies novel
mathematical connections among quantum many-body physics, quantum information
theory, and machine learning. While keeping the TN unitary in the training
phase, TN states can be defined, which optimally encodes each class of the
images into a quantum many-body state. We study the quantum features of the TN
states, including quantum entanglement and fidelity. We suggest these
quantities could be novel properties that characterize the image classes, as
well as the machine learning tasks. Our work could be further applied to
identifying possible quantum properties of certain artificial intelligence
methods.">"Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A
  Quantum Information Theoretic Perspective on Deep Architectures"</a>,
Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng, Raul Blázquez García, Gang Su, Maciej Lewenstein,
arXiv: <a href="http://arxiv.org/abs/1710.04833v1">1710.04833</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.01662v2" title="Generative modeling, which learns joint probability distribution from
training data and generates samples according to it, is an important task in
machine learning and artificial intelligence. Inspired by probabilistic
interpretation of quantum physics, we propose a generative model using matrix
product states, which is a tensor network originally proposed for describing
(particularly one-dimensional) entangled quantum states. Our model enjoys
efficient learning by utilizing the density matrix renormalization group method
which allows dynamic adjusting dimensions of the tensors, and offers an
efficient direct sampling approach, Zipper, for generative tasks. We apply our
method to generative modeling of several standard datasets including the
principled Bars and Stripes, random binary patterns and the MNIST handwritten
digits, to illustrate ability of our model, and discuss features as well as
drawbacks of our model over popular generative models such as Hopfield model,
Boltzmann machines and generative adversarial networks. Our work shed light on
many interesting directions for future exploration on the development of
quantum-inspired algorithms for unsupervised machine learning, which is of
possibility of being realized by a quantum device.">"Unsupervised Generative Modeling Using Matrix Product States"</a>,
Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang, Pan Zhang,
arXiv: <a href="http://arxiv.org/abs/1709.01662v2">1709.01662</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1704.01552v2" title="Deep convolutional networks have witnessed unprecedented success in various
machine learning applications. Formal understanding on what makes these
networks so successful is gradually unfolding, but for the most part there are
still significant mysteries to unravel. The inductive bias, which reflects
prior knowledge embedded in the network architecture, is one of them. In this
work, we establish a fundamental connection between the fields of quantum
physics and deep learning. We use this connection for asserting novel
theoretical observations regarding the role that the number of channels in each
layer of the convolutional network fulfills in the overall inductive bias.
Specifically, we show an equivalence between the function realized by a deep
convolutional arithmetic circuit (ConvAC) and a quantum many-body wave
function, which relies on their common underlying tensorial structure. This
facilitates the use of quantum entanglement measures as well-defined
quantifiers of a deep network's expressive ability to model intricate
correlation structures of its inputs. Most importantly, the construction of a
deep ConvAC in terms of a Tensor Network is made available. This description
enables us to carry a graph-theoretic analysis of a convolutional network, with
which we demonstrate a direct control over the inductive bias of the deep
network via its channel numbers, that are related to the min-cut in the
underlying graph. This result is relevant to any practitioner designing a
network for a specific task. We theoretically analyze ConvACs, and empirically
validate our findings on more common ConvNets which involve ReLU activations
and max pooling. Beyond the results described above, the description of a deep
convolutional network in well-defined graph-theoretic tools and the formal
connection to quantum entanglement, are two interdisciplinary bridges that are
brought forth by this work.">"Deep Learning and Quantum Entanglement: Fundamental Connections with
  Implications to Network Design"</a>,
Yoav Levine, David Yakira, Nadav Cohen, Amnon Shashua,
arXiv: <a href="http://arxiv.org/abs/1704.01552v2">1704.01552</a>,
4/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1703.00810v3" title="Despite their great success, there is still no comprehensive theoretical
understanding of learning with Deep Neural Networks (DNNs) or their inner
organization. Previous work proposed to analyze DNNs in the \textit{Information
Plane}; i.e., the plane of the Mutual Information values that each layer
preserves on the input and output variables. They suggested that the goal of
the network is to optimize the Information Bottleneck (IB) tradeoff between
compression and prediction, successively, for each layer.
  In this work we follow up on this idea and demonstrate the effectiveness of
the Information-Plane visualization of DNNs. Our main results are: (i) most of
the training epochs in standard DL are spent on {\emph compression} of the
input to efficient representation and not on fitting the training labels. (ii)
The representation compression phase begins when the training errors becomes
small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift
to smaller training error into a stochastic relaxation, or random diffusion,
constrained by the training error value. (iii) The converged layers lie on or
very close to the Information Bottleneck (IB) theoretical bound, and the maps
from the input to any hidden layer and from this hidden layer to the output
satisfy the IB self-consistent equations. This generalization through noise
mechanism is unique to Deep Neural Networks and absent in one layer networks.
(iv) The training time is dramatically reduced when adding more hidden layers.
Thus the main advantage of the hidden layers is computational. This can be
explained by the reduced relaxation time, as this it scales super-linearly
(exponentially for simple diffusion) with the information compression from the
previous layer.">"Opening the Black Box of Deep Neural Networks via Information"</a>,
Ravid Shwartz-Ziv, Naftali Tishby,
arXiv: <a href="http://arxiv.org/abs/1703.00810v3">1703.00810</a>,
3/2017</p>
</li>
</ul>
<h2>Quantum Computation and Quantum Algorithms for Machine Learning</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1712.04709v1" title="Variational Bayes (VB) inference is one of the most important algorithms in
machine learning and widely used in engineering and industry. However, VB is
known to suffer from the problem of local optima. In this Letter, we generalize
VB by using quantum mechanics, and propose a new algorithm, which we call
quantum annealing variational Bayes (QAVB) inference. We then show that QAVB
drastically improve the performance of VB by applying them to a clustering
problem described by a Gaussian mixture model. Finally, we discuss an intuitive
understanding on how QAVB works well.">"A Quantum Extension of Variational Bayes Inference"</a>,
Hideyuki Miyahara, Yuki Sughiyama,
arXiv: <a href="http://arxiv.org/abs/1712.04709v1">1712.04709</a>,
12/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1712.05304v1" title="The question has remained open if near-term gate model quantum computers will
offer a quantum advantage for practical applications in the pre-fault tolerance
noise regime. A class of algorithms which have shown some promise in this
regard are the so-called classical-quantum hybrid variational algorithms. Here
we develop a low-depth quantum algorithm to train quantum Boltzmann machine
neural networks using such variational methods. We introduce a method which
employs the quantum approximate optimization algorithm as a subroutine in order
to approximately sample from Gibbs states of Ising Hamiltonians. We use this
approximate Gibbs sampling to train neural networks for which we demonstrate
training convergence for numerically simulated noisy circuits with depolarizing
errors of rates of up to 4%.">"A quantum algorithm to train neural networks using low-depth circuits"</a>,
Guillaume Verdon, Michael Broughton, Jacob Biamonte,
arXiv: <a href="http://arxiv.org/abs/1712.05304v1">1712.05304</a>,
12/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1711.06652v1" title="Security for machine learning has begun to become a serious issue for present
day applications. An important question remaining is whether emerging quantum
technologies will help or hinder the security of machine learning. Here we
discuss a number of ways that quantum information can be used to help make
quantum classifiers more secure or private. In particular, we demonstrate a
form of robust principal component analysis that, under some circumstances, can
provide an exponential speedup relative to robust methods used at present. To
demonstrate this approach we introduce a linear combinations of unitaries
Hamiltonian simulation method that we show functions when given an imprecise
Hamiltonian oracle, which may be of independent interest. We also introduce a
new quantum approach for bagging and boosting that can use quantum
superposition over the classifiers or splits of the training set to aggregate
over many more models than would be possible classically. Finally, we provide a
private form of \(k\)--means clustering that can be used to prevent an all
powerful adversary from learning more than a small fraction of a bit from any
user. These examples show the role that quantum technologies can play in the
security of ML and vice versa. This illustrates that quantum computing can
provide useful advantages to machine learning apart from speedups.">"Hardening Quantum Machine Learning Against Adversaries"</a>,
Nathan Wiebe, Ram Shankar Siva Kumar,
arXiv: <a href="http://arxiv.org/abs/1711.06652v1">1711.06652</a>,
11/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1711.02038v1" title="A central task in the field of quantum computing is to find applications
where quantum computer could provide exponential speedup over any classical
computer. Machine learning represents an important field with broad
applications where quantum computer may offer significant speedup. Several
quantum algorithms for discriminative machine learning have been found based on
efficient solving of linear algebraic problems, with potential exponential
speedup in runtime under the assumption of effective input from a quantum
random access memory. In machine learning, generative models represent another
large class which is widely used for both supervised and unsupervised learning.
Here, we propose an efficient quantum algorithm for machine learning based on a
quantum generative model. We prove that our proposed model is exponentially
more powerful to represent probability distributions compared with classical
generative models and has exponential speedup in training and inference at
least for some instances under a reasonable assumption in computational
complexity theory. Our result opens a new direction for quantum machine
learning and offers a remarkable example in which a quantum algorithm shows
exponential improvement over any classical algorithm in an important
application field.">"An efficient quantum algorithm for generative machine learning"</a>,
Xun Gao, Zhengyu Zhang, Luming Duan,
arXiv: <a href="http://arxiv.org/abs/1711.02038v1">1711.02038</a>,
11/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1710.09016v1" title="Hidden Quantum Markov Models (HQMMs) can be thought of as quantum
probabilistic graphical models that can model sequential data. We extend
previous work on HQMMs with three contributions: (1) we show how classical
hidden Markov models (HMMs) can be simulated on a quantum circuit, (2) we
reformulate HQMMs by relaxing the constraints for modeling HMMs on quantum
circuits, and (3) we present a learning algorithm to estimate the parameters of
an HQMM from data. While our algorithm requires further optimization to handle
larger datasets, we are able to evaluate our algorithm using several synthetic
datasets. We show that on HQMM generated data, our algorithm learns HQMMs with
the same number of hidden states and predictive accuracy as the true HQMMs,
while HMMs learned with the Baum-Welch algorithm require more states to match
the predictive accuracy.">"Learning Hidden Quantum Markov Models"</a>,
Siddarth Srinivasan, Geoff Gordon, Byron Boots,
arXiv: <a href="http://arxiv.org/abs/1710.09016v1">1710.09016</a>,
10/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.08519v1" title="We study the quantum synchronization between a pair of two-level systems
inside two coupledcavities. Using a digital-analog decomposition of the master
equation that rules the system dynamics, we show that this approach leads to
quantum synchronization between both two-level systems. Moreover, we can
identify in this digital-analog block decomposition the fundamental elements of
a quantum machine learning protocol, in which the agent and the environment
(learning units) interact through a mediating system, namely, the register. If
we can additionally equip this algorithm with a classical feedback mechanism,
which consists of projective measurements in the register, reinitialization of
the register state and local conditional operations on the agent and register
subspace, a powerful and flexible quantum machine learning protocol emerges.
Indeed, numerical simulations show that this protocol enhances the
synchronization process, even when every subsystem experience different
loss/decoherence mechanisms, and give us flexibility to choose the
synchronization state. Finally, we propose an implementation based on current
technologies in superconducting circuits.">"Enhanced Quantum Synchronization via Quantum Machine Learning"</a>,
F. A. Cárdenas-López, M. Sanz, J. C. Retamal, E. Solano,
arXiv: <a href="http://arxiv.org/abs/1709.08519v1">1709.08519</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.07848v1" title="We propose a protocol to perform generalized quantum reinforcement learning
with quantum technologies. At variance with recent results on quantum
reinforcement learning with superconducting circuits [L. Lamata, Sci. Rep. 7,
1609 (2017)], in our current protocol coherent feedback during the learning
process is not required, enabling its implementation in a wide variety of
quantum systems. We consider diverse possible scenarios for an agent, an
environment, and a register that connects them, involving multiqubit and
multilevel systems, as well as open-system dynamics. We finally propose
possible implementations of this protocol in trapped ions and superconducting
circuits. The field of quantum reinforcement learning with quantum technologies
will enable enhanced quantum control, as well as more efficient machine
learning calculations.">"Generalized Quantum Reinforcement Learning with Quantum Technologies"</a>,
F. A. Cárdenas-López, L. Lamata, J. C. Retamal, E. Solano,
arXiv: <a href="http://arxiv.org/abs/1709.07848v1">1709.07848</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1709.07409v1" title="The quantum autoencoder is a recent paradigm in the field of quantum machine
learning, which may enable an enhanced use of resources in quantum
technologies. To this end, quantum neural networks with less nodes in the inner
than in the outer layers were considered. Here, we propose a useful connection
between approximate quantum adders and quantum autoencoders. Specifically, this
link allows us to employ optimized approximate quantum adders, obtained with
genetic algorithms, for the implementation of quantum autoencoders for a
variety of initial states. Furthermore, we can also directly optimize the
quantum autoencoders via genetic algorithms. Our approach opens a different
path for the design of quantum autoencoders in controllable quantum platforms.">"Quantum Autoencoders via Quantum Adders with Genetic Algorithms"</a>,
L. Lamata, U. Alvarez-Rodriguez, J. D. Martín-Guerrero, M. Sanz, E. Solano,
arXiv: <a href="http://arxiv.org/abs/1709.07409v1">1709.07409</a>,
9/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1707.08561v3" title="Recently, increased computational power and data availability, as well as
algorithmic advances, have led machine learning techniques to impressive
results in regression, classification, data-generation and reinforcement
learning tasks. Despite these successes, the proximity to the physical limits
of chip fabrication alongside the increasing size of datasets are motivating a
growing number of researchers to explore the possibility of harnessing the
power of quantum computation to speed-up classical machine learning algorithms.
Here we review the literature in quantum machine learning and discuss
perspectives for a mixed readership of classical machine learning and quantum
computation experts. Particular emphasis will be placed on clarifying the
limitations of quantum algorithms, how they compare with their best classical
counterparts and why quantum resources are expected to provide advantages for
learning problems. Learning in the presence of noise and certain
computationally hard problems in machine learning are identified as promising
directions for the field. Practical questions, like how to upload classical
data into quantum form, will also be addressed.">"Quantum machine learning: a classical perspective"</a>,
Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, Andrea Rocchetto, Simone Severini, Leonard Wossnig,
arXiv: <a href="http://arxiv.org/abs/1707.08561v3">1707.08561</a>,
7/2017</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1703.05402v1" title="Efficiently characterising quantum systems, verifying operations of quantum
devices and validating underpinning physical models, are central challenges for
the development of quantum technologies and for our continued understanding of
foundational physics. Machine-learning enhanced by quantum simulators has been
proposed as a route to improve the computational cost of performing these
studies. Here we interface two different quantum systems through a classical
channel - a silicon-photonics quantum simulator and an electron spin in a
diamond nitrogen-vacancy centre - and use the former to learn the latter's
Hamiltonian via Bayesian inference. We learn the salient Hamiltonian parameter
with an uncertainty of approximately \(10^{-5}\). Furthermore, an observed
saturation in the learning algorithm suggests deficiencies in the underlying
Hamiltonian model, which we exploit to further improve the model itself. We go
on to implement an interactive version of the protocol and experimentally show
its ability to characterise the operation of the quantum photonic device. This
work demonstrates powerful new quantum-enhanced techniques for investigating
foundational physical models and characterising quantum technologies.">"Experimental Quantum Hamiltonian Learning"</a>,
Jianwei Wang, Stefano Paesani, Raffaele Santagati, Sebastian Knauer, Antonio A. Gentile, Nathan Wiebe, Maurangelo Petruzzella, Jeremy L. O'Brien, John G. Rarity, Anthony Laing, Mark G. Thompson,
arXiv: <a href="http://arxiv.org/abs/1703.05402v1">1703.05402</a>,
3/2017</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></p>
        </section>
      </div>

  </section>

  <!-- Two: Including columns with links to most recent articles -->
  <!--
    <section class="wrapper style1 container special">
        <div class="row">
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/machine-learning-for-quantum-design-at-pi.html" rel='bookmark'><h3>Machine Learning for Quantum Design at PI</h3></a>
              </header>
              <p>The Perimeter Institute for Theoretical Physics in Waterloo, Canada, hosts "</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/machine-learning-for-quantum-design-at-pi.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" rel='bookmark'><h3>Workshop Machine Learning for Quantum Technology at MPL Erlangen</h3></a>
              </header>
              <p>The Max Planck Institute for the Science of Light in Erlangen, Germany, hosts <a href="https://www.mpl.mpg.de/divisions/marquardt-division/workshops/2019-machi</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/DL-theory.html" rel='bookmark'><h3>The Theory of Deep Learning - Part I</h3></a>
              </header>
              <p>Why do modern deep neural networks (DNNs) perform so well on previously unseen test data, even when their number of weights is much larger than the number of data points</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/DL-theory.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
        </div>
    </section>  -->
  </article>

<!-- Footer -->
<footer id="footer">

    <ul class="icons">
    </ul>

    <span class="copyright">&copy; physicsml. All rights reserved. <br> Powered by <a href="https://blog.getpelican.com">Pelican</a>. Theme Twenty, Design: <a href="http://html5up.net">HTML5 UP</a>. Implemented and maintained by Anna Go.</span>

</footer>
</body>
</html>
