<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.dropotron.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/skel-layers.min.js"></script>
    <script src="/js/init.js"></script>
    <link rel="stylesheet" href="/css/pygment.css" />
    <noscript>
        <link rel="stylesheet" href="/css/skel.css" />
        <link rel="stylesheet" href="/css/style.css" />
        <link rel="stylesheet" href="/css/style-noscript.css" />
    </noscript>
    <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
    <!-- <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" integrity="sha384-3AB7yXWz4OeoZcPbieVW64vVXEwADiYyAEhwilzWsLw+9FgqpyjjStpPnpBO8o8S" crossorigin="anonymous">
    <link  href="http://fonts.googleapis.com/css?family=Anonymous+Pro:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" >
    <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie/v8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie/v9.css" /><![endif]-->

    <title>Papers | &#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297; </title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width">
</head>

<body class=" loading">



    <!-- Header -->
    <header id="header" >
        <h1 class="logo">
            <a href="..">&#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297;</a>
        </h1>
        <nav id="nav">
            <ul>
                <!-- <li class="current"><a href="index.html">Welcome</a></li> -->
                    <li><a href="/category/news.html">News</a></li>
                    <li><a href="/category/articles.html">Blog</a></li>
                    <li class="submenu">
                            <a href="#">Papers</a>
                            <ul style="list-style: none;">
                                    <li><a href="/pages/papers-19.html">2019</a></li>
                                    <li><a href="/pages/papers-18.html">2018</a></li>
                                    <li><a href="/pages/papers-17.html">2017</a></li>
                                    <li><a href="/pages/papers-16.html">2016</a></li>
                                    <li><a href="/pages/papers-15.html">2015</a></li>
                            </ul>
                    </li>
                <!--
                <li class="submenu">
                    <a href="../">No Blog</a>
                    <ul>
                            <li >
                                <a href="../category/articles/">Articles</a>
                            </li>
                            <li >
                                <a href="../category/news/">News</a>
                            </li>
                    </ul>
                </li>
                -->
                <!--
                <li><a href="#" class="button special">Nothing to Sign Up to</a></li>
                -->
            </ul>
        </nav>
    </header>

<!-- Main -->
<article id="main">

  <header class="special container">
    <span class="icon fas fa-paperclip"></span>
    <h2>Papers</h2>
    <!-- add page sub title here -->
    <p>The following are recent papers combining the fields of physics - especially quantum mechanics - and machine learning. </br> Please email <a href="mailto:agolubeva@pitp.ca">Anna Go</a> if you would like to see a paper added to this page.</p>
  </header>

  <!-- One -->
  <section class="wrapper style4 container">

    <!-- Content -->
      <div class="content">
        <section>
          <!-- <a href="#" class="image feature"><img src="images/pic04.jpg" alt="" /></a> -->
          <p><h2>Applying Machine Learning to Physics</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1509.04298v2" title="We put forward a strategy to encode a quantum operation into the unmodulated
dynamics of a quantum network without the need of external control pulses,
measurements or active feedback. Our optimization scheme, inspired by
supervised machine learning, consists in engineering the pairwise couplings
between the network qubits so that the target quantum operation is encoded in
the natural reduced dynamics of a network section. The efficacy of the proposed
scheme is demonstrated by the finding of uncontrolled four-qubit networks that
implement either the Toffoli gate, the Fredkin gate, or remote logic
operations. The proposed Toffoli gate is stable against imperfections, has a
high-fidelity for fault tolerant quantum computation, and is fast, being based
on the non-equilibrium dynamics.">"Quantum gate learning in engineered qubit networks: Toffoli gate with
  always-on interactions"</a>,
Leonardo Banchi, Nicola Pancotti, Sougato Bose,
arXiv: <a href="http://arxiv.org/abs/1509.04298v2">1509.04298</a>,
9/2015</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1509.02749v2" title="Quantum mechanics predicts a number of at first sight counterintuitive
phenomena. It is therefore a question whether our intuition is the best way to
find new experiments. Here we report the development of the computer algorithm
Melvin which is able to find new experimental implementations for the creation
and manipulation of complex quantum states. And indeed, the discovered
experiments extensively use unfamiliar and asymmetric techniques which are
challenging to understand intuitively. The results range from the first
implementation of a high-dimensional Greenberger-Horne-Zeilinger (GHZ) state,
to a vast variety of experiments for asymmetrically entangled quantum states --
a feature that can only exist when both the number of involved parties and
dimensions is larger than 2. Additionally, new types of high-dimensional
transformations are found that perform cyclic operations. Melvin autonomously
learns from solutions for simpler systems, which significantly speeds up the
discovery rate of more complex experiments. The ability to automate the design
of a quantum experiment can be applied to many quantum systems and allows the
physical realization of quantum states previously thought of only on paper.">"Automated Search for new Quantum Experiments"</a>,
Mario Krenn, Mehul Malik, Robert Fickler, Radek Lapkiewicz, Anton Zeilinger,
arXiv: <a href="http://arxiv.org/abs/1509.02749v2">1509.02749</a>,
9/2015</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1404.1333v2" title="Kernel ridge regression is used to approximate the kinetic energy of
non-interacting fermions in a one-dimensional box as a functional of their
density. The properties of different kernels and methods of cross-validation
are explored, and highly accurate energies are achieved. Accurate {\em
constrained optimal densities} are found via a modified Euler-Lagrange
constrained minimization of the total energy. A projected gradient descent
algorithm is derived using local principal component analysis. Additionally, a
sparse grid representation of the density can be used without degrading the
performance of the methods. The implications for machine-learned density
functional approximations are discussed.">"Understanding Machine-learned Density Functionals"</a>,
Li Li, John C. Snyder, Isabelle M. Pelaschier, Jessica Huang, Uma-Naresh Niranjan, Paul Duncan, Matthias Rupp, Klaus-Robert MÃ¼ller, Kieron Burke,
arXiv: <a href="http://arxiv.org/abs/1404.1333v2">1404.1333</a>,
4/2014</p>
</li>
<li>
<p><a href="http://stacks.iop.org/0295-5075/50/i=1/a=094" title="It is shown that magnetic bubble films behaviour can be described by using a 2D super-Ising Hamiltonian. Calculated hysteresis curves and magnetic domain patterns are successfully compared with experimental results taken in the literature. The reciprocal problem of finding parameters of the super-Ising model to reproduce computed or experimental magnetic domain pictures is solved by using a perceptron neural network.">"Analysis of magnetic domain patterns by a perceptron neural network"</a>,
S. Courtin and S. Padovani,
4/2000</p>
</li>
</ul>
<h2>Physics-Inspired Ideas Applied to Machine Learning</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1410.3831v1" title="Deep learning is a broad set of techniques that uses multiple layers of
representation to automatically learn relevant features directly from
structured data. Recently, such techniques have yielded record-breaking results
on a diverse set of difficult machine learning tasks in computer vision, speech
recognition, and natural language processing. Despite the enormous success of
deep learning, relatively little is understood theoretically about why these
techniques are so successful at feature learning and compression. Here, we show
that deep learning is intimately related to one of the most important and
successful techniques in theoretical physics, the renormalization group (RG).
RG is an iterative coarse-graining scheme that allows for the extraction of
relevant features (i.e. operators) as a physical system is examined at
different length scales. We construct an exact mapping from the variational
renormalization group, first introduced by Kadanoff, and deep learning
architectures based on Restricted Boltzmann Machines (RBMs). We illustrate
these ideas using the nearest-neighbor Ising Model in one and two-dimensions.
Our results suggests that deep learning algorithms may be employing a
generalized RG-like scheme to learn relevant features from data.">"An exact mapping between the Variational Renormalization Group and Deep
  Learning"</a>,
Pankaj Mehta, David J. Schwab,
arXiv: <a href="http://arxiv.org/abs/1410.3831v1">1410.3831</a>,
10/2014</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/1407.3124v2" title="In this paper we review basic and emerging models and associated algorithms
for large-scale tensor networks, especially Tensor Train (TT) decompositions
using novel mathematical and graphical representations. We discus the concept
of tensorization (i.e., creating very high-order tensors from lower-order
original data) and super compression of data achieved via quantized tensor
train (QTT) networks. The purpose of a tensorization and quantization is to
achieve, via low-rank tensor approximations &quot;super&quot; compression, and
meaningful, compact representation of structured data. The main objective of
this paper is to show how tensor networks can be used to solve a wide class of
big data optimization problems (that are far from tractable by classical
numerical methods) by applying tensorization and performing all operations
using relatively small size matrices and tensors and applying iteratively
optimized and approximative tensor contractions.
  Keywords: Tensor networks, tensor train (TT) decompositions, matrix product
states (MPS), matrix product operators (MPO), basic tensor operations,
tensorization, distributed representation od data optimization problems for
very large-scale problems: generalized eigenvalue decomposition (GEVD),
PCA/SVD, canonical correlation analysis (CCA).">"Tensor Networks for Big Data Analytics and Large-Scale Optimization
  Problems"</a>,
Andrzej Cichocki,
arXiv: <a href="http://arxiv.org/abs/1407.3124v2">1407.3124</a>,
7/2014</p>
</li>
</ul>
<h2>Quantum Computation and Quantum Algorithms for Machine Learning</h2>
<ul>
<li>
<p><a href="http://arxiv.org/abs/1307.0411v2" title="Machine-learning tasks frequently involve problems of manipulating and
classifying large numbers of vectors in high-dimensional spaces. Classical
algorithms for solving such problems typically take time polynomial in the
number of vectors and the dimension of the space. Quantum computers are good at
manipulating high-dimensional vectors in large tensor product spaces. This
paper provides supervised and unsupervised quantum machine learning algorithms
for cluster assignment and cluster finding. Quantum machine learning can take
time logarithmic in both the number of vectors and their dimension, an
exponential speed-up over classical algorithms.">"Quantum algorithms for supervised and unsupervised machine learning"</a>,
Seth Lloyd, Masoud Mohseni, Patrick Rebentrost,
arXiv: <a href="http://arxiv.org/abs/1307.0411v2">1307.0411</a>,
7/2013</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/quant-ph/0411140v2" title="In this article we give several new results on the complexity of algorithms
that learn Boolean functions from quantum queries and quantum examples.
  Hunziker et al. conjectured that for any class C of Boolean functions, the
number of quantum black-box queries which are required to exactly identify an
unknown function from C is \(O(\frac{\log |C|}{\sqrt{{\hat{\gamma}}^{C}}})\),
where \(\hat{\gamma}^{C}\) is a combinatorial parameter of the class C. We
essentially resolve this conjecture in the affirmative by giving a quantum
algorithm that, for any class C, identifies any unknown function from C using
\(O(\frac{\log |C| \log \log |C|}{\sqrt{{\hat{\gamma}}^{C}}})\) quantum black-box
queries.
  We consider a range of natural problems intermediate between the exact
learning problem (in which the learner must obtain all bits of information
about the black-box function) and the usual problem of computing a predicate
(in which the learner must obtain only one bit of information about the
black-box function). We give positive and negative results on when the quantum
and classical query complexities of these intermediate problems are
polynomially related to each other.
  Finally, we improve the known lower bounds on the number of quantum examples
(as opposed to quantum black-box queries) required for \((\epsilon,\delta)\)-PAC
learning any concept class of Vapnik-Chervonenkis dimension d over the domain
\(\{0,1\}^n\) from \(\Omega(\frac{d}{n})\) to \(\Omega(\frac{1}{\epsilon}\log
\frac{1}{\delta}+d+\frac{\sqrt{d}}{\epsilon})\). This new lower bound comes
closer to matching known upper bounds for classical PAC learning.">"Improved Bounds on Quantum Learning Algorithms"</a>,
Alp Atici, Rocco A. Servedio,
arXiv: <a href="http://arxiv.org/abs/quant-ph/0411140v2">quant-ph/0411140</a>,
11/2004</p>
</li>
<li>
<p><a href="http://arxiv.org/abs/quant-ph/0309059v1" title="Concept learning provides a natural framework in which to place the problems
solved by the quantum algorithms of Bernstein-Vazirani and Grover. By combining
the tools used in these algorithms--quantum fast transforms and amplitude
amplification--with a novel (in this context) tool--a solution method for
geometrical optimization problems--we derive a general technique for quantum
concept learning. We name this technique &quot;Amplified Impatient Learning&quot; and
apply it to construct quantum algorithms solving two new problems: BATTLESHIP
and MAJORITY, more efficiently than is possible classically.">"The geometry of quantum learning"</a>,
Markus Hunziker, David A. Meyer, Jihun Park, James Pommersheim, Mitch Rothstein,
arXiv: <a href="http://arxiv.org/abs/quant-ph/0309059v1">quant-ph/0309059</a>,
9/2003</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></p>
        </section>
      </div>

  </section>

  <!-- Two: Including columns with links to most recent articles -->
  <!--
    <section class="wrapper style1 container special">
        <div class="row">
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/machine-learning-for-quantum-design-at-pi.html" rel='bookmark'><h3>Machine Learning for Quantum Design at PI</h3></a>
              </header>
              <p>The Perimeter Institute for Theoretical Physics in Waterloo, Canada, hosts "</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/machine-learning-for-quantum-design-at-pi.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" rel='bookmark'><h3>Workshop Machine Learning for Quantum Technology at MPL Erlangen</h3></a>
              </header>
              <p>The Max Planck Institute for the Science of Light in Erlangen, Germany, hosts <a href="https://www.mpl.mpg.de/divisions/marquardt-division/workshops/2019-machi</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/DL-theory.html" rel='bookmark'><h3>The Theory of Deep Learning - Part I</h3></a>
              </header>
              <p>Why do modern deep neural networks (DNNs) perform so well on previously unseen test data, even when their number of weights is much larger than the number of data points</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/DL-theory.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
        </div>
    </section>  -->
  </article>

<!-- Footer -->
<footer id="footer">

    <ul class="icons">
    </ul>

    <span class="copyright">&copy; physicsml. All rights reserved. <br> Powered by <a href="https://blog.getpelican.com">Pelican</a>. Theme Twenty, Design: <a href="http://html5up.net">HTML5 UP</a>. Implemented and maintained by Anna Go and Danny Kong.</span>

</footer>
</body>
</html>
